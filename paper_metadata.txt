Title: Evaluating Open Language Models Across Task Types, Application Domains, and Reasoning Types: An In-Depth Experimental Analysis
PDF: https://arxiv.org/pdf/2406.11402
Summary: The rapid rise of Language Models (LMs) has expanded their use in severalapplications. Yet, due to constraints of model size, associated cost, orproprietary restrictions, utilizing state-of-the-art (SOTA) LLMs is not alwaysfeasible. With open, smaller LMs emerging, more applications can leverage theircapabilities, but selecting the right LM can be challenging. This work conductsan in-depth experimental analysis of the semantic correctness of outputs of 10smaller, open LMs across three aspects: task types, application domains andreasoning types, using diverse prompt styles. We demonstrate that mosteffective models and prompt styles vary depending on the specific requirements.Our analysis provides a comparative assessment of LMs and prompt styles using aproposed three-tier schema of aspects for their strategic selection based onuse-case and other constraints. We also show that if utilized appropriately,these LMs can compete with, and sometimes outperform, SOTA LLMs likeDeepSeek-v2, GPT-3.5-Turbo, and GPT-4o.
---
Title: Task Me Anything
PDF: https://arxiv.org/pdf/2406.11775
Summary: Benchmarks for large multimodal language models (MLMs) now serve tosimultaneously assess the general capabilities of models instead of evaluatingfor a specific capability. As a result, when a developer wants to identifywhich models to use for their application, they are overwhelmed by the numberof benchmarks and remain uncertain about which benchmark's results are mostreflective of their specific use case. This paper introduces Task-Me-Anything,a benchmark generation engine which produces a benchmark tailored to a user'sneeds. Task-Me-Anything maintains an extendable taxonomy of visual assets andcan programmatically generate a vast number of task instances. Additionally, italgorithmically addresses user queries regarding MLM performance efficientlywithin a computational budget. It contains 113K images, 10K videos, 2K 3Dobject assets, over 365 object categories, 655 attributes, and 335relationships. It can generate 750M image/video question-answering pairs, whichfocus on evaluating MLM perceptual capabilities. Task-Me-Anything revealscritical insights: open-source MLMs excel in object and attribute recognitionbut lack spatial and temporal understanding; each model exhibits uniquestrengths and weaknesses; larger models generally perform better, thoughexceptions exist; and GPT4o demonstrates challenges in recognizingrotating/moving objects and distinguishing colors.
---
Title: From Pixels to Prose: A Large Dataset of Dense Image Captions
PDF: https://arxiv.org/pdf/2406.10328
Summary: Training large vision-language models requires extensive, high-qualityimage-text pairs. Existing web-scraped datasets, however, are noisy and lackdetailed image descriptions. To bridge this gap, we introduce PixelProse, acomprehensive dataset of over 16M (million) synthetically generated captions,leveraging cutting-edge vision-language models for detailed and accuratedescriptions. To ensure data integrity, we rigorously analyze our dataset forproblematic content, including child sexual abuse material (CSAM), personallyidentifiable information (PII), and toxicity. We also provide valuable metadatasuch as watermark presence and aesthetic scores, aiding in further datasetfiltering. We hope PixelProse will be a valuable resource for futurevision-language research. PixelProse is available athttps://huggingface.co/datasets/tomg-group-umd/pixelprose
---
Title: VideoLLM-online: Online Video Large Language Model for Streaming Video
PDF: https://arxiv.org/pdf/2406.11816
Summary: Recent Large Language Models have been enhanced with vision capabilities,enabling them to comprehend images, videos, and interleaved vision-languagecontent. However, the learning methods of these large multimodal modelstypically treat videos as predetermined clips, making them less effective andefficient at handling streaming video inputs. In this paper, we propose a novelLearning-In-Video-Stream (LIVE) framework, which enables temporally aligned,long-context, and real-time conversation within a continuous video stream. OurLIVE framework comprises comprehensive approaches to achieve video streamingdialogue, encompassing: (1) a training objective designed to perform languagemodeling for continuous streaming inputs, (2) a data generation scheme thatconverts offline temporal annotations into a streaming dialogue format, and (3)an optimized inference pipeline to speed up the model responses in real-worldvideo streams. With our LIVE framework, we built VideoLLM-online model uponLlama-2/Llama-3 and demonstrate its significant advantages in processingstreaming videos. For instance, on average, our model can support streamingdialogue in a 5-minute video clip at over 10 FPS on an A100 GPU. Moreover, italso showcases state-of-the-art performance on public offline video benchmarks,such as recognition, captioning, and forecasting. The code, model, data, anddemo have been made available at https://showlab.github.io/videollm-online.
---
Title: THEANINE: Revisiting Memory Management in Long-term Conversations with Timeline-augmented Response Generation
PDF: https://arxiv.org/pdf/2406.10996
Summary: Large language models (LLMs) are capable of processing lengthy dialoguehistories during prolonged interaction with users without additional memorymodules; however, their responses tend to overlook or incorrectly recallinformation from the past. In this paper, we revisit memory-augmented responsegeneration in the era of LLMs. While prior work focuses on getting rid ofoutdated memories, we argue that such memories can provide contextual cues thathelp dialogue systems understand the development of past events and, therefore,benefit response generation. We present Theanine, a framework that augmentsLLMs' response generation with memory timelines -- series of memories thatdemonstrate the development and causality of relevant past events. Along withTheanine, we introduce TeaFarm, a counterfactual-driven question-answeringpipeline addressing the limitation of G-Eval in long-term conversations.Supplementary videos of our methods and the TeaBag dataset for TeaFarmevaluation are in https://theanine-693b0.web.app/.
---
Title: WPO: Enhancing RLHF with Weighted Preference Optimization
PDF: https://arxiv.org/pdf/2406.11827
Summary: Reinforcement learning from human feedback (RLHF) is a promising solution toalign large language models (LLMs) more closely with human values. Off-policypreference optimization, where the preference data is obtained from othermodels, is widely adopted due to its cost efficiency and scalability. However,off-policy preference optimization often suffers from a distributional gapbetween the policy used for data collection and the target policy, leading tosuboptimal optimization. In this paper, we propose a novel strategy to mitigatethis problem by simulating on-policy learning with off-policy preference data.Our Weighted Preference Optimization (WPO) method adapts off-policy data toresemble on-policy data more closely by reweighting preference pairs accordingto their probability under the current policy. This method not only addressesthe distributional gap problem but also enhances the optimization processwithout incurring additional costs. We validate our method on instructionfollowing benchmarks including Alpaca Eval 2 and MT-bench. WPO not onlyoutperforms Direct Preference Optimization (DPO) by up to 5.6% on Alpaca Eval 2but also establishes a remarkable length-controlled winning rate againstGPT-4-turbo of 48.6% based on Llama-3-8B-Instruct, making it the strongest 8Bmodel on the leaderboard. We will release the code and models athttps://github.com/wzhouad/WPO.
---
Title: WildVision: Evaluating Vision-Language Models in the Wild with Human Preferences
PDF: https://arxiv.org/pdf/2406.11069
Summary: Recent breakthroughs in vision-language models (VLMs) emphasize the necessityof benchmarking human preferences in real-world multimodal interactions. Toaddress this gap, we launched WildVision-Arena (WV-Arena), an online platformthat collects human preferences to evaluate VLMs. We curated WV-Bench byselecting 500 high-quality samples from 8,000 user submissions in WV-Arena.WV-Bench uses GPT-4 as the judge to compare each VLM with Claude-3-Sonnet,achieving a Spearman correlation of 0.94 with the WV-Arena Elo. Thissignificantly outperforms other benchmarks like MMVet, MMMU, and MMStar.  Our comprehensive analysis of 20K real-world interactions reveals importantinsights into the failure cases of top-performing VLMs. For example, we findthat although GPT-4V surpasses many other models like Reka-Flash, Opus, andYi-VL-Plus in simple visual recognition and reasoning tasks, it still faceschallenges with subtle contextual cues, spatial reasoning, visual imagination,and expert domain knowledge. Additionally, current VLMs exhibit issues withhallucinations and safety when intentionally provoked. We are releasing ourchat and feedback data to further advance research in the field of VLMs.
---
Title: MeshAnything: Artist-Created Mesh Generation with Autoregressive Transformers
PDF: https://arxiv.org/pdf/2406.10163
Summary: Recently, 3D assets created via reconstruction and generation have matchedthe quality of manually crafted assets, highlighting their potential forreplacement. However, this potential is largely unrealized because these assetsalways need to be converted to meshes for 3D industry applications, and themeshes produced by current mesh extraction methods are significantly inferiorto Artist-Created Meshes (AMs), i.e., meshes created by human artists.Specifically, current mesh extraction methods rely on dense faces and ignoregeometric features, leading to inefficiencies, complicated post-processing, andlower representation quality. To address these issues, we introduceMeshAnything, a model that treats mesh extraction as a generation problem,producing AMs aligned with specified shapes. By converting 3D assets in any 3Drepresentation into AMs, MeshAnything can be integrated with various 3D assetproduction methods, thereby enhancing their application across the 3D industry.The architecture of MeshAnything comprises a VQ-VAE and a shape-conditioneddecoder-only transformer. We first learn a mesh vocabulary using the VQ-VAE,then train the shape-conditioned decoder-only transformer on this vocabularyfor shape-conditioned autoregressive mesh generation. Our extensive experimentsshow that our method generates AMs with hundreds of times fewer faces,significantly improving storage, rendering, and simulation efficiencies, whileachieving precision comparable to previous methods.
---
Title: Vid3D: Synthesis of Dynamic 3D Scenes using 2D Video Diffusion
PDF: https://arxiv.org/pdf/2406.11196
Summary: A recent frontier in computer vision has been the task of 3D videogeneration, which consists of generating a time-varying 3D representation of ascene. To generate dynamic 3D scenes, current methods explicitly model 3Dtemporal dynamics by jointly optimizing for consistency across both time andviews of the scene. In this paper, we instead investigate whether it isnecessary to explicitly enforce multiview consistency over time, as currentapproaches do, or if it is sufficient for a model to generate 3Drepresentations of each timestep independently. We hence propose a model,Vid3D, that leverages 2D video diffusion to generate 3D videos by firstgenerating a 2D "seed" of the video's temporal dynamics and then independentlygenerating a 3D representation for each timestep in the seed video. We evaluateVid3D against two state-of-the-art 3D video generation methods and find thatVid3D is achieves comparable results despite not explicitly modeling 3Dtemporal dynamics. We further ablate how the quality of Vid3D depends on thenumber of views generated per frame. While we observe some degradation withfewer views, performance degradation remains minor. Our results thus suggestthat 3D temporal knowledge may not be necessary to generate high-qualitydynamic 3D scenes, potentially enabling simpler generative algorithms for thistask.
---
Title: mDPO: Conditional Preference Optimization for Multimodal Large Language Models
PDF: https://arxiv.org/pdf/2406.11839
Summary: Direct preference optimization (DPO) has shown to be an effective method forlarge language model (LLM) alignment. Recent works have attempted to apply DPOto multimodal scenarios but have found it challenging to achieve consistentimprovement. Through a comparative experiment, we identify the unconditionalpreference problem in multimodal preference optimization, where the modeloverlooks the image condition. To address this problem, we propose mDPO, amultimodal DPO objective that prevents the over-prioritization of language-onlypreferences by also optimizing image preference. Moreover, we introduce areward anchor that forces the reward to be positive for chosen responses,thereby avoiding the decrease in their likelihood -- an intrinsic problem ofrelative preference optimization. Experiments on two multimodal LLMs ofdifferent sizes and three widely used benchmarks demonstrate that mDPOeffectively addresses the unconditional preference problem in multimodalpreference optimization and significantly improves model performance,particularly in reducing hallucination.
---
Title: L4GM: Large 4D Gaussian Reconstruction Model
PDF: https://arxiv.org/pdf/2406.10324
Summary: We present L4GM, the first 4D Large Reconstruction Model that producesanimated objects from a single-view video input -- in a single feed-forwardpass that takes only a second. Key to our success is a novel dataset ofmultiview videos containing curated, rendered animated objects from Objaverse.This dataset depicts 44K diverse objects with 110K animations rendered in 48viewpoints, resulting in 12M videos with a total of 300M frames. We keep ourL4GM simple for scalability and build directly on top of LGM, a pretrained 3DLarge Reconstruction Model that outputs 3D Gaussian ellipsoids from multiviewimage input. L4GM outputs a per-frame 3D Gaussian Splatting representation fromvideo frames sampled at a low fps and then upsamples the representation to ahigher fps to achieve temporal smoothness. We add temporal self-attentionlayers to the base LGM to help it learn consistency across time, and utilize aper-timestep multiview rendering loss to train the model. The representation isupsampled to a higher framerate by training an interpolation model whichproduces intermediate 3D Gaussian representations. We showcase that L4GM thatis only trained on synthetic data generalizes extremely well on in-the-wildvideos, producing high quality animated 3D assets.
---
Title: DataComp-LM: In search of the next generation of training sets for language models
PDF: https://arxiv.org/pdf/2406.11794
Summary: We introduce DataComp for Language Models (DCLM), a testbed for controlleddataset experiments with the goal of improving language models. As part ofDCLM, we provide a standardized corpus of 240T tokens extracted from CommonCrawl, effective pretraining recipes based on the OpenLM framework, and a broadsuite of 53 downstream evaluations. Participants in the DCLM benchmark canexperiment with data curation strategies such as deduplication, filtering, anddata mixing at model scales ranging from 412M to 7B parameters. As a baselinefor DCLM, we conduct extensive experiments and find that model-based filteringis key to assembling a high-quality training set. The resulting dataset,DCLM-Baseline enables training a 7B parameter language model from scratch to64% 5-shot accuracy on MMLU with 2.6T training tokens. Compared to MAP-Neo, theprevious state-of-the-art in open-data language models, DCLM-Baselinerepresents a 6.6 percentage point improvement on MMLU while being trained with40% less compute. Our baseline model is also comparable to Mistral-7B-v0.3 andLlama 3 8B on MMLU (63% & 66%), and performs similarly on an average of 53natural language understanding tasks while being trained with 6.6x less computethan Llama 3 8B. Our results highlight the importance of dataset design fortraining language models and offer a starting point for further research ondata curation.
---
Title: GAMA: A Large Audio-Language Model with Advanced Audio Understanding and Complex Reasoning Abilities
PDF: https://arxiv.org/pdf/2406.11768
Summary: Perceiving and understanding non-speech sounds and non-verbal speech isessential to making decisions that help us interact with our surroundings. Inthis paper, we propose GAMA, a novel General-purpose Large Audio-Language Model(LALM) with Advanced Audio Understanding and Complex Reasoning Abilities. Webuild GAMA by integrating an LLM with multiple types of audio representations,including features from a custom Audio Q-Former, a multi-layer aggregator thataggregates features from multiple layers of an audio encoder. We fine-tune GAMAon a large-scale audio-language dataset, which augments it with audiounderstanding capabilities. Next, we propose CompA-R (Instruction-Tuning forComplex Audio Reasoning), a synthetically generated instruction-tuning (IT)dataset with instructions that require the model to perform complex reasoningon the input audio. We instruction-tune GAMA with CompA-R to endow it withcomplex reasoning abilities, where we further add a soft prompt as input withhigh-level semantic evidence by leveraging event tags of the input audio.Finally, we also propose CompA-R-test, a human-labeled evaluation dataset forevaluating the capabilities of LALMs on open-ended audio question-answeringthat requires complex reasoning. Through automated and expert humanevaluations, we show that GAMA outperforms all other LALMs in literature ondiverse audio understanding tasks by margins of 1%-84%. Further, GAMA IT-ed onCompA-R proves to be superior in its complex reasoning and instructionfollowing capabilities.
---
Title: Be like a Goldfish, Don't Memorize! Mitigating Memorization in Generative LLMs
PDF: https://arxiv.org/pdf/2406.10209
Summary: Large language models can memorize and repeat their training data, causingprivacy and copyright risks. To mitigate memorization, we introduce a subtlemodification to the next-token training objective that we call the goldfishloss. During training, a randomly sampled subset of tokens are excluded fromthe loss computation. These dropped tokens are not memorized by the model,which prevents verbatim reproduction of a complete chain of tokens from thetraining set. We run extensive experiments training billion-scale Llama-2models, both pre-trained and trained from scratch, and demonstrate significantreductions in extractable memorization with little to no impact on downstreambenchmarks.
---
Title: RVT-2: Learning Precise Manipulation from Few Demonstrations
PDF: https://arxiv.org/pdf/2406.08545
Summary: In this work, we study how to build a robotic system that can solve multiple3D manipulation tasks given language instructions. To be useful in industrialand household domains, such a system should be capable of learning new taskswith few demonstrations and solving them precisely. Prior works, like PerActand RVT, have studied this problem, however, they often struggle with tasksrequiring high precision. We study how to make them more effective, precise,and fast. Using a combination of architectural and system-level improvements,we propose RVT-2, a multitask 3D manipulation model that is 6X faster intraining and 2X faster in inference than its predecessor RVT. RVT-2 achieves anew state-of-the-art on RLBench, improving the success rate from 65% to 82%.RVT-2 is also effective in the real world, where it can learn tasks requiringhigh precision, like picking up and inserting plugs, with just 10demonstrations. Visual results, code, and trained model are provided at:https://robotic-view-transformer-2.github.io/.
---
Title: Vivid-ZOO: Multi-View Video Generation with Diffusion Model
PDF: https://arxiv.org/pdf/2406.08659
Summary: While diffusion models have shown impressive performance in 2D image/videogeneration, diffusion-based Text-to-Multi-view-Video (T2MVid) generationremains underexplored. The new challenges posed by T2MVid generation lie in thelack of massive captioned multi-view videos and the complexity of modeling suchmulti-dimensional distribution. To this end, we propose a novel diffusion-basedpipeline that generates high-quality multi-view videos centered around adynamic 3D object from text. Specifically, we factor the T2MVid problem intoviewpoint-space and time components. Such factorization allows us to combineand reuse layers of advanced pre-trained multi-view image and 2D videodiffusion models to ensure multi-view consistency as well as temporal coherencefor the generated multi-view videos, largely reducing the training cost. Wefurther introduce alignment modules to align the latent spaces of layers fromthe pre-trained multi-view and the 2D video diffusion models, addressing thereused layers' incompatibility that arises from the domain gap between 2D andmulti-view data. In support of this and future research, we further contributea captioned multi-view video dataset. Experimental results demonstrate that ourmethod generates high-quality multi-view videos, exhibiting vivid motions,temporal coherence, and multi-view consistency, given a variety of textprompts.
---
Title: AV-GS: Learning Material and Geometry Aware Priors for Novel View Acoustic Synthesis
PDF: https://arxiv.org/pdf/2406.08920
Summary: Novel view acoustic synthesis (NVAS) aims to render binaural audio at anytarget viewpoint, given a mono audio emitted by a sound source at a 3D scene.Existing methods have proposed NeRF-based implicit models to exploit visualcues as a condition for synthesizing binaural audio. However, in addition tolow efficiency originating from heavy NeRF rendering, these methods all have alimited ability of characterizing the entire scene environment such as roomgeometry, material properties, and the spatial relation between the listenerand sound source. To address these issues, we propose a novel Audio-VisualGaussian Splatting (AV-GS) model. To obtain a material-aware and geometry-awarecondition for audio synthesis, we learn an explicit point-based scenerepresentation with an audio-guidance parameter on locally initialized Gaussianpoints, taking into account the space relation from the listener and soundsource. To make the visual scene model audio adaptive, we propose a pointdensification and pruning strategy to optimally distribute the Gaussian points,with the per-point contribution in sound propagation (e.g., more points neededfor texture-less wall surfaces as they affect sound path diversion). Extensiveexperiments validate the superiority of our AV-GS over existing alternatives onthe real-world RWAS and simulation-based SoundSpaces datasets.
---
Title: VideoGUI: A Benchmark for GUI Automation from Instructional Videos
PDF: https://arxiv.org/pdf/2406.10227
Summary: Graphical User Interface (GUI) automation holds significant promise forenhancing human productivity by assisting with computer tasks. Existing taskformulations primarily focus on simple tasks that can be specified by a single,language-only instruction, such as "Insert a new slide." In this work, weintroduce VideoGUI, a novel multi-modal benchmark designed to evaluate GUIassistants on visual-centric GUI tasks. Sourced from high-quality webinstructional videos, our benchmark focuses on tasks involving professional andnovel software (e.g., Adobe Photoshop or Stable Diffusion WebUI) and complexactivities (e.g., video editing). VideoGUI evaluates GUI assistants through ahierarchical process, allowing for identification of the specific levels atwhich they may fail: (i) high-level planning: reconstruct procedural subtasksfrom visual conditions without language descriptions; (ii) middle-levelplanning: generate sequences of precise action narrations based on visual state(i.e., screenshot) and goals; (iii) atomic action execution: perform specificactions such as accurately clicking designated elements. For each level, wedesign evaluation metrics across individual dimensions to provide clearsignals, such as individual performance in clicking, dragging, typing, andscrolling for atomic action execution. Our evaluation on VideoGUI reveals thateven the SoTA large multimodal model GPT4o performs poorly on visual-centricGUI tasks, especially for high-level planning.
---
Title: SEACrowd: A Multilingual Multimodal Data Hub and Benchmark Suite for Southeast Asian Languages
PDF: https://arxiv.org/pdf/2406.10118
Summary: Southeast Asia (SEA) is a region rich in linguistic diversity and culturalvariety, with over 1,300 indigenous languages and a population of 671 millionpeople. However, prevailing AI models suffer from a significant lack ofrepresentation of texts, images, and audio datasets from SEA, compromising thequality of AI models for SEA languages. Evaluating models for SEA languages ischallenging due to the scarcity of high-quality datasets, compounded by thedominance of English training data, raising concerns about potential culturalmisrepresentation. To address these challenges, we introduce SEACrowd, acollaborative initiative that consolidates a comprehensive resource hub thatfills the resource gap by providing standardized corpora in nearly 1,000 SEAlanguages across three modalities. Through our SEACrowd benchmarks, we assessthe quality of AI models on 36 indigenous languages across 13 tasks, offeringvaluable insights into the current AI landscape in SEA. Furthermore, we proposestrategies to facilitate greater AI advancements, maximizing potential utilityand resource equity for the future of AI in SEA.
---
Title: Training-free Camera Control for Video Generation
PDF: https://arxiv.org/pdf/2406.10126
Summary: We propose a training-free and robust solution to offer camera movementcontrol for off-the-shelf video diffusion models. Unlike previous work, ourmethod does not require any supervised finetuning on camera-annotated datasetsor self-supervised training via data augmentation. Instead, it can be pluggedand played with most pretrained video diffusion models and generate cameracontrollable videos with a single image or text prompt as input. Theinspiration of our work comes from the layout prior that intermediate latentshold towards generated results, thus rearranging noisy pixels in them will makeoutput content reallocated as well. As camera move could also be seen as a kindof pixel rearrangement caused by perspective change, videos could bereorganized following specific camera motion if their noisy latents changeaccordingly. Established on this, we propose our method CamTrol, which enablesrobust camera control for video diffusion models. It is achieved by a two-stageprocess. First, we model image layout rearrangement through explicit cameramovement in 3D point cloud space. Second, we generate videos with camera motionusing layout prior of noisy latents formed by a series of rearranged images.Extensive experiments have demonstrated the robustness our method holds incontrolling camera motion of generated videos. Furthermore, we show that ourmethod can produce impressive results in generating 3D rotation videos withdynamic content. Project page at https://lifedecoder.github.io/CamTrol/.
---
Title: Make It Count: Text-to-Image Generation with an Accurate Number of Objects
PDF: https://arxiv.org/pdf/2406.10210
Summary: Despite the unprecedented success of text-to-image diffusion models,controlling the number of depicted objects using text is surprisingly hard.This is important for various applications from technical documents, tochildren's books to illustrating cooking recipes. Generating object-correctcounts is fundamentally challenging because the generative model needs to keepa sense of separate identity for every instance of the object, even if severalobjects look identical or overlap, and then carry out a global computationimplicitly during generation. It is still unknown if such representationsexist. To address count-correct generation, we first identify features withinthe diffusion model that can carry the object identity information. We then usethem to separate and count instances of objects during the denoising processand detect over-generation and under-generation. We fix the latter by traininga model that predicts both the shape and location of a missing object, based onthe layout of existing ones, and show how it can be used to guide denoisingwith correct object count. Our approach, CountGen, does not depend on externalsource to determine object layout, but rather uses the prior from the diffusionmodel itself, creating prompt-dependent and seed-dependent layouts. Evaluatedon two benchmark datasets, we find that CountGen strongly outperforms thecount-accuracy of existing baselines.
---
Title: BABILong: Testing the Limits of LLMs with Long Context Reasoning-in-a-Haystack
PDF: https://arxiv.org/pdf/2406.10149
Summary: In recent years, the input context sizes of large language models (LLMs) haveincreased dramatically. However, existing evaluation methods have not keptpace, failing to comprehensively assess the efficiency of models in handlinglong contexts. To bridge this gap, we introduce the BABILong benchmark,designed to test language models' ability to reason across facts distributed inextremely long documents. BABILong includes a diverse set of 20 reasoningtasks, including fact chaining, simple induction, deduction, counting, andhandling lists/sets. These tasks are challenging on their own, and even moredemanding when the required facts are scattered across long natural text. Ourevaluations show that popular LLMs effectively utilize only 10-20\% of thecontext and their performance declines sharply with increased reasoningcomplexity. Among alternatives to in-context reasoning, Retrieval-AugmentedGeneration methods achieve a modest 60\% accuracy on single-fact questionanswering, independent of context length. Among context extension methods, thehighest performance is demonstrated by recurrent memory transformers, enablingthe processing of lengths up to 11 million tokens. The BABILong benchmark isextendable to any length to support the evaluation of new upcoming models withincreased capabilities, and we provide splits up to 1 million token lengths.
---
Title: GaussianSR: 3D Gaussian Super-Resolution with 2D Diffusion Priors
PDF: https://arxiv.org/pdf/2406.10111
Summary: Achieving high-resolution novel view synthesis (HRNVS) from low-resolutioninput views is a challenging task due to the lack of high-resolution data.Previous methods optimize high-resolution Neural Radiance Field (NeRF) fromlow-resolution input views but suffer from slow rendering speed. In this work,we base our method on 3D Gaussian Splatting (3DGS) due to its capability ofproducing high-quality images at a faster rendering speed. To alleviate theshortage of data for higher-resolution synthesis, we propose to leverageoff-the-shelf 2D diffusion priors by distilling the 2D knowledge into 3D withScore Distillation Sampling (SDS). Nevertheless, applying SDS directly toGaussian-based 3D super-resolution leads to undesirable and redundant 3DGaussian primitives, due to the randomness brought by generative priors. Tomitigate this issue, we introduce two simple yet effective techniques to reducestochastic disturbances introduced by SDS. Specifically, we 1) shrink the rangeof diffusion timestep in SDS with an annealing strategy; 2) randomly discardredundant Gaussian primitives during densification. Extensive experiments havedemonstrated that our proposed GaussainSR can attain high-quality results forHRNVS with only low-resolution inputs on both synthetic and real-worlddatasets. Project page: https://chchnii.github.io/GaussianSR/
---
Title: GEB-1.3B: Open Lightweight Large Language Model
PDF: https://arxiv.org/pdf/2406.09900
Summary: Recently developed large language models (LLMs) such as ChatGPT, Claude, andLlama have demonstrated impressive abilities, and even surpass human-levelperformance in several tasks. Despite their success, the resource-intensivedemands of these models, requiring significant computational power for bothtraining and inference, limit their deployment to high-performance servers.Additionally, the extensive calculation requirements of the models often leadto increased latency in response times. With the increasing need for LLMs tooperate efficiently on CPUs, research about lightweight models that areoptimized for CPU inference has emerged. In this work, we introduce GEB-1.3B, alightweight LLM trained on 550 billion tokens in both Chinese and Englishlanguages. We employ novel training techniques, including ROPE,Group-Query-Attention, and FlashAttention-2, to accelerate training whilemaintaining model performance. Additionally, we fine-tune the model using 10million samples of instruction data to enhance alignment. GEB-1.3B exhibitsoutstanding performance on general benchmarks such as MMLU, C-Eval, and CMMLU,outperforming comparative models such as MindLLM-1.3B and TinyLLaMA-1.1B.Notably, the FP32 version of GEB-1.3B achieves commendable inference times onCPUs, with ongoing efforts to further enhance speed through advancedquantization techniques. The release of GEB-1.3B as an open-source model marksa significant contribution to the development of lightweight LLMs, promising tofoster further research and innovation in the field.
---
Title: MaskLID: Code-Switching Language Identification through Iterative Masking
PDF: https://arxiv.org/pdf/2406.06263
Summary: We present MaskLID, a simple, yet effective, code-switching (CS) languageidentification (LID) method. MaskLID does not require any training and isdesigned to complement current high-performance sentence-level LIDs.Sentence-level LIDs are classifiers trained on monolingual texts to providesingle labels, typically using a softmax layer to turn scores intoprobabilities. However, in cases where a sentence is composed in both L1 and L2languages, the LID classifier often only returns the dominant label L1. Toaddress this limitation, MaskLID employs a strategy to mask text featuresassociated with L1, allowing the LID to classify the text as L2 in the nextround. This method uses the LID itself to identify the features that requiremasking and does not rely on any external resource. In this work, we explorethe use of MaskLID for two open-source LIDs (GlotLID and OpenLID), that areboth based on the FastText architecture. Code and demo are available athttps://github.com/cisnlp/MaskLID.
---
Title: XLand-100B: A Large-Scale Multi-Task Dataset for In-Context Reinforcement Learning
PDF: https://arxiv.org/pdf/2406.08973
Summary: Following the success of the in-context learning paradigm in large-scalelanguage and computer vision models, the recently emerging field of in-contextreinforcement learning is experiencing a rapid growth. However, its developmenthas been held back by the lack of challenging benchmarks, as all theexperiments have been carried out in simple environments and on small-scaledatasets. We present XLand-100B, a large-scale dataset for in-contextreinforcement learning based on the XLand-MiniGrid environment, as a first stepto alleviate this problem. It contains complete learning histories for nearly30,000 different tasks, covering 100B transitions and 2.5B episodes. Ittook 50,000 GPU hours to collect the dataset, which is beyond the reach ofmost academic labs. Along with the dataset, we provide the utilities toreproduce or expand it even further. With this substantial effort, we aim todemocratize research in the rapidly growing field of in-context reinforcementlearning and provide a solid foundation for further scaling. The code isopen-source and available under Apache 2.0 licence athttps://github.com/dunno-lab/xland-minigrid-datasets.
---
Title: ChartMimic: Evaluating LMM's Cross-Modal Reasoning Capability via Chart-to-Code Generation
PDF: https://arxiv.org/pdf/2406.09961
Summary: We introduce a new benchmark, ChartMimic, aimed at assessing thevisually-grounded code generation capabilities of large multimodal models(LMMs). ChartMimic utilizes information-intensive visual charts and textualinstructions as inputs, requiring LMMs to generate the corresponding code forchart rendering. ChartMimic includes 1,000 human-curated (figure, instruction,code) triplets, which represent the authentic chart use cases found inscientific papers across various domains(e.g., Physics, Computer Science,Economics, etc). These charts span 18 regular types and 4 advanced types,diversifying into 191 subcategories. Furthermore, we propose multi-levelevaluation metrics to provide an automatic and thorough assessment of theoutput code and the rendered charts. Unlike existing code generationbenchmarks, ChartMimic places emphasis on evaluating LMMs' capacity toharmonize a blend of cognitive capabilities, encompassing visual understanding,code generation, and cross-modal reasoning. The evaluation of 3 proprietarymodels and 11 open-weight models highlights the substantial challenges posed byChartMimic. Even the advanced GPT-4V, Claude-3-opus only achieve an averagescore of 73.2 and 53.7, respectively, indicating significant room forimprovement. We anticipate that ChartMimic will inspire the development ofLMMs, advancing the pursuit of artificial general intelligence.
---
Title: Designing a Dashboard for Transparency and Control of Conversational AI
PDF: https://arxiv.org/pdf/2406.07882
Summary: Conversational LLMs function as black box systems, leaving users guessingabout why they see the output they do. This lack of transparency is potentiallyproblematic, especially given concerns around bias and truthfulness. To addressthis issue, we present an end-to-end prototype-connecting interpretabilitytechniques with user experience design-that seeks to make chatbots moretransparent. We begin by showing evidence that a prominent open-source LLM hasa "user model": examining the internal state of the system, we can extract datarelated to a user's age, gender, educational level, and socioeconomic status.Next, we describe the design of a dashboard that accompanies the chatbotinterface, displaying this user model in real time. The dashboard can also beused to control the user model and the system's behavior. Finally, we discuss astudy in which users conversed with the instrumented system. Our resultssuggest that users appreciate seeing internal states, which helped them exposebiased behavior and increased their sense of control. Participants also madevaluable suggestions that point to future directions for both design andmachine learning research. The project page and video demo of our TalkTunersystem are available at https://bit.ly/talktuner-project-page
---
Title: Rethinking Human Evaluation Protocol for Text-to-Video Models: Enhancing Reliability,Reproducibility, and Practicality
PDF: https://arxiv.org/pdf/2406.08845
Summary: Recent text-to-video (T2V) technology advancements, as demonstrated by modelssuch as Gen2, Pika, and Sora, have significantly broadened its applicabilityand popularity. Despite these strides, evaluating these models posessubstantial challenges. Primarily, due to the limitations inherent in automaticmetrics, manual evaluation is often considered a superior method for assessingT2V generation. However, existing manual evaluation protocols facereproducibility, reliability, and practicality issues. To address thesechallenges, this paper introduces the Text-to-Video Human Evaluation (T2VHE)protocol, a comprehensive and standardized protocol for T2V models. The T2VHEprotocol includes well-defined metrics, thorough annotator training, and aneffective dynamic evaluation module. Experimental results demonstrate that thisprotocol not only ensures high-quality annotations but can also reduceevaluation costs by nearly 50%. We will open-source the entire setup of theT2VHE protocol, including the complete protocol workflow, the dynamicevaluation component details, and the annotation interface code. This will helpcommunities establish more sophisticated human assessment protocols.
---
Title: GUI Odyssey: A Comprehensive Dataset for Cross-App GUI Navigation on Mobile Devices
PDF: https://arxiv.org/pdf/2406.08451
Summary: Smartphone users often navigate across multiple applications (apps) tocomplete tasks such as sharing content between social media platforms.Autonomous Graphical User Interface (GUI) navigation agents can enhance userexperience in communication, entertainment, and productivity by streamliningworkflows and reducing manual intervention. However, prior GUI agents oftentrained with datasets comprising simple tasks that can be completed within asingle app, leading to poor performance in cross-app navigation. To addressthis problem, we introduce GUI Odyssey, a comprehensive dataset for trainingand evaluating cross-app navigation agents. GUI Odyssey consists of 7,735episodes from 6 mobile devices, spanning 6 types of cross-app tasks, 201 apps,and 1.4K app combos. Leveraging GUI Odyssey, we developed OdysseyAgent, amultimodal cross-app navigation agent by fine-tuning the Qwen-VL model with ahistory resampling module. Extensive experiments demonstrate OdysseyAgent'ssuperior accuracy compared to existing models. For instance, OdysseyAgentsurpasses fine-tuned Qwen-VL and zero-shot GPT-4V by 1.44\% and 55.49\%in-domain accuracy, and 2.29\% and 48.14\% out-of-domain accuracy on average.The dataset and code will be released inhttps://github.com/OpenGVLab/GUI-Odyssey.
---
Title: OmniCorpus: A Unified Multimodal Corpus of 10 Billion-Level Images Interleaved with Text
PDF: https://arxiv.org/pdf/2406.08418
Summary: Image-text interleaved data, consisting of multiple images and texts arrangedin a natural document format, aligns with the presentation paradigm of internetdata and closely resembles human reading habits. Recent studies have shown thatsuch data aids multimodal in-context learning and maintains the capabilities oflarge language models during multimodal fine-tuning. However, the limited scaleand diversity of current image-text interleaved data restrict the developmentof multimodal large language models. In this paper, we introduce OmniCorpus, a10 billion-scale image-text interleaved dataset. Using an efficient dataengine, we filter and extract large-scale high-quality documents, which contain8.6 billion images and 1,696 billion text tokens. Compared to counterparts(e.g., MMC4, OBELICS), our dataset 1) has 15 times larger scales whilemaintaining good data quality; 2) features more diverse sources, including bothEnglish and non-English websites as well as video-centric websites; 3) is moreflexible, easily degradable from an image-text interleaved format to pure textcorpus and image-text pairs. Through comprehensive analysis and experiments, wevalidate the quality, usability, and effectiveness of the proposed dataset. Wehope this could provide a solid data foundation for future multimodal modelresearch. Code and data are released athttps://github.com/OpenGVLab/OmniCorpus.
---
Title: Needle In A Multimodal Haystack
PDF: https://arxiv.org/pdf/2406.07230
Summary: With the rapid advancement of multimodal large language models (MLLMs), theirevaluation has become increasingly comprehensive. However, understanding longmultimodal content, as a foundational ability for real-world applications,remains underexplored. In this work, we present Needle In A Multimodal Haystack(MM-NIAH), the first benchmark specifically designed to systematically evaluatethe capability of existing MLLMs to comprehend long multimodal documents. Ourbenchmark includes three types of evaluation tasks: multimodal retrieval,counting, and reasoning. In each task, the model is required to answer thequestions according to different key information scattered throughout the givenmultimodal document. Evaluating the leading MLLMs on MM-NIAH, we observe thatexisting models still have significant room for improvement on these tasks,especially on vision-centric evaluation. We hope this work can provide aplatform for further research on long multimodal document comprehension andcontribute to the advancement of MLLMs. Code and benchmark are released athttps://github.com/OpenGVLab/MM-NIAH.
---
Title: Decoding the Diversity: A Review of the Indic AI Research Landscape
PDF: https://arxiv.org/pdf/2406.09559
Summary: This review paper provides a comprehensive overview of large language model(LLM) research directions within Indic languages. Indic languages are thosespoken in the Indian subcontinent, including India, Pakistan, Bangladesh, SriLanka, Nepal, and Bhutan, among others. These languages have a rich culturaland linguistic heritage and are spoken by over 1.5 billion people worldwide.With the tremendous market potential and growing demand for natural languageprocessing (NLP) based applications in diverse languages, generativeapplications for Indic languages pose unique challenges and opportunities forresearch. Our paper deep dives into the recent advancements in Indic generativemodeling, contributing with a taxonomy of research directions, tabulating 84recent publications. Research directions surveyed in this paper include LLMdevelopment, fine-tuning existing LLMs, development of corpora, benchmarkingand evaluation, as well as publications around specific techniques, tools, andapplications. We found that researchers across the publications emphasize thechallenges associated with limited data availability, lack of standardization,and the peculiar linguistic complexities of Indic languages. This work aims toserve as a valuable resource for researchers and practitioners working in thefield of NLP, particularly those focused on Indic languages, and contributes tothe development of more accurate and efficient LLM applications for theselanguages.
---
Title: Glyph-ByT5-v2: A Strong Aesthetic Baseline for Accurate Multilingual Visual Text Rendering
PDF: https://arxiv.org/pdf/2406.10208
Summary: Recently, Glyph-ByT5 has achieved highly accurate visual text renderingperformance in graphic design images. However, it still focuses solely onEnglish and performs relatively poorly in terms of visual appeal. In this work,we address these two fundamental limitations by presenting Glyph-ByT5-v2 andGlyph-SDXL-v2, which not only support accurate visual text rendering for 10different languages but also achieve much better aesthetic quality. To achievethis, we make the following contributions: (i) creating a high-qualitymultilingual glyph-text and graphic design dataset consisting of more than 1million glyph-text pairs and 10 million graphic design image-text pairscovering nine other languages, (ii) building a multilingual visual paragraphbenchmark consisting of 1,000 prompts, with 100 for each language, to assessmultilingual visual spelling accuracy, and (iii) leveraging the lateststep-aware preference learning approach to enhance the visual aestheticquality. With the combination of these techniques, we deliver a powerfulcustomized multilingual text encoder, Glyph-ByT5-v2, and a strong aestheticgraphic generation model, Glyph-SDXL-v2, that can support accurate spelling in10 different languages. We perceive our work as a significant advancement,considering that the latest DALL-E3 and Ideogram 1.0 still struggle with themultilingual visual text rendering task.
---
Title: 4M-21: An Any-to-Any Vision Model for Tens of Tasks and Modalities
PDF: https://arxiv.org/pdf/2406.09406
Summary: Current multimodal and multitask foundation models like 4M or UnifiedIO showpromising results, but in practice their out-of-the-box abilities to acceptdiverse inputs and perform diverse tasks are limited by the (usually rathersmall) number of modalities and tasks they are trained on. In this paper, weexpand upon the capabilities of them by training a single model on tens ofhighly diverse modalities and by performing co-training on large-scalemultimodal datasets and text corpora. This includes training on severalsemantic and geometric modalities, feature maps from recent state of the artmodels like DINOv2 and ImageBind, pseudo labels of specialist models like SAMand 4DHumans, and a range of new modalities that allow for novel ways tointeract with the model and steer the generation, for example image metadata orcolor palettes. A crucial step in this process is performing discretetokenization on various modalities, whether they are image-like, neural networkfeature maps, vectors, structured data like instance segmentation or humanposes, or data that can be represented as text. Through this, we expand on theout-of-the-box capabilities of multimodal models and specifically show thepossibility of training one model to solve at least 3x more tasks/modalitiesthan existing ones and doing so without a loss in performance. This enablesmore fine-grained and controllable multimodal generation capabilities andallows us to study the distillation of models trained on diverse data andobjectives into a unified model. We successfully scale the training to a threebillion parameter model using tens of modalities and different datasets. Theresulting models and training code are open sourced at 4m.epfl.ch.
---
Title: Understanding Hallucinations in Diffusion Models through Mode Interpolation
PDF: https://arxiv.org/pdf/2406.09358
Summary: Colloquially speaking, image generation models based upon diffusion processesare frequently said to exhibit "hallucinations," samples that could never occurin the training data. But where do such hallucinations come from? In thispaper, we study a particular failure mode in diffusion models, which we termmode interpolation. Specifically, we find that diffusion models smoothly"interpolate" between nearby data modes in the training set, to generatesamples that are completely outside the support of the original trainingdistribution; this phenomenon leads diffusion models to generate artifacts thatnever existed in real data (i.e., hallucinations). We systematically study thereasons for, and the manifestation of this phenomenon. Through experiments on1D and 2D Gaussians, we show how a discontinuous loss landscape in thediffusion model's decoder leads to a region where any smooth approximation willcause such hallucinations. Through experiments on artificial datasets withvarious shapes, we show how hallucination leads to the generation ofcombinations of shapes that never existed. Finally, we show that diffusionmodels in fact know when they go out of support and hallucinate. This iscaptured by the high variance in the trajectory of the generated sample towardsthe final few backward sampling process. Using a simple metric to capture thisvariance, we can remove over 95% of hallucinations at generation time whileretaining 96% of in-support samples. We conclude our exploration by showing theimplications of such hallucination (and its removal) on the collapse (andstabilization) of recursive training on synthetic data with experiments onMNIST and 2D Gaussians dataset. We release our code athttps://github.com/locuslab/diffusion-model-hallucination.
---
Title: CMC-Bench: Towards a New Paradigm of Visual Signal Compression
PDF: https://arxiv.org/pdf/2406.09356
Summary: Ultra-low bitrate image compression is a challenging and demanding topic.With the development of Large Multimodal Models (LMMs), a Cross ModalityCompression (CMC) paradigm of Image-Text-Image has emerged. Compared withtraditional codecs, this semantic-level compression can reduce image data sizeto 0.1\% or even lower, which has strong potential applications. However, CMChas certain defects in consistency with the original image and perceptualquality. To address this problem, we introduce CMC-Bench, a benchmark of thecooperative performance of Image-to-Text (I2T) and Text-to-Image (T2I) modelsfor image compression. This benchmark covers 18,000 and 40,000 imagesrespectively to verify 6 mainstream I2T and 12 T2I models, including 160,000subjective preference scores annotated by human experts. At ultra-low bitrates,this paper proves that the combination of some I2T and T2I models has surpassedthe most advanced visual signal codecs; meanwhile, it highlights where LMMs canbe further optimized toward the compression task. We encourage LMM developersto participate in this test to promote the evolution of visual signal codecprotocols.
---
Title: MLKV: Multi-Layer Key-Value Heads for Memory Efficient Transformer Decoding
PDF: https://arxiv.org/pdf/2406.09297
Summary: Auto-regressive inference of transformers benefit greatly from Key-Value (KV)caching, but can lead to major memory bottlenecks as model size, batch size,and sequence length grow at scale. We introduce Multi-Layer Key-Value (MLKV)sharing, a novel approach extending KV sharing across transformer layers toreduce memory usage beyond what was possible with Multi-Query Attention (MQA)and Grouped-Query Attention (GQA). Evaluations on various NLP benchmarks andinference metrics using uptrained Pythia-160M variants demonstrate that MLKVsignificantly reduces memory usage with minimal performance loss, reducing KVcache size down to a factor of 6x compared to MQA. These results highlightMLKV's potential for efficient deployment of transformer models at scale. Weprovide code at https://github.com/zaydzuhri/pythia-mlkv
---
Title: Estimating the Hallucination Rate of Generative AI
PDF: https://arxiv.org/pdf/2406.07457
Summary: This work is about estimating the hallucination rate for in-context learning(ICL) with Generative AI. In ICL, a conditional generative model (CGM) isprompted with a dataset and asked to make a prediction based on that dataset.The Bayesian interpretation of ICL assumes that the CGM is calculating aposterior predictive distribution over an unknown Bayesian model of a latentparameter and data. With this perspective, we define a hallucinationas a generated prediction that has low-probability under the true latentparameter. We develop a new method that takes an ICL problem -- that is, a CGM,a dataset, and a prediction question -- and estimates the probability that aCGM will generate a hallucination. Our method only requires generating queriesand responses from the model and evaluating its response log probability. Weempirically evaluate our method on synthetic regression and natural languageICL tasks using large language models.
---
Title: CVQA: Culturally-diverse Multilingual Visual Question Answering Benchmark
PDF: https://arxiv.org/pdf/2406.05967
Summary: Visual Question Answering (VQA) is an important task in multimodal AI, and itis often used to test the ability of vision-language models to understand andreason on knowledge present in both visual and textual data. However, most ofthe current VQA models use datasets that are primarily focused on English and afew major world languages, with images that are typically Western-centric.While recent efforts have tried to increase the number of languages covered onVQA datasets, they still lack diversity in low-resource languages. Moreimportantly, although these datasets often extend their linguistic range viatranslation or some other approaches, they usually keep images the same,resulting in narrow cultural representation. To address these limitations, weconstruct CVQA, a new Culturally-diverse multilingual Visual Question Answeringbenchmark, designed to cover a rich set of languages and cultures, where weengage native speakers and cultural experts in the data collection process. Asa result, CVQA includes culturally-driven images and questions from across 28countries on four continents, covering 26 languages with 11 scripts, providinga total of 9k questions. We then benchmark several Multimodal Large LanguageModels (MLLMs) on CVQA, and show that the dataset is challenging for thecurrent state-of-the-art models. This benchmark can serve as a probingevaluation suite for assessing the cultural capability and bias of multimodalmodels and hopefully encourage more research efforts toward increasing culturalawareness and linguistic diversity in this field.
---
Title: mOSCAR: A Large-scale Multilingual and Multimodal Document-level Corpus
PDF: https://arxiv.org/pdf/2406.08707
Summary: Multimodal Large Language Models (mLLMs) are trained on a large amount oftext-image data. While most mLLMs are trained on caption-like data only,Alayrac et al. [2022] showed that additionally training them on interleavedsequences of text and images can lead to the emergence of in-context learningcapabilities. However, the dataset they used, M3W, is not public and is only inEnglish. There have been attempts to reproduce their results but the releaseddatasets are English-only. In contrast, current multilingual and multimodaldatasets are either composed of caption-like only or medium-scale or fullyprivate data. This limits mLLM research for the 7,000 other languages spoken inthe world. We therefore introduce mOSCAR, to the best of our knowledge thefirst large-scale multilingual and multimodal document corpus crawled from theweb. It covers 163 languages, 315M documents, 214B tokens and 1.2B images. Wecarefully conduct a set of filtering and evaluation steps to make sure mOSCARis sufficiently safe, diverse and of good quality. We additionally train twotypes of multilingual model to prove the benefits of mOSCAR: (1) a modeltrained on a subset of mOSCAR and captioning data and (2) a model train oncaptioning data only. The model additionally trained on mOSCAR shows a strongboost in few-shot learning performance across various multilingual image-texttasks and benchmarks, confirming previous findings for English-only mLLMs.
---
Title: Toffee: Efficient Million-Scale Dataset Construction for Subject-Driven Text-to-Image Generation
PDF: https://arxiv.org/pdf/2406.09305
Summary: In subject-driven text-to-image generation, recent works have achievedsuperior performance by training the model on synthetic datasets containingnumerous image pairs. Trained on these datasets, generative models can producetext-aligned images for specific subject from arbitrary testing image in azero-shot manner. They even outperform methods which require additionalfine-tuning on testing images. However, the cost of creating such datasets isprohibitive for most researchers. To generate a single training pair, currentmethods fine-tune a pre-trained text-to-image model on the subject image tocapture fine-grained details, then use the fine-tuned model to create imagesfor the same subject based on creative text prompts. Consequently, constructinga large-scale dataset with millions of subjects can require hundreds ofthousands of GPU hours. To tackle this problem, we propose Toffee, an efficientmethod to construct datasets for subject-driven editing and generation.Specifically, our dataset construction does not need any subject-levelfine-tuning. After pre-training two generative models, we are able to generateinfinite number of high-quality samples. We construct the first large-scaledataset for subject-driven image editing and generation, which contains 5million image pairs, text prompts, and masks. Our dataset is 5 times the sizeof previous largest dataset, yet our cost is tens of thousands of GPU hourslower. To test the proposed dataset, we also propose a model which is capableof both subject-driven image editing and generation. By simply training themodel on our proposed dataset, it obtains competitive results, illustrating theeffectiveness of the proposed dataset construction framework.
---
Title: Commonsense-T2I Challenge: Can Text-to-Image Generation Models Understand Commonsense?
PDF: https://arxiv.org/pdf/2406.07546
Summary: We present a novel task and benchmark for evaluating the ability oftext-to-image(T2I) generation models to produce images that fit commonsense inreal life, which we call Commonsense-T2I. Given two adversarial text promptscontaining an identical set of action words with minor differences, such as "alightbulb without electricity" v.s. "a lightbulb with electricity", we evaluatewhether T2I models can conduct visual-commonsense reasoning, e.g. produceimages that fit "the lightbulb is unlit" vs. "the lightbulb is lit"correspondingly. Commonsense-T2I presents an adversarial challenge, providingpairwise text prompts along with expected outputs. The dataset is carefullyhand-curated by experts and annotated with fine-grained labels, such ascommonsense type and likelihood of the expected outputs, to assist analyzingmodel behavior. We benchmark a variety of state-of-the-art (sota) T2I modelsand surprisingly find that, there is still a large gap between image synthesisand real life photos--even the DALL-E 3 model could only achieve 48.92% onCommonsense-T2I, and the stable diffusion XL model only achieves 24.92%accuracy. Our experiments show that GPT-enriched prompts cannot solve thischallenge, and we include a detailed analysis about possible reasons for suchdeficiency. We aim for Commonsense-T2I to serve as a high-quality evaluationbenchmark for T2I commonsense checking, fostering advancements in real lifeimage generation.
---
Title: MuirBench: A Comprehensive Benchmark for Robust Multi-image Understanding
PDF: https://arxiv.org/pdf/2406.09411
Summary: We introduce MuirBench, a comprehensive benchmark that focuses on robustmulti-image understanding capabilities of multimodal LLMs. MuirBench consistsof 12 diverse multi-image tasks (e.g., scene understanding, ordering) thatinvolve 10 categories of multi-image relations (e.g., multiview, temporalrelations). Comprising 11,264 images and 2,600 multiple-choice questions,MuirBench is created in a pairwise manner, where each standard instance ispaired with an unanswerable variant that has minimal semantic differences, inorder for a reliable assessment. Evaluated upon 20 recent multi-modal LLMs, ourresults reveal that even the best-performing models like GPT-4o and Gemini Profind it challenging to solve MuirBench, achieving 68.0% and 49.3% in accuracy.Open-source multimodal LLMs trained on single images can hardly generalize tomulti-image questions, hovering below 33.3% in accuracy. These resultshighlight the importance of MuirBench in encouraging the community to developmultimodal LLMs that can look beyond a single image, suggesting potentialpathways for future improvements.
---
Title: EMMA: Your Text-to-Image Diffusion Model Can Secretly Accept Multi-Modal Prompts
PDF: https://arxiv.org/pdf/2406.09162
Summary: Recent advancements in image generation have enabled the creation ofhigh-quality images from text conditions. However, when facing multi-modalconditions, such as text combined with reference appearances, existing methodsstruggle to balance multiple conditions effectively, typically showing apreference for one modality over others. To address this challenge, weintroduce EMMA, a novel image generation model accepting multi-modal promptsbuilt upon the state-of-the-art text-to-image (T2I) diffusion model, ELLA. EMMAseamlessly incorporates additional modalities alongside text to guide imagegeneration through an innovative Multi-modal Feature Connector design, whicheffectively integrates textual and supplementary modal information using aspecial attention mechanism. By freezing all parameters in the original T2Idiffusion model and only adjusting some additional layers, we reveal aninteresting finding that the pre-trained T2I diffusion model can secretlyaccept multi-modal prompts. This interesting property facilitates easyadaptation to different existing frameworks, making EMMA a flexible andeffective tool for producing personalized and context-aware images and evenvideos. Additionally, we introduce a strategy to assemble learned EMMA modulesto produce images conditioned on multiple modalities simultaneously,eliminating the need for additional training with mixed multi-modal prompts.Extensive experiments demonstrate the effectiveness of EMMA in maintaining highfidelity and detail in generated images, showcasing its potential as a robustsolution for advanced multi-modal conditional image generation tasks.
---
Title: LRM-Zero: Training Large Reconstruction Models with Synthesized Data
PDF: https://arxiv.org/pdf/2406.09371
Summary: We present LRM-Zero, a Large Reconstruction Model (LRM) trained entirely onsynthesized 3D data, achieving high-quality sparse-view 3D reconstruction. Thecore of LRM-Zero is our procedural 3D dataset, Zeroverse, which isautomatically synthesized from simple primitive shapes with random texturingand augmentations (e.g., height fields, boolean differences, and wireframes).Unlike previous 3D datasets (e.g., Objaverse) which are often captured orcrafted by humans to approximate real 3D data, Zeroverse completely ignoresrealistic global semantics but is rich in complex geometric and texture detailsthat are locally similar to or even more intricate than real objects. Wedemonstrate that our LRM-Zero, trained with our fully synthesized Zeroverse,can achieve high visual quality in the reconstruction of real-world objects,competitive with models trained on Objaverse. We also analyze several criticaldesign choices of Zeroverse that contribute to LRM-Zero's capability andtraining stability. Our work demonstrates that 3D reconstruction, one of thecore tasks in 3D vision, can potentially be addressed without the semantics ofreal-world objects. The Zeroverse's procedural synthesis code and interactivevisualization are available at: https://desaixie.github.io/lrm-zero/.
---
Title: Language Model Council: Benchmarking Foundation Models on Highly Subjective Tasks by Consensus
PDF: https://arxiv.org/pdf/2406.08598
Summary: The rapid advancement of Large Language Models (LLMs) necessitates robust andchallenging benchmarks. Leaderboards like Chatbot Arena rank LLMs based on howwell their responses align with human preferences. However, many tasks such asthose related to emotional intelligence, creative writing, or persuasiveness,are highly subjective and often lack majoritarian human agreement. Judges mayhave irreconcilable disagreements about what constitutes a better response. Toaddress the challenge of ranking LLMs on highly subjective tasks, we propose anovel benchmarking framework, the Language Model Council (LMC). The LMCoperates through a democratic process to: 1) formulate a test set through equalparticipation, 2) administer the test among council members, and 3) evaluateresponses as a collective jury. We deploy a council of 20 newest LLMs on anopen-ended emotional intelligence task: responding to interpersonal dilemmas.Our results show that the LMC produces rankings that are more separable,robust, and less biased than those from any individual LLM judge, and is moreconsistent with a human-established leaderboard compared to other benchmarks.
---
Title: Real3D: Scaling Up Large Reconstruction Models with Real-World Images
PDF: https://arxiv.org/pdf/2406.08479
Summary: The default strategy for training single-view Large Reconstruction Models(LRMs) follows the fully supervised route using large-scale datasets ofsynthetic 3D assets or multi-view captures. Although these resources simplifythe training procedure, they are hard to scale up beyond the existing datasetsand they are not necessarily representative of the real distribution of objectshapes. To address these limitations, in this paper, we introduce Real3D, thefirst LRM system that can be trained using single-view real-world images.Real3D introduces a novel self-training framework that can benefit from boththe existing synthetic data and diverse single-view real images. We propose twounsupervised losses that allow us to supervise LRMs at the pixel- andsemantic-level, even for training examples without ground-truth 3D or novelviews. To further improve performance and scale up the image data, we developan automatic data curation approach to collect high-quality examples fromin-the-wild images. Our experiments show that Real3D consistently outperformsprior work in four diverse evaluation settings that include real and syntheticdata, as well as both in-domain and out-of-domain shapes. Code and model can befound here: https://hwjiang1510.github.io/Real3D/
---
Title: Samba: Simple Hybrid State Space Models for Efficient Unlimited Context Language Modeling
PDF: https://arxiv.org/pdf/2406.07522
Summary: Efficiently modeling sequences with infinite context length has been along-standing problem. Past works suffer from either the quadratic computationcomplexity or the limited extrapolation ability on length generalization. Inthis work, we present Samba, a simple hybrid architecture that layer-wisecombines Mamba, a selective State Space Model (SSM), with Sliding WindowAttention (SWA). Samba selectively compresses a given sequence into recurrenthidden states while still maintaining the ability to precisely recall memorieswith the attention mechanism. We scale Samba up to 3.8B parameters with 3.2Ttraining tokens and show that Samba substantially outperforms thestate-of-the-art models based on pure attention or SSMs on a wide range ofbenchmarks. When trained on 4K length sequences, Samba can be efficientlyextrapolated to 256K context length with perfect memory recall and showimproved token predictions up to 1M context length. As a linear-time sequencemodel, Samba enjoys a 3.73x higher throughput compared to Transformers withgrouped-query attention when processing user prompts of 128K length, and 3.64xspeedup when generating 64K tokens with unlimited streaming. A sampleimplementation of Samba is publicly available inhttps://github.com/microsoft/Samba.
---
Title: TC-Bench: Benchmarking Temporal Compositionality in Text-to-Video and Image-to-Video Generation
PDF: https://arxiv.org/pdf/2406.08656
Summary: Video generation has many unique challenges beyond those of image generation.The temporal dimension introduces extensive possible variations across frames,over which consistency and continuity may be violated. In this study, we movebeyond evaluating simple actions and argue that generated videos shouldincorporate the emergence of new concepts and their relation transitions likein real-world videos as time progresses. To assess the TemporalCompositionality of video generation models, we propose TC-Bench, a benchmarkof meticulously crafted text prompts, corresponding ground truth videos, androbust evaluation metrics. The prompts articulate the initial and final statesof scenes, effectively reducing ambiguities for frame development andsimplifying the assessment of transition completion. In addition, by collectingaligned real-world videos corresponding to the prompts, we expand TC-Bench'sapplicability from text-conditional models to image-conditional ones that canperform generative frame interpolation. We also develop new metrics to measurethe completeness of component transitions in generated videos, whichdemonstrate significantly higher correlations with human judgments thanexisting metrics. Our comprehensive experimental results reveal that most videogenerators achieve less than 20% of the compositional changes, highlightingenormous space for future improvement. Our analysis indicates that currentvideo generation models struggle to interpret descriptions of compositionalchanges and synthesize various components across different time steps.
---
