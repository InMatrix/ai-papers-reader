Title: Deep Bayesian Active Learning for Preference Modeling in Large Language Models
PDF: https://arxiv.org/pdf/2406.10023
Summary: Leveraging human preferences for steering the behavior of Large LanguageModels (LLMs) has demonstrated notable success in recent years. Nonetheless,data selection and labeling are still a bottleneck for these systems,particularly at large scale. Hence, selecting the most informative points foracquiring human feedback may considerably reduce the cost of preferencelabeling and unleash the further development of LLMs. Bayesian Active Learningprovides a principled framework for addressing this challenge and hasdemonstrated remarkable success in diverse settings. However, previous attemptsto employ it for Preference Modeling did not meet such expectations. In thiswork, we identify that naive epistemic uncertainty estimation leads to theacquisition of redundant samples. We address this by proposing the BayesianActive Learner for Preference Modeling (BAL-PM), a novel stochastic acquisitionpolicy that not only targets points of high epistemic uncertainty according tothe preference model but also seeks to maximize the entropy of the acquiredprompt distribution in the feature space spanned by the employed LLM. Notably,our experiments demonstrate that BAL-PM requires 33% to 68% fewer preferencelabels in two popular human preference datasets and exceeds previous stochasticBayesian acquisition policies.
---
Title: Humor in AI: Massive Scale Crowd-Sourced Preferences and Benchmarks for Cartoon Captioning
PDF: https://arxiv.org/pdf/2406.10522
Summary: We present a novel multimodal preference dataset for creative tasks,consisting of over 250 million human ratings on more than 2.2 million captions,collected through crowdsourcing rating data for The New Yorker's weekly cartooncaption contest over the past eight years. This unique dataset supports thedevelopment and evaluation of multimodal large language models andpreference-based fine-tuning algorithms for humorous caption generation. Wepropose novel benchmarks for judging the quality of model-generated captions,utilizing both GPT4 and human judgments to establish ranking-based evaluationstrategies. Our experimental results highlight the limitations of currentfine-tuning methods, such as RLHF and DPO, when applied to creative tasks.Furthermore, we demonstrate that even state-of-the-art models like GPT4 andClaude currently underperform top human contestants in generating humorouscaptions. As we conclude this extensive data collection effort, we release theentire preference dataset to the research community, fostering furtheradvancements in AI humor generation and evaluation.
---
Title: Unifying Multimodal Retrieval via Document Screenshot Embedding
PDF: https://arxiv.org/pdf/2406.11251
Summary: In the real world, documents are organized in different formats and variedmodalities. Traditional retrieval pipelines require tailored document parsingtechniques and content extraction modules to prepare input for indexing. Thisprocess is tedious, prone to errors, and has information loss. To this end, wepropose Document Screenshot Embedding} (DSE), a novel retrieval paradigm thatregards document screenshots as a unified input format, which does not requireany content extraction preprocess and preserves all the information in adocument (e.g., text, image and layout). DSE leverages a large vision-languagemodel to directly encode document screenshots into dense representations forretrieval. To evaluate our method, we first craft the dataset of Wiki-SS, a1.3M Wikipedia web page screenshots as the corpus to answer the questions fromthe Natural Questions dataset. In such a text-intensive document retrievalsetting, DSE shows competitive effectiveness compared to other text retrievalmethods relying on parsing. For example, DSE outperforms BM25 by 17 points intop-1 retrieval accuracy. Additionally, in a mixed-modality task of slideretrieval, DSE significantly outperforms OCR text retrieval methods by over 15points in nDCG@10. These experiments show that DSE is an effective documentretrieval paradigm for diverse types of documents. Model checkpoints, code, andWiki-SS collection will be released.
---
Title: A Simple and Effective $L_2$ Norm-Based Strategy for KV Cache Compression
PDF: https://arxiv.org/pdf/2406.11430
Summary: The deployment of large language models (LLMs) is often hindered by theextensive memory requirements of the Key-Value (KV) cache, especially ascontext lengths increase. Existing approaches to reduce the KV cache sizeinvolve either fine-tuning the model to learn a compression strategy orleveraging attention scores to reduce the sequence length. We analyse theattention distributions in decoder-only Transformers-based models and observethat attention allocation patterns stay consistent across most layers.Surprisingly, we find a clear correlation between the L_2 and the attentionscores over cached KV pairs, where a low L_2 of a key embedding usually leadsto a high attention score during decoding. This finding indicates that theinfluence of a KV pair is potentially determined by the key embedding itselfbefore being queried. Based on this observation, we compress the KV cache basedon the L_2 of key embeddings. Our experimental results show that this simplestrategy can reduce the KV cache size by 50% on language modelling andneedle-in-a-haystack tasks and 90% on passkey retrieval tasks without losingaccuracy.
---
Title: Pandora: Towards General World Model with Natural Language Actions and Video States
PDF: https://arxiv.org/pdf/2406.09455
Summary: World models simulate future states of the world in response to differentactions. They facilitate interactive content creation and provides a foundationfor grounded, long-horizon reasoning. Current foundation models do not fullymeet the capabilities of general world models: large language models (LLMs) areconstrained by their reliance on language modality and their limitedunderstanding of the physical world, while video models lack interactive actioncontrol over the world simulations. This paper makes a step towards building ageneral world model by introducing Pandora, a hybrid autoregressive-diffusionmodel that simulates world states by generating videos and allows real-timecontrol with free-text actions. Pandora achieves domain generality, videoconsistency, and controllability through large-scale pretraining andinstruction tuning. Crucially, Pandora bypasses the cost oftraining-from-scratch by integrating a pretrained LLM (7B) and a pretrainedvideo model, requiring only additional lightweight finetuning. We illustrateextensive outputs by Pandora across diverse domains (indoor/outdoor,natural/urban, human/robot, 2D/3D, etc.). The results indicate great potentialof building stronger general world models with larger-scale training.
---
Title: HiddenTables & PyQTax: A Cooperative Game and Dataset For TableQA to Ensure Scale and Data Privacy Across a Myriad of Taxonomies
PDF: https://arxiv.org/pdf/2406.10803
Summary: A myriad of different Large Language Models (LLMs) face a common challenge incontextually analyzing table question-answering tasks. These challenges areengendered from (1) finite context windows for large tables, (2) multi-faceteddiscrepancies amongst tokenization patterns against cell boundaries, and (3)various limitations stemming from data confidentiality in the process of usingexternal models such as gpt-3.5-turbo. We propose a cooperative game dubbed"HiddenTables" as a potential resolution to this challenge. In essence,"HiddenTables" is played between the code-generating LLM "Solver" and the"Oracle" which evaluates the ability of the LLM agents to solve Table QA tasks.This game is based on natural language schemas and importantly, ensures thesecurity of the underlying data. We provide evidential experiments on a diverseset of tables that demonstrate an LLM's collective inability to generalize andperform on complex queries, handle compositional dependencies, and alignnatural language to programmatic commands when concrete table schemas areprovided. Unlike encoder-based models, we have pushed the boundaries of"HiddenTables" to not be limited by the number of rows - therefore we exhibitimproved efficiency in prompt and completion tokens. Our infrastructure hasspawned a new dataset "PyQTax" that spans across 116,671 question-table-answertriplets and provides additional fine-grained breakdowns & labels for varyingquestion taxonomies. Therefore, in tandem with our academic contributionsregarding LLMs' deficiency in TableQA tasks, "HiddenTables" is a tactilemanifestation of how LLMs can interact with massive datasets while ensuringdata security and minimizing generation costs.
---
Title: In-Context Editing: Learning Knowledge from Self-Induced Distributions
PDF: https://arxiv.org/pdf/2406.11194
Summary: The existing fine-tuning paradigm for language models is brittle in knowledgeediting scenarios, where the model must incorporate new information withoutextensive retraining. This brittleness often results in overfitting, reducedperformance, and unnatural language generation. To address this, we proposeConsistent In-Context Editing (ICE), a novel approach that leverages themodel's in-context learning capability to tune toward a contextual distributionrather than a one-hot target. ICE introduces a straightforward optimizationframework that includes both a target and a procedure, enhancing the robustnessand effectiveness of gradient-based tuning methods. We provide analyticalinsights into ICE across four critical aspects of knowledge editing: accuracy,locality, generalization, and linguistic quality, showing its advantages.Experimental results across four datasets confirm the effectiveness of ICE anddemonstrate its potential for continual editing, ensuring that updatedinformation is incorporated while preserving the integrity of the model.
---
Title: Consistency^2: Consistent and Fast 3D Painting with Latent Consistency Models
PDF: https://arxiv.org/pdf/2406.11202
Summary: Generative 3D Painting is among the top productivity boosters inhigh-resolution 3D asset management and recycling. Ever since text-to-imagemodels became accessible for inference on consumer hardware, the performance of3D Painting methods has consistently improved and is currently close toplateauing. At the core of most such models lies denoising diffusion in thelatent space, an inherently time-consuming iterative process. Multipletechniques have been developed recently to accelerate generation and reducesampling iterations by orders of magnitude. Designed for 2D generative imaging,these techniques do not come with recipes for lifting them into 3D. In thispaper, we address this shortcoming by proposing a Latent Consistency Model(LCM) adaptation for the task at hand. We analyze the strengths and weaknessesof the proposed model and evaluate it quantitatively and qualitatively. Basedon the Objaverse dataset samples study, our 3D painting method attains strongpreference in all evaluations. Source code is available athttps://github.com/kongdai123/consistency2.
---
Title: Just How Flexible are Neural Networks in Practice?
PDF: https://arxiv.org/pdf/2406.11463
Summary: It is widely believed that a neural network can fit a training set containingat least as many samples as it has parameters, underpinning notions ofoverparameterized and underparameterized models. In practice, however, we onlyfind solutions accessible via our training procedure, including the optimizerand regularizers, limiting flexibility. Moreover, the exact parameterization ofthe function class, built into an architecture, shapes its loss surface andimpacts the minima we find. In this work, we examine the ability of neuralnetworks to fit data in practice. Our findings indicate that: (1) standardoptimizers find minima where the model can only fit training sets withsignificantly fewer samples than it has parameters; (2) convolutional networksare more parameter-efficient than MLPs and ViTs, even on randomly labeled data;(3) while stochastic training is thought to have a regularizing effect, SGDactually finds minima that fit more training data than full-batch gradientdescent; (4) the difference in capacity to fit correctly labeled andincorrectly labeled samples can be predictive of generalization; (5) ReLUactivation functions result in finding minima that fit more data despite beingdesigned to avoid vanishing and exploding gradients in deep architectures.
---
Title: Exploring the Role of Large Language Models in Prompt Encoding for Diffusion Models
PDF: https://arxiv.org/pdf/2406.11831
Summary: Large language models (LLMs) based on decoder-only transformers havedemonstrated superior text understanding capabilities compared to CLIP andT5-series models. However, the paradigm for utilizing current advanced LLMs intext-to-image diffusion models remains to be explored. We observed an unusualphenomenon: directly using a large language model as the prompt encodersignificantly degrades the prompt-following ability in image generation. Weidentified two main obstacles behind this issue. One is the misalignmentbetween the next token prediction training in LLM and the requirement fordiscriminative prompt features in diffusion models. The other is the intrinsicpositional bias introduced by the decoder-only architecture. To deal with thisissue, we propose a novel framework to fully harness the capabilities of LLMs.Through the carefully designed usage guidance, we effectively enhance the textrepresentation capability for prompt encoding and eliminate its inherentpositional bias. This allows us to integrate state-of-the-art LLMs into thetext-to-image generation model flexibly. Furthermore, we also provide aneffective manner to fuse multiple LLMs into our framework. Considering theexcellent performance and scaling capabilities demonstrated by the transformerarchitecture, we further design an LLM-Infused Diffusion Transformer (LI-DiT)based on the framework. We conduct extensive experiments to validate LI-DiTacross model size and data size. Benefiting from the inherent ability of theLLMs and our innovative designs, the prompt understanding performance of LI-DiTeasily surpasses state-of-the-art open-source models as well as mainstreamclosed-source commercial models including Stable Diffusion 3, DALL-E 3, andMidjourney V6. The powerful LI-DiT-10B will be available after furtheroptimization and security checks.
---
Title: MMDU: A Multi-Turn Multi-Image Dialog Understanding Benchmark and Instruction-Tuning Dataset for LVLMs
PDF: https://arxiv.org/pdf/2406.11833
Summary: Generating natural and meaningful responses to communicate with multi-modalhuman inputs is a fundamental capability of Large Vision-LanguageModels(LVLMs). While current open-source LVLMs demonstrate promisingperformance in simplified scenarios such as single-turn single-image input,they fall short in real-world conversation scenarios such as followinginstructions in a long context history with multi-turn and multi-images.Existing LVLM benchmarks primarily focus on single-choice questions orshort-form responses, which do not adequately assess the capabilities of LVLMsin real-world human-AI interaction applications. Therefore, we introduce MMDU,a comprehensive benchmark, and MMDU-45k, a large-scale instruction tuningdataset, designed to evaluate and improve LVLMs' abilities in multi-turn andmulti-image conversations. We employ the clustering algorithm to ffnd therelevant images and textual descriptions from the open-source Wikipedia andconstruct the question-answer pairs by human annotators with the assistance ofthe GPT-4o model. MMDU has a maximum of 18k image+text tokens, 20 images, and27 turns, which is at least 5x longer than previous benchmarks and poseschallenges to current LVLMs. Our in-depth analysis of 15 representative LVLMsusing MMDU reveals that open-source LVLMs lag behind closed-source counterpartsdue to limited conversational instruction tuning data. We demonstrate thatffne-tuning open-source LVLMs on MMDU-45k signiffcantly address this gap,generating longer and more accurate conversations, and improving scores on MMDUand existing benchmarks (MMStar: +1.1%, MathVista: +1.5%, ChartQA:+1.2%). Ourcontributions pave the way for bridging the gap between current LVLM models andreal-world application demands. This project is available athttps://github.com/Liuziyu77/MMDU.
---
Title: CoLoR-Filter: Conditional Loss Reduction Filtering for Targeted Language Model Pre-training
PDF: https://arxiv.org/pdf/2406.10670
Summary: Selecting high-quality data for pre-training is crucial in shaping thedownstream task performance of language models. A major challenge lies inidentifying this optimal subset, a problem generally considered intractable,thus necessitating scalable and effective heuristics. In this work, we proposea data selection method, CoLoR-Filter (Conditional Loss Reduction Filtering),which leverages an empirical Bayes-inspired approach to derive a simple andcomputationally efficient selection criterion based on the relative loss valuesof two auxiliary models.  In addition to the modeling rationale, we evaluate CoLoR-Filter empiricallyon two language modeling tasks: (1) selecting data from C4 for domainadaptation to evaluation on Books and (2) selecting data from C4 for a suite ofdownstream multiple-choice question answering tasks. We demonstrate favorablescaling both as we subselect more aggressively and using small auxiliary modelsto select data for large target models. As one headline result, CoLoR-Filterdata selected using a pair of 150m parameter auxiliary models can train a 1.2bparameter target model to match a 1.2b parameter model trained on 25b randomlyselected tokens with 25x less data for Books and 11x less data for thedownstream tasks.  Code: https://github.com/davidbrandfonbrener/color-filter-olmo  Filtered data:https://huggingface.co/datasets/davidbrandfonbrener/color-filtered-c4
---
Title: How Do Large Language Models Acquire Factual Knowledge During Pretraining?
PDF: https://arxiv.org/pdf/2406.11813
Summary: Despite the recent observation that large language models (LLMs) can storesubstantial factual knowledge, there is a limited understanding of themechanisms of how they acquire factual knowledge through pretraining. This workaddresses this gap by studying how LLMs acquire factual knowledge duringpretraining. The findings reveal several important insights into the dynamicsof factual knowledge acquisition during pretraining. First, counterintuitively,we observe that pretraining on more data shows no significant improvement inthe model's capability to acquire and maintain factual knowledge. Next, thereis a power-law relationship between training steps and forgetting ofmemorization and generalization of factual knowledge, and LLMs trained withduplicated training data exhibit faster forgetting. Third, training LLMs withlarger batch sizes can enhance the models' robustness to forgetting. Overall,our observations suggest that factual knowledge acquisition in LLM pretrainingoccurs by progressively increasing the probability of factual knowledgepresented in the pretraining data at each step. However, this increase isdiluted by subsequent forgetting. Based on this interpretation, we demonstratethat we can provide plausible explanations for recently observed behaviors ofLLMs, such as the poor performance of LLMs on long-tail knowledge and thebenefits of deduplicating the pretraining corpus.
---
Title: LLaNA: Large Language and NeRF Assistant
PDF: https://arxiv.org/pdf/2406.11840
Summary: Multimodal Large Language Models (MLLMs) have demonstrated an excellentunderstanding of images and 3D data. However, both modalities have shortcomingsin holistically capturing the appearance and geometry of objects. Meanwhile,Neural Radiance Fields (NeRFs), which encode information within the weights ofa simple Multi-Layer Perceptron (MLP), have emerged as an increasinglywidespread modality that simultaneously encodes the geometry and photorealisticappearance of objects. This paper investigates the feasibility andeffectiveness of ingesting NeRF into MLLM. We create LLaNA, the firstgeneral-purpose NeRF-language assistant capable of performing new tasks such asNeRF captioning and Q\&A. Notably, our method directly processes the weights ofthe NeRF's MLP to extract information about the represented objects without theneed to render images or materialize 3D data structures. Moreover, we build adataset of NeRFs with text annotations for various NeRF-language tasks with nohuman intervention. Based on this dataset, we develop a benchmark to evaluatethe NeRF understanding capability of our method. Results show that processingNeRF weights performs favourably against extracting 2D or 3D representationsfrom NeRFs.
---
Title: MINT-1T: Scaling Open-Source Multimodal Data by 10x: A Multimodal Dataset with One Trillion Tokens
PDF: https://arxiv.org/pdf/2406.11271
Summary: Multimodal interleaved datasets featuring free-form interleaved sequences ofimages and text are crucial for training frontier large multimodal models(LMMs). Despite the rapid progression of open-source LMMs, there remains apronounced scarcity of large-scale, diverse open-source multimodal interleaveddatasets. In response, we introduce MINT-1T, the most extensive and diverseopen-source Multimodal INTerleaved dataset to date. MINT-1T comprises onetrillion text tokens and three billion images, a 10x scale-up from existingopen-source datasets. Additionally, we include previously untapped sources suchas PDFs and ArXiv papers. As scaling multimodal interleaved datasets requiressubstantial engineering effort, sharing the data curation process and releasingthe dataset greatly benefits the community. Our experiments show that LMMstrained on MINT-1T rival the performance of models trained on the previousleading dataset, OBELICS. Our data and code will be released athttps://github.com/mlfoundations/MINT-1T.
---
Title: Breaking the Attention Bottleneck
PDF: https://arxiv.org/pdf/2406.10906
Summary: Attention-based transformers have become the standard architecture in manydeep learning fields, primarily due to their ability to model long-rangedependencies and handle variable-length input sequences. However, the attentionmechanism with its quadratic complexity is a significant bottleneck in thetransformer architecture. This algorithm is only uni-directional in the decoderand converges to a static pattern in over-parametrized decoder-only models. Iaddress this issue by developing a generative function as attention oractivation replacement. It still has the auto-regressive character by comparingeach token with the previous one. In my test setting with nanoGPT this yields asmaller loss while having a smaller model. The loss further drops byincorporating an average context vector. This concept of attention replacementis distributed under the GNU AGPL v3 license athttps://gitlab.com/Bachstelze/causal_generation.
---
Title: Evaluating Open Language Models Across Task Types, Application Domains, and Reasoning Types: An In-Depth Experimental Analysis
PDF: https://arxiv.org/pdf/2406.11402
Summary: The rapid rise of Language Models (LMs) has expanded their use in severalapplications. Yet, due to constraints of model size, associated cost, orproprietary restrictions, utilizing state-of-the-art (SOTA) LLMs is not alwaysfeasible. With open, smaller LMs emerging, more applications can leverage theircapabilities, but selecting the right LM can be challenging. This work conductsan in-depth experimental analysis of the semantic correctness of outputs of 10smaller, open LMs across three aspects: task types, application domains andreasoning types, using diverse prompt styles. We demonstrate that mosteffective models and prompt styles vary depending on the specific requirements.Our analysis provides a comparative assessment of LMs and prompt styles using aproposed three-tier schema of aspects for their strategic selection based onuse-case and other constraints. We also show that if utilized appropriately,these LMs can compete with, and sometimes outperform, SOTA LLMs likeDeepSeek-v2, GPT-3.5-Turbo, and GPT-4o.
---
Title: Task Me Anything
PDF: https://arxiv.org/pdf/2406.11775
Summary: Benchmarks for large multimodal language models (MLMs) now serve tosimultaneously assess the general capabilities of models instead of evaluatingfor a specific capability. As a result, when a developer wants to identifywhich models to use for their application, they are overwhelmed by the numberof benchmarks and remain uncertain about which benchmark's results are mostreflective of their specific use case. This paper introduces Task-Me-Anything,a benchmark generation engine which produces a benchmark tailored to a user'sneeds. Task-Me-Anything maintains an extendable taxonomy of visual assets andcan programmatically generate a vast number of task instances. Additionally, italgorithmically addresses user queries regarding MLM performance efficientlywithin a computational budget. It contains 113K images, 10K videos, 2K 3Dobject assets, over 365 object categories, 655 attributes, and 335relationships. It can generate 750M image/video question-answering pairs, whichfocus on evaluating MLM perceptual capabilities. Task-Me-Anything revealscritical insights: open-source MLMs excel in object and attribute recognitionbut lack spatial and temporal understanding; each model exhibits uniquestrengths and weaknesses; larger models generally perform better, thoughexceptions exist; and GPT4o demonstrates challenges in recognizingrotating/moving objects and distinguishing colors.
---
Title: From Pixels to Prose: A Large Dataset of Dense Image Captions
PDF: https://arxiv.org/pdf/2406.10328
Summary: Training large vision-language models requires extensive, high-qualityimage-text pairs. Existing web-scraped datasets, however, are noisy and lackdetailed image descriptions. To bridge this gap, we introduce PixelProse, acomprehensive dataset of over 16M (million) synthetically generated captions,leveraging cutting-edge vision-language models for detailed and accuratedescriptions. To ensure data integrity, we rigorously analyze our dataset forproblematic content, including child sexual abuse material (CSAM), personallyidentifiable information (PII), and toxicity. We also provide valuable metadatasuch as watermark presence and aesthetic scores, aiding in further datasetfiltering. We hope PixelProse will be a valuable resource for futurevision-language research. PixelProse is available athttps://huggingface.co/datasets/tomg-group-umd/pixelprose
---
Title: VideoLLM-online: Online Video Large Language Model for Streaming Video
PDF: https://arxiv.org/pdf/2406.11816
Summary: Recent Large Language Models have been enhanced with vision capabilities,enabling them to comprehend images, videos, and interleaved vision-languagecontent. However, the learning methods of these large multimodal modelstypically treat videos as predetermined clips, making them less effective andefficient at handling streaming video inputs. In this paper, we propose a novelLearning-In-Video-Stream (LIVE) framework, which enables temporally aligned,long-context, and real-time conversation within a continuous video stream. OurLIVE framework comprises comprehensive approaches to achieve video streamingdialogue, encompassing: (1) a training objective designed to perform languagemodeling for continuous streaming inputs, (2) a data generation scheme thatconverts offline temporal annotations into a streaming dialogue format, and (3)an optimized inference pipeline to speed up the model responses in real-worldvideo streams. With our LIVE framework, we built VideoLLM-online model uponLlama-2/Llama-3 and demonstrate its significant advantages in processingstreaming videos. For instance, on average, our model can support streamingdialogue in a 5-minute video clip at over 10 FPS on an A100 GPU. Moreover, italso showcases state-of-the-art performance on public offline video benchmarks,such as recognition, captioning, and forecasting. The code, model, data, anddemo have been made available at https://showlab.github.io/videollm-online.
---
Title: THEANINE: Revisiting Memory Management in Long-term Conversations with Timeline-augmented Response Generation
PDF: https://arxiv.org/pdf/2406.10996
Summary: Large language models (LLMs) are capable of processing lengthy dialoguehistories during prolonged interaction with users without additional memorymodules; however, their responses tend to overlook or incorrectly recallinformation from the past. In this paper, we revisit memory-augmented responsegeneration in the era of LLMs. While prior work focuses on getting rid ofoutdated memories, we argue that such memories can provide contextual cues thathelp dialogue systems understand the development of past events and, therefore,benefit response generation. We present Theanine, a framework that augmentsLLMs' response generation with memory timelines -- series of memories thatdemonstrate the development and causality of relevant past events. Along withTheanine, we introduce TeaFarm, a counterfactual-driven question-answeringpipeline addressing the limitation of G-Eval in long-term conversations.Supplementary videos of our methods and the TeaBag dataset for TeaFarmevaluation are in https://theanine-693b0.web.app/.
---
Title: WPO: Enhancing RLHF with Weighted Preference Optimization
PDF: https://arxiv.org/pdf/2406.11827
Summary: Reinforcement learning from human feedback (RLHF) is a promising solution toalign large language models (LLMs) more closely with human values. Off-policypreference optimization, where the preference data is obtained from othermodels, is widely adopted due to its cost efficiency and scalability. However,off-policy preference optimization often suffers from a distributional gapbetween the policy used for data collection and the target policy, leading tosuboptimal optimization. In this paper, we propose a novel strategy to mitigatethis problem by simulating on-policy learning with off-policy preference data.Our Weighted Preference Optimization (WPO) method adapts off-policy data toresemble on-policy data more closely by reweighting preference pairs accordingto their probability under the current policy. This method not only addressesthe distributional gap problem but also enhances the optimization processwithout incurring additional costs. We validate our method on instructionfollowing benchmarks including Alpaca Eval 2 and MT-bench. WPO not onlyoutperforms Direct Preference Optimization (DPO) by up to 5.6% on Alpaca Eval 2but also establishes a remarkable length-controlled winning rate againstGPT-4-turbo of 48.6% based on Llama-3-8B-Instruct, making it the strongest 8Bmodel on the leaderboard. We will release the code and models athttps://github.com/wzhouad/WPO.
---
Title: WildVision: Evaluating Vision-Language Models in the Wild with Human Preferences
PDF: https://arxiv.org/pdf/2406.11069
Summary: Recent breakthroughs in vision-language models (VLMs) emphasize the necessityof benchmarking human preferences in real-world multimodal interactions. Toaddress this gap, we launched WildVision-Arena (WV-Arena), an online platformthat collects human preferences to evaluate VLMs. We curated WV-Bench byselecting 500 high-quality samples from 8,000 user submissions in WV-Arena.WV-Bench uses GPT-4 as the judge to compare each VLM with Claude-3-Sonnet,achieving a Spearman correlation of 0.94 with the WV-Arena Elo. Thissignificantly outperforms other benchmarks like MMVet, MMMU, and MMStar.  Our comprehensive analysis of 20K real-world interactions reveals importantinsights into the failure cases of top-performing VLMs. For example, we findthat although GPT-4V surpasses many other models like Reka-Flash, Opus, andYi-VL-Plus in simple visual recognition and reasoning tasks, it still faceschallenges with subtle contextual cues, spatial reasoning, visual imagination,and expert domain knowledge. Additionally, current VLMs exhibit issues withhallucinations and safety when intentionally provoked. We are releasing ourchat and feedback data to further advance research in the field of VLMs.
---
Title: MeshAnything: Artist-Created Mesh Generation with Autoregressive Transformers
PDF: https://arxiv.org/pdf/2406.10163
Summary: Recently, 3D assets created via reconstruction and generation have matchedthe quality of manually crafted assets, highlighting their potential forreplacement. However, this potential is largely unrealized because these assetsalways need to be converted to meshes for 3D industry applications, and themeshes produced by current mesh extraction methods are significantly inferiorto Artist-Created Meshes (AMs), i.e., meshes created by human artists.Specifically, current mesh extraction methods rely on dense faces and ignoregeometric features, leading to inefficiencies, complicated post-processing, andlower representation quality. To address these issues, we introduceMeshAnything, a model that treats mesh extraction as a generation problem,producing AMs aligned with specified shapes. By converting 3D assets in any 3Drepresentation into AMs, MeshAnything can be integrated with various 3D assetproduction methods, thereby enhancing their application across the 3D industry.The architecture of MeshAnything comprises a VQ-VAE and a shape-conditioneddecoder-only transformer. We first learn a mesh vocabulary using the VQ-VAE,then train the shape-conditioned decoder-only transformer on this vocabularyfor shape-conditioned autoregressive mesh generation. Our extensive experimentsshow that our method generates AMs with hundreds of times fewer faces,significantly improving storage, rendering, and simulation efficiencies, whileachieving precision comparable to previous methods.
---
Title: Vid3D: Synthesis of Dynamic 3D Scenes using 2D Video Diffusion
PDF: https://arxiv.org/pdf/2406.11196
Summary: A recent frontier in computer vision has been the task of 3D videogeneration, which consists of generating a time-varying 3D representation of ascene. To generate dynamic 3D scenes, current methods explicitly model 3Dtemporal dynamics by jointly optimizing for consistency across both time andviews of the scene. In this paper, we instead investigate whether it isnecessary to explicitly enforce multiview consistency over time, as currentapproaches do, or if it is sufficient for a model to generate 3Drepresentations of each timestep independently. We hence propose a model,Vid3D, that leverages 2D video diffusion to generate 3D videos by firstgenerating a 2D "seed" of the video's temporal dynamics and then independentlygenerating a 3D representation for each timestep in the seed video. We evaluateVid3D against two state-of-the-art 3D video generation methods and find thatVid3D is achieves comparable results despite not explicitly modeling 3Dtemporal dynamics. We further ablate how the quality of Vid3D depends on thenumber of views generated per frame. While we observe some degradation withfewer views, performance degradation remains minor. Our results thus suggestthat 3D temporal knowledge may not be necessary to generate high-qualitydynamic 3D scenes, potentially enabling simpler generative algorithms for thistask.
---
Title: mDPO: Conditional Preference Optimization for Multimodal Large Language Models
PDF: https://arxiv.org/pdf/2406.11839
Summary: Direct preference optimization (DPO) has shown to be an effective method forlarge language model (LLM) alignment. Recent works have attempted to apply DPOto multimodal scenarios but have found it challenging to achieve consistentimprovement. Through a comparative experiment, we identify the unconditionalpreference problem in multimodal preference optimization, where the modeloverlooks the image condition. To address this problem, we propose mDPO, amultimodal DPO objective that prevents the over-prioritization of language-onlypreferences by also optimizing image preference. Moreover, we introduce areward anchor that forces the reward to be positive for chosen responses,thereby avoiding the decrease in their likelihood -- an intrinsic problem ofrelative preference optimization. Experiments on two multimodal LLMs ofdifferent sizes and three widely used benchmarks demonstrate that mDPOeffectively addresses the unconditional preference problem in multimodalpreference optimization and significantly improves model performance,particularly in reducing hallucination.
---
Title: L4GM: Large 4D Gaussian Reconstruction Model
PDF: https://arxiv.org/pdf/2406.10324
Summary: We present L4GM, the first 4D Large Reconstruction Model that producesanimated objects from a single-view video input -- in a single feed-forwardpass that takes only a second. Key to our success is a novel dataset ofmultiview videos containing curated, rendered animated objects from Objaverse.This dataset depicts 44K diverse objects with 110K animations rendered in 48viewpoints, resulting in 12M videos with a total of 300M frames. We keep ourL4GM simple for scalability and build directly on top of LGM, a pretrained 3DLarge Reconstruction Model that outputs 3D Gaussian ellipsoids from multiviewimage input. L4GM outputs a per-frame 3D Gaussian Splatting representation fromvideo frames sampled at a low fps and then upsamples the representation to ahigher fps to achieve temporal smoothness. We add temporal self-attentionlayers to the base LGM to help it learn consistency across time, and utilize aper-timestep multiview rendering loss to train the model. The representation isupsampled to a higher framerate by training an interpolation model whichproduces intermediate 3D Gaussian representations. We showcase that L4GM thatis only trained on synthetic data generalizes extremely well on in-the-wildvideos, producing high quality animated 3D assets.
---
Title: DataComp-LM: In search of the next generation of training sets for language models
PDF: https://arxiv.org/pdf/2406.11794
Summary: We introduce DataComp for Language Models (DCLM), a testbed for controlleddataset experiments with the goal of improving language models. As part ofDCLM, we provide a standardized corpus of 240T tokens extracted from CommonCrawl, effective pretraining recipes based on the OpenLM framework, and a broadsuite of 53 downstream evaluations. Participants in the DCLM benchmark canexperiment with data curation strategies such as deduplication, filtering, anddata mixing at model scales ranging from 412M to 7B parameters. As a baselinefor DCLM, we conduct extensive experiments and find that model-based filteringis key to assembling a high-quality training set. The resulting dataset,DCLM-Baseline enables training a 7B parameter language model from scratch to64% 5-shot accuracy on MMLU with 2.6T training tokens. Compared to MAP-Neo, theprevious state-of-the-art in open-data language models, DCLM-Baselinerepresents a 6.6 percentage point improvement on MMLU while being trained with40% less compute. Our baseline model is also comparable to Mistral-7B-v0.3 andLlama 3 8B on MMLU (63% & 66%), and performs similarly on an average of 53natural language understanding tasks while being trained with 6.6x less computethan Llama 3 8B. Our results highlight the importance of dataset design fortraining language models and offer a starting point for further research ondata curation.
---
Title: GAMA: A Large Audio-Language Model with Advanced Audio Understanding and Complex Reasoning Abilities
PDF: https://arxiv.org/pdf/2406.11768
Summary: Perceiving and understanding non-speech sounds and non-verbal speech isessential to making decisions that help us interact with our surroundings. Inthis paper, we propose GAMA, a novel General-purpose Large Audio-Language Model(LALM) with Advanced Audio Understanding and Complex Reasoning Abilities. Webuild GAMA by integrating an LLM with multiple types of audio representations,including features from a custom Audio Q-Former, a multi-layer aggregator thataggregates features from multiple layers of an audio encoder. We fine-tune GAMAon a large-scale audio-language dataset, which augments it with audiounderstanding capabilities. Next, we propose CompA-R (Instruction-Tuning forComplex Audio Reasoning), a synthetically generated instruction-tuning (IT)dataset with instructions that require the model to perform complex reasoningon the input audio. We instruction-tune GAMA with CompA-R to endow it withcomplex reasoning abilities, where we further add a soft prompt as input withhigh-level semantic evidence by leveraging event tags of the input audio.Finally, we also propose CompA-R-test, a human-labeled evaluation dataset forevaluating the capabilities of LALMs on open-ended audio question-answeringthat requires complex reasoning. Through automated and expert humanevaluations, we show that GAMA outperforms all other LALMs in literature ondiverse audio understanding tasks by margins of 1%-84%. Further, GAMA IT-ed onCompA-R proves to be superior in its complex reasoning and instructionfollowing capabilities.
---
Title: Be like a Goldfish, Don't Memorize! Mitigating Memorization in Generative LLMs
PDF: https://arxiv.org/pdf/2406.10209
Summary: Large language models can memorize and repeat their training data, causingprivacy and copyright risks. To mitigate memorization, we introduce a subtlemodification to the next-token training objective that we call the goldfishloss. During training, a randomly sampled subset of tokens are excluded fromthe loss computation. These dropped tokens are not memorized by the model,which prevents verbatim reproduction of a complete chain of tokens from thetraining set. We run extensive experiments training billion-scale Llama-2models, both pre-trained and trained from scratch, and demonstrate significantreductions in extractable memorization with little to no impact on downstreambenchmarks.
---
Title: RVT-2: Learning Precise Manipulation from Few Demonstrations
PDF: https://arxiv.org/pdf/2406.08545
Summary: In this work, we study how to build a robotic system that can solve multiple3D manipulation tasks given language instructions. To be useful in industrialand household domains, such a system should be capable of learning new taskswith few demonstrations and solving them precisely. Prior works, like PerActand RVT, have studied this problem, however, they often struggle with tasksrequiring high precision. We study how to make them more effective, precise,and fast. Using a combination of architectural and system-level improvements,we propose RVT-2, a multitask 3D manipulation model that is 6X faster intraining and 2X faster in inference than its predecessor RVT. RVT-2 achieves anew state-of-the-art on RLBench, improving the success rate from 65% to 82%.RVT-2 is also effective in the real world, where it can learn tasks requiringhigh precision, like picking up and inserting plugs, with just 10demonstrations. Visual results, code, and trained model are provided at:https://robotic-view-transformer-2.github.io/.
---
Title: Vivid-ZOO: Multi-View Video Generation with Diffusion Model
PDF: https://arxiv.org/pdf/2406.08659
Summary: While diffusion models have shown impressive performance in 2D image/videogeneration, diffusion-based Text-to-Multi-view-Video (T2MVid) generationremains underexplored. The new challenges posed by T2MVid generation lie in thelack of massive captioned multi-view videos and the complexity of modeling suchmulti-dimensional distribution. To this end, we propose a novel diffusion-basedpipeline that generates high-quality multi-view videos centered around adynamic 3D object from text. Specifically, we factor the T2MVid problem intoviewpoint-space and time components. Such factorization allows us to combineand reuse layers of advanced pre-trained multi-view image and 2D videodiffusion models to ensure multi-view consistency as well as temporal coherencefor the generated multi-view videos, largely reducing the training cost. Wefurther introduce alignment modules to align the latent spaces of layers fromthe pre-trained multi-view and the 2D video diffusion models, addressing thereused layers' incompatibility that arises from the domain gap between 2D andmulti-view data. In support of this and future research, we further contributea captioned multi-view video dataset. Experimental results demonstrate that ourmethod generates high-quality multi-view videos, exhibiting vivid motions,temporal coherence, and multi-view consistency, given a variety of textprompts.
---
Title: AV-GS: Learning Material and Geometry Aware Priors for Novel View Acoustic Synthesis
PDF: https://arxiv.org/pdf/2406.08920
Summary: Novel view acoustic synthesis (NVAS) aims to render binaural audio at anytarget viewpoint, given a mono audio emitted by a sound source at a 3D scene.Existing methods have proposed NeRF-based implicit models to exploit visualcues as a condition for synthesizing binaural audio. However, in addition tolow efficiency originating from heavy NeRF rendering, these methods all have alimited ability of characterizing the entire scene environment such as roomgeometry, material properties, and the spatial relation between the listenerand sound source. To address these issues, we propose a novel Audio-VisualGaussian Splatting (AV-GS) model. To obtain a material-aware and geometry-awarecondition for audio synthesis, we learn an explicit point-based scenerepresentation with an audio-guidance parameter on locally initialized Gaussianpoints, taking into account the space relation from the listener and soundsource. To make the visual scene model audio adaptive, we propose a pointdensification and pruning strategy to optimally distribute the Gaussian points,with the per-point contribution in sound propagation (e.g., more points neededfor texture-less wall surfaces as they affect sound path diversion). Extensiveexperiments validate the superiority of our AV-GS over existing alternatives onthe real-world RWAS and simulation-based SoundSpaces datasets.
---
Title: VideoGUI: A Benchmark for GUI Automation from Instructional Videos
PDF: https://arxiv.org/pdf/2406.10227
Summary: Graphical User Interface (GUI) automation holds significant promise forenhancing human productivity by assisting with computer tasks. Existing taskformulations primarily focus on simple tasks that can be specified by a single,language-only instruction, such as "Insert a new slide." In this work, weintroduce VideoGUI, a novel multi-modal benchmark designed to evaluate GUIassistants on visual-centric GUI tasks. Sourced from high-quality webinstructional videos, our benchmark focuses on tasks involving professional andnovel software (e.g., Adobe Photoshop or Stable Diffusion WebUI) and complexactivities (e.g., video editing). VideoGUI evaluates GUI assistants through ahierarchical process, allowing for identification of the specific levels atwhich they may fail: (i) high-level planning: reconstruct procedural subtasksfrom visual conditions without language descriptions; (ii) middle-levelplanning: generate sequences of precise action narrations based on visual state(i.e., screenshot) and goals; (iii) atomic action execution: perform specificactions such as accurately clicking designated elements. For each level, wedesign evaluation metrics across individual dimensions to provide clearsignals, such as individual performance in clicking, dragging, typing, andscrolling for atomic action execution. Our evaluation on VideoGUI reveals thateven the SoTA large multimodal model GPT4o performs poorly on visual-centricGUI tasks, especially for high-level planning.
---
Title: SEACrowd: A Multilingual Multimodal Data Hub and Benchmark Suite for Southeast Asian Languages
PDF: https://arxiv.org/pdf/2406.10118
Summary: Southeast Asia (SEA) is a region rich in linguistic diversity and culturalvariety, with over 1,300 indigenous languages and a population of 671 millionpeople. However, prevailing AI models suffer from a significant lack ofrepresentation of texts, images, and audio datasets from SEA, compromising thequality of AI models for SEA languages. Evaluating models for SEA languages ischallenging due to the scarcity of high-quality datasets, compounded by thedominance of English training data, raising concerns about potential culturalmisrepresentation. To address these challenges, we introduce SEACrowd, acollaborative initiative that consolidates a comprehensive resource hub thatfills the resource gap by providing standardized corpora in nearly 1,000 SEAlanguages across three modalities. Through our SEACrowd benchmarks, we assessthe quality of AI models on 36 indigenous languages across 13 tasks, offeringvaluable insights into the current AI landscape in SEA. Furthermore, we proposestrategies to facilitate greater AI advancements, maximizing potential utilityand resource equity for the future of AI in SEA.
---
Title: Training-free Camera Control for Video Generation
PDF: https://arxiv.org/pdf/2406.10126
Summary: We propose a training-free and robust solution to offer camera movementcontrol for off-the-shelf video diffusion models. Unlike previous work, ourmethod does not require any supervised finetuning on camera-annotated datasetsor self-supervised training via data augmentation. Instead, it can be pluggedand played with most pretrained video diffusion models and generate cameracontrollable videos with a single image or text prompt as input. Theinspiration of our work comes from the layout prior that intermediate latentshold towards generated results, thus rearranging noisy pixels in them will makeoutput content reallocated as well. As camera move could also be seen as a kindof pixel rearrangement caused by perspective change, videos could bereorganized following specific camera motion if their noisy latents changeaccordingly. Established on this, we propose our method CamTrol, which enablesrobust camera control for video diffusion models. It is achieved by a two-stageprocess. First, we model image layout rearrangement through explicit cameramovement in 3D point cloud space. Second, we generate videos with camera motionusing layout prior of noisy latents formed by a series of rearranged images.Extensive experiments have demonstrated the robustness our method holds incontrolling camera motion of generated videos. Furthermore, we show that ourmethod can produce impressive results in generating 3D rotation videos withdynamic content. Project page at https://lifedecoder.github.io/CamTrol/.
---
Title: Make It Count: Text-to-Image Generation with an Accurate Number of Objects
PDF: https://arxiv.org/pdf/2406.10210
Summary: Despite the unprecedented success of text-to-image diffusion models,controlling the number of depicted objects using text is surprisingly hard.This is important for various applications from technical documents, tochildren's books to illustrating cooking recipes. Generating object-correctcounts is fundamentally challenging because the generative model needs to keepa sense of separate identity for every instance of the object, even if severalobjects look identical or overlap, and then carry out a global computationimplicitly during generation. It is still unknown if such representationsexist. To address count-correct generation, we first identify features withinthe diffusion model that can carry the object identity information. We then usethem to separate and count instances of objects during the denoising processand detect over-generation and under-generation. We fix the latter by traininga model that predicts both the shape and location of a missing object, based onthe layout of existing ones, and show how it can be used to guide denoisingwith correct object count. Our approach, CountGen, does not depend on externalsource to determine object layout, but rather uses the prior from the diffusionmodel itself, creating prompt-dependent and seed-dependent layouts. Evaluatedon two benchmark datasets, we find that CountGen strongly outperforms thecount-accuracy of existing baselines.
---
Title: BABILong: Testing the Limits of LLMs with Long Context Reasoning-in-a-Haystack
PDF: https://arxiv.org/pdf/2406.10149
Summary: In recent years, the input context sizes of large language models (LLMs) haveincreased dramatically. However, existing evaluation methods have not keptpace, failing to comprehensively assess the efficiency of models in handlinglong contexts. To bridge this gap, we introduce the BABILong benchmark,designed to test language models' ability to reason across facts distributed inextremely long documents. BABILong includes a diverse set of 20 reasoningtasks, including fact chaining, simple induction, deduction, counting, andhandling lists/sets. These tasks are challenging on their own, and even moredemanding when the required facts are scattered across long natural text. Ourevaluations show that popular LLMs effectively utilize only 10-20\% of thecontext and their performance declines sharply with increased reasoningcomplexity. Among alternatives to in-context reasoning, Retrieval-AugmentedGeneration methods achieve a modest 60\% accuracy on single-fact questionanswering, independent of context length. Among context extension methods, thehighest performance is demonstrated by recurrent memory transformers, enablingthe processing of lengths up to 11 million tokens. The BABILong benchmark isextendable to any length to support the evaluation of new upcoming models withincreased capabilities, and we provide splits up to 1 million token lengths.
---
Title: GaussianSR: 3D Gaussian Super-Resolution with 2D Diffusion Priors
PDF: https://arxiv.org/pdf/2406.10111
Summary: Achieving high-resolution novel view synthesis (HRNVS) from low-resolutioninput views is a challenging task due to the lack of high-resolution data.Previous methods optimize high-resolution Neural Radiance Field (NeRF) fromlow-resolution input views but suffer from slow rendering speed. In this work,we base our method on 3D Gaussian Splatting (3DGS) due to its capability ofproducing high-quality images at a faster rendering speed. To alleviate theshortage of data for higher-resolution synthesis, we propose to leverageoff-the-shelf 2D diffusion priors by distilling the 2D knowledge into 3D withScore Distillation Sampling (SDS). Nevertheless, applying SDS directly toGaussian-based 3D super-resolution leads to undesirable and redundant 3DGaussian primitives, due to the randomness brought by generative priors. Tomitigate this issue, we introduce two simple yet effective techniques to reducestochastic disturbances introduced by SDS. Specifically, we 1) shrink the rangeof diffusion timestep in SDS with an annealing strategy; 2) randomly discardredundant Gaussian primitives during densification. Extensive experiments havedemonstrated that our proposed GaussainSR can attain high-quality results forHRNVS with only low-resolution inputs on both synthetic and real-worlddatasets. Project page: https://chchnii.github.io/GaussianSR/
---
Title: GEB-1.3B: Open Lightweight Large Language Model
PDF: https://arxiv.org/pdf/2406.09900
Summary: Recently developed large language models (LLMs) such as ChatGPT, Claude, andLlama have demonstrated impressive abilities, and even surpass human-levelperformance in several tasks. Despite their success, the resource-intensivedemands of these models, requiring significant computational power for bothtraining and inference, limit their deployment to high-performance servers.Additionally, the extensive calculation requirements of the models often leadto increased latency in response times. With the increasing need for LLMs tooperate efficiently on CPUs, research about lightweight models that areoptimized for CPU inference has emerged. In this work, we introduce GEB-1.3B, alightweight LLM trained on 550 billion tokens in both Chinese and Englishlanguages. We employ novel training techniques, including ROPE,Group-Query-Attention, and FlashAttention-2, to accelerate training whilemaintaining model performance. Additionally, we fine-tune the model using 10million samples of instruction data to enhance alignment. GEB-1.3B exhibitsoutstanding performance on general benchmarks such as MMLU, C-Eval, and CMMLU,outperforming comparative models such as MindLLM-1.3B and TinyLLaMA-1.1B.Notably, the FP32 version of GEB-1.3B achieves commendable inference times onCPUs, with ongoing efforts to further enhance speed through advancedquantization techniques. The release of GEB-1.3B as an open-source model marksa significant contribution to the development of lightweight LLMs, promising tofoster further research and innovation in the field.
---
Title: MaskLID: Code-Switching Language Identification through Iterative Masking
PDF: https://arxiv.org/pdf/2406.06263
Summary: We present MaskLID, a simple, yet effective, code-switching (CS) languageidentification (LID) method. MaskLID does not require any training and isdesigned to complement current high-performance sentence-level LIDs.Sentence-level LIDs are classifiers trained on monolingual texts to providesingle labels, typically using a softmax layer to turn scores intoprobabilities. However, in cases where a sentence is composed in both L1 and L2languages, the LID classifier often only returns the dominant label L1. Toaddress this limitation, MaskLID employs a strategy to mask text featuresassociated with L1, allowing the LID to classify the text as L2 in the nextround. This method uses the LID itself to identify the features that requiremasking and does not rely on any external resource. In this work, we explorethe use of MaskLID for two open-source LIDs (GlotLID and OpenLID), that areboth based on the FastText architecture. Code and demo are available athttps://github.com/cisnlp/MaskLID.
---
Title: XLand-100B: A Large-Scale Multi-Task Dataset for In-Context Reinforcement Learning
PDF: https://arxiv.org/pdf/2406.08973
Summary: Following the success of the in-context learning paradigm in large-scalelanguage and computer vision models, the recently emerging field of in-contextreinforcement learning is experiencing a rapid growth. However, its developmenthas been held back by the lack of challenging benchmarks, as all theexperiments have been carried out in simple environments and on small-scaledatasets. We present XLand-100B, a large-scale dataset for in-contextreinforcement learning based on the XLand-MiniGrid environment, as a first stepto alleviate this problem. It contains complete learning histories for nearly30,000 different tasks, covering 100B transitions and 2.5B episodes. Ittook 50,000 GPU hours to collect the dataset, which is beyond the reach ofmost academic labs. Along with the dataset, we provide the utilities toreproduce or expand it even further. With this substantial effort, we aim todemocratize research in the rapidly growing field of in-context reinforcementlearning and provide a solid foundation for further scaling. The code isopen-source and available under Apache 2.0 licence athttps://github.com/dunno-lab/xland-minigrid-datasets.
---
Title: ChartMimic: Evaluating LMM's Cross-Modal Reasoning Capability via Chart-to-Code Generation
PDF: https://arxiv.org/pdf/2406.09961
Summary: We introduce a new benchmark, ChartMimic, aimed at assessing thevisually-grounded code generation capabilities of large multimodal models(LMMs). ChartMimic utilizes information-intensive visual charts and textualinstructions as inputs, requiring LMMs to generate the corresponding code forchart rendering. ChartMimic includes 1,000 human-curated (figure, instruction,code) triplets, which represent the authentic chart use cases found inscientific papers across various domains(e.g., Physics, Computer Science,Economics, etc). These charts span 18 regular types and 4 advanced types,diversifying into 191 subcategories. Furthermore, we propose multi-levelevaluation metrics to provide an automatic and thorough assessment of theoutput code and the rendered charts. Unlike existing code generationbenchmarks, ChartMimic places emphasis on evaluating LMMs' capacity toharmonize a blend of cognitive capabilities, encompassing visual understanding,code generation, and cross-modal reasoning. The evaluation of 3 proprietarymodels and 11 open-weight models highlights the substantial challenges posed byChartMimic. Even the advanced GPT-4V, Claude-3-opus only achieve an averagescore of 73.2 and 53.7, respectively, indicating significant room forimprovement. We anticipate that ChartMimic will inspire the development ofLMMs, advancing the pursuit of artificial general intelligence.
---
Title: Designing a Dashboard for Transparency and Control of Conversational AI
PDF: https://arxiv.org/pdf/2406.07882
Summary: Conversational LLMs function as black box systems, leaving users guessingabout why they see the output they do. This lack of transparency is potentiallyproblematic, especially given concerns around bias and truthfulness. To addressthis issue, we present an end-to-end prototype-connecting interpretabilitytechniques with user experience design-that seeks to make chatbots moretransparent. We begin by showing evidence that a prominent open-source LLM hasa "user model": examining the internal state of the system, we can extract datarelated to a user's age, gender, educational level, and socioeconomic status.Next, we describe the design of a dashboard that accompanies the chatbotinterface, displaying this user model in real time. The dashboard can also beused to control the user model and the system's behavior. Finally, we discuss astudy in which users conversed with the instrumented system. Our resultssuggest that users appreciate seeing internal states, which helped them exposebiased behavior and increased their sense of control. Participants also madevaluable suggestions that point to future directions for both design andmachine learning research. The project page and video demo of our TalkTunersystem are available at https://bit.ly/talktuner-project-page
---
Title: Rethinking Human Evaluation Protocol for Text-to-Video Models: Enhancing Reliability,Reproducibility, and Practicality
PDF: https://arxiv.org/pdf/2406.08845
Summary: Recent text-to-video (T2V) technology advancements, as demonstrated by modelssuch as Gen2, Pika, and Sora, have significantly broadened its applicabilityand popularity. Despite these strides, evaluating these models posessubstantial challenges. Primarily, due to the limitations inherent in automaticmetrics, manual evaluation is often considered a superior method for assessingT2V generation. However, existing manual evaluation protocols facereproducibility, reliability, and practicality issues. To address thesechallenges, this paper introduces the Text-to-Video Human Evaluation (T2VHE)protocol, a comprehensive and standardized protocol for T2V models. The T2VHEprotocol includes well-defined metrics, thorough annotator training, and aneffective dynamic evaluation module. Experimental results demonstrate that thisprotocol not only ensures high-quality annotations but can also reduceevaluation costs by nearly 50%. We will open-source the entire setup of theT2VHE protocol, including the complete protocol workflow, the dynamicevaluation component details, and the annotation interface code. This will helpcommunities establish more sophisticated human assessment protocols.
---
Title: GUI Odyssey: A Comprehensive Dataset for Cross-App GUI Navigation on Mobile Devices
PDF: https://arxiv.org/pdf/2406.08451
Summary: Smartphone users often navigate across multiple applications (apps) tocomplete tasks such as sharing content between social media platforms.Autonomous Graphical User Interface (GUI) navigation agents can enhance userexperience in communication, entertainment, and productivity by streamliningworkflows and reducing manual intervention. However, prior GUI agents oftentrained with datasets comprising simple tasks that can be completed within asingle app, leading to poor performance in cross-app navigation. To addressthis problem, we introduce GUI Odyssey, a comprehensive dataset for trainingand evaluating cross-app navigation agents. GUI Odyssey consists of 7,735episodes from 6 mobile devices, spanning 6 types of cross-app tasks, 201 apps,and 1.4K app combos. Leveraging GUI Odyssey, we developed OdysseyAgent, amultimodal cross-app navigation agent by fine-tuning the Qwen-VL model with ahistory resampling module. Extensive experiments demonstrate OdysseyAgent'ssuperior accuracy compared to existing models. For instance, OdysseyAgentsurpasses fine-tuned Qwen-VL and zero-shot GPT-4V by 1.44\% and 55.49\%in-domain accuracy, and 2.29\% and 48.14\% out-of-domain accuracy on average.The dataset and code will be released inhttps://github.com/OpenGVLab/GUI-Odyssey.
---
Title: OmniCorpus: A Unified Multimodal Corpus of 10 Billion-Level Images Interleaved with Text
PDF: https://arxiv.org/pdf/2406.08418
Summary: Image-text interleaved data, consisting of multiple images and texts arrangedin a natural document format, aligns with the presentation paradigm of internetdata and closely resembles human reading habits. Recent studies have shown thatsuch data aids multimodal in-context learning and maintains the capabilities oflarge language models during multimodal fine-tuning. However, the limited scaleand diversity of current image-text interleaved data restrict the developmentof multimodal large language models. In this paper, we introduce OmniCorpus, a10 billion-scale image-text interleaved dataset. Using an efficient dataengine, we filter and extract large-scale high-quality documents, which contain8.6 billion images and 1,696 billion text tokens. Compared to counterparts(e.g., MMC4, OBELICS), our dataset 1) has 15 times larger scales whilemaintaining good data quality; 2) features more diverse sources, including bothEnglish and non-English websites as well as video-centric websites; 3) is moreflexible, easily degradable from an image-text interleaved format to pure textcorpus and image-text pairs. Through comprehensive analysis and experiments, wevalidate the quality, usability, and effectiveness of the proposed dataset. Wehope this could provide a solid data foundation for future multimodal modelresearch. Code and data are released athttps://github.com/OpenGVLab/OmniCorpus.
---
Title: Needle In A Multimodal Haystack
PDF: https://arxiv.org/pdf/2406.07230
Summary: With the rapid advancement of multimodal large language models (MLLMs), theirevaluation has become increasingly comprehensive. However, understanding longmultimodal content, as a foundational ability for real-world applications,remains underexplored. In this work, we present Needle In A Multimodal Haystack(MM-NIAH), the first benchmark specifically designed to systematically evaluatethe capability of existing MLLMs to comprehend long multimodal documents. Ourbenchmark includes three types of evaluation tasks: multimodal retrieval,counting, and reasoning. In each task, the model is required to answer thequestions according to different key information scattered throughout the givenmultimodal document. Evaluating the leading MLLMs on MM-NIAH, we observe thatexisting models still have significant room for improvement on these tasks,especially on vision-centric evaluation. We hope this work can provide aplatform for further research on long multimodal document comprehension andcontribute to the advancement of MLLMs. Code and benchmark are released athttps://github.com/OpenGVLab/MM-NIAH.
---
Title: Decoding the Diversity: A Review of the Indic AI Research Landscape
PDF: https://arxiv.org/pdf/2406.09559
Summary: This review paper provides a comprehensive overview of large language model(LLM) research directions within Indic languages. Indic languages are thosespoken in the Indian subcontinent, including India, Pakistan, Bangladesh, SriLanka, Nepal, and Bhutan, among others. These languages have a rich culturaland linguistic heritage and are spoken by over 1.5 billion people worldwide.With the tremendous market potential and growing demand for natural languageprocessing (NLP) based applications in diverse languages, generativeapplications for Indic languages pose unique challenges and opportunities forresearch. Our paper deep dives into the recent advancements in Indic generativemodeling, contributing with a taxonomy of research directions, tabulating 84recent publications. Research directions surveyed in this paper include LLMdevelopment, fine-tuning existing LLMs, development of corpora, benchmarkingand evaluation, as well as publications around specific techniques, tools, andapplications. We found that researchers across the publications emphasize thechallenges associated with limited data availability, lack of standardization,and the peculiar linguistic complexities of Indic languages. This work aims toserve as a valuable resource for researchers and practitioners working in thefield of NLP, particularly those focused on Indic languages, and contributes tothe development of more accurate and efficient LLM applications for theselanguages.
---
Title: Glyph-ByT5-v2: A Strong Aesthetic Baseline for Accurate Multilingual Visual Text Rendering
PDF: https://arxiv.org/pdf/2406.10208
Summary: Recently, Glyph-ByT5 has achieved highly accurate visual text renderingperformance in graphic design images. However, it still focuses solely onEnglish and performs relatively poorly in terms of visual appeal. In this work,we address these two fundamental limitations by presenting Glyph-ByT5-v2 andGlyph-SDXL-v2, which not only support accurate visual text rendering for 10different languages but also achieve much better aesthetic quality. To achievethis, we make the following contributions: (i) creating a high-qualitymultilingual glyph-text and graphic design dataset consisting of more than 1million glyph-text pairs and 10 million graphic design image-text pairscovering nine other languages, (ii) building a multilingual visual paragraphbenchmark consisting of 1,000 prompts, with 100 for each language, to assessmultilingual visual spelling accuracy, and (iii) leveraging the lateststep-aware preference learning approach to enhance the visual aestheticquality. With the combination of these techniques, we deliver a powerfulcustomized multilingual text encoder, Glyph-ByT5-v2, and a strong aestheticgraphic generation model, Glyph-SDXL-v2, that can support accurate spelling in10 different languages. We perceive our work as a significant advancement,considering that the latest DALL-E3 and Ideogram 1.0 still struggle with themultilingual visual text rendering task.
---
