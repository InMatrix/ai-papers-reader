Here are the latest Machine Learning papers with implications for HCI, organized by the provided research topics:

```json
[
  {
    "topic": "AI for Software Development",
    "papers": [
      {
        "title": "When \"Correct\" Is Not Safe: Can We Trust Functionally Correct Patches Generated by Code Agents?",
        "relevance": "This paper uncovers a critical security vulnerability: code agents generating functionally correct yet vulnerable (FCV) patches. For AI in software development, this directly impacts developer trust, safety, and the reliability of AI-assisted coding tools. From an HCI perspective, it highlights the need for developers to be aware of and scrutinize AI-generated code, necessitating better explanation of potential risks and new evaluation paradigms beyond mere functional correctness to ensure human safety and secure codebases.",
        "url": "https://arxiv.org/pdf/2510.17862"
      },
      {
        "title": "From Charts to Code: A Hierarchical Benchmark for Multimodal Models",
        "relevance": "This paper introduces Chart2Code, a user-driven benchmark for evaluating multimodal models' ability to generate code from chart images. This is a direct application of AI in software development, assisting users with data visualization. From an HCI perspective, the benchmark's focus on both code correctness and the *visual fidelity* of the rendered charts ensures that evaluation aligns with user expectations and practical usability, driving the development of AI tools that are more effective and user-friendly for developers.",
        "url": "https://arxiv.org/pdf/2510.17932"
      },
      {
        "title": "AlphaOPT: Formulating Optimization Programs with Self-Improving LLM Experience Library",
        "relevance": "AlphaOPT enables LLMs to learn to formulate optimization programs and executable solver code, representing an advanced AI tool for software development, particularly in specialized computational domains. The system's self-improving 'experience library' stores knowledge in an 'explicit and interpretable' format (taxonomy, condition, explanation, example). This transparency is crucial for HCI, allowing human developers to inspect, understand, and intervene in the AI's program generation process, fostering trust and effective collaboration in complex problem-solving scenarios.",
        "url": "https://arxiv.org/pdf/2510.18428"
      }
    ]
  },
  {
    "topic": "AI Agents",
    "papers": [
      {
        "title": "ColorAgent: Building A Robust, Personalized, and Interactive OS Agent",
        "relevance": "This paper introduces ColorAgent, an OS agent explicitly designed for robust, personalized, and proactive user interaction. It aims to be a 'warm, collaborative partner,' directly addressing key HCI concerns for AI agents, such as long-horizon interaction, user intent recognition, and proactive engagement. By leveraging reinforcement learning and a multi-agent framework, ColorAgent pushes the boundaries of how humans can intuitively and effectively interact with autonomous software systems in complex operating system environments, emphasizing user experience and collaboration.",
        "url": "https://arxiv.org/pdf/2510.19386"
      },
      {
        "title": "TheMCPCompany: Creating General-purpose Agents with Task-specific Tools",
        "relevance": "This paper presents TheMCPCompany, a benchmark for evaluating tool-calling agents interacting with thousands of real-world services. It directly addresses the core AI agent challenge of effective tool use and navigation in complex digital environments. From an HCI perspective, the benchmark reveals that even state-of-the-art models struggle with combining tools and navigating large toolsets, highlighting significant challenges for user experience and agent reliability in practical, general-purpose applications that require agents to understand and execute human goals in tool-rich settings.",
        "url": "https://arxiv.org/pdf/2510.19286"
      },
      {
        "title": "Static Sandboxes Are Inadequate: Modeling Societal Complexity Requires Open-Ended Co-Evolution in LLM-Based Multi-Agent Simulations",
        "relevance": "This paper provides a critical perspective and roadmap for LLM-based multi-agent systems, arguing against static simulations and for open-ended co-evolution to model societal complexity. This is highly relevant to AI agents as it challenges the fundamental approach to agent design and evaluation. For HCI, it emphasizes the need for systems that can adapt, exhibit unexpected behaviors, and align with human values in dynamic social contexts, pushing researchers to consider the long-term implications of AI agents on society and user interaction.",
        "url": "https://arxiv.org/pdf/2510.13982"
      }
    ]
  },
  {
    "topic": "LLM Evaluation Methods",
    "papers": [
      {
        "title": "The Atomic Instruction Gap: Instruction-Tuned LLMs Struggle with Simple, Self-Contained Directives",
        "relevance": "This paper rigorously evaluates instruction-tuned LLMs, uncovering significant instruction-format biases and weak adherence to basic directives. From an HCI standpoint, this research is crucial for understanding the reliability and usability of LLMs. It directly impacts user satisfaction and trust when interacting with LLMs that struggle with fundamental instruction following. The findings highlight the inadequacy of current evaluation and training paradigms, advocating for new methods that ensure LLMs can reliably respond to human prompts, regardless of minor variations in instruction format.",
        "url": "https://arxiv.org/pdf/2510.17388"
      },
      {
        "title": "Are they lovers or friends? Evaluating LLMs' Social Reasoning in English and Korean Dialogues",
        "relevance": "This paper introduces SCRIPTS, a unique dataset and evaluation method for assessing LLMs' social reasoning capabilities in dialogues. It reveals significant limitations, language disparities, and potential bias amplification in LLMs' ability to infer interpersonal relationships. From an HCI perspective, robust social reasoning is critical for LLMs to interact effectively and appropriately with humans, especially in conversational interfaces. The findings underscore the urgent need for evaluation methods that address social intelligence, fairness, and cross-cultural nuances to build more socially-aware and trustworthy LLMs.",
        "url": "https://arxiv.org/pdf/2510.19028"
      },
      {
        "title": "ProfBench: Multi-Domain Rubrics requiring Professional Knowledge to Answer and Judge",
        "relevance": "ProfBench introduces a novel benchmark and evaluation methodology for LLMs in complex professional domains, emphasizing the need for human expert knowledge to assess quality. It also develops LLM-Judges to scale evaluation affordably. From an HCI perspective, this work is highly relevant for evaluating LLMs on tasks that demand professional accuracy, analytical depth, and structured report generation, directly addressing user satisfaction and trust in AI assistants handling specialized information. It moves beyond simple QA to assess real-world application quality.",
        "url": "https://arxiv.org/pdf/2510.18941"
      }
    ]
  },
  {
    "topic": "Reinforcement Learning",
    "papers": [
      {
        "title": "Expanding the Action Space of LLMs to Reason Beyond Language",
        "relevance": "This paper introduces ExpA and ExpA Reinforcement Learning (EARL), enabling LLMs to reason and act by interacting with external environments beyond mere language generation. This fundamentally expands the scope of RL for AI agents, allowing them to perform multi-turn interactions and contingent planning with external tools. From an HCI perspective, this research offers a pathway to creating more capable and versatile agents that can achieve complex user-defined goals by interacting with the digital world, requiring new paradigms for human-agent collaboration and feedback.",
        "url": "https://arxiv.org/pdf/2510.07581"
      },
      {
        "title": "LoongRL:Reinforcement Learning for Advanced Reasoning over Long Contexts",
        "relevance": "LoongRL applies reinforcement learning to enhance LLMs' reasoning capabilities over long contexts, a crucial aspect for complex tasks. It introduces KeyChain to create challenging RL data and induces an emergent 'plan-retrieve-reason-recheck' pattern. From an HCI perspective, improving long-context reasoning is vital for agents to handle complex instructions and information, impacting user trust and reducing cognitive load. Furthermore, understanding and potentially externalizing such emergent reasoning patterns could lay groundwork for more explainable RL systems and agent behaviors.",
        "url": "https://arxiv.org/pdf/2510.19363"
      },
      {
        "title": "ColorAgent: Building A Robust, Personalized, and Interactive OS Agent",
        "relevance": "This paper details ColorAgent, an OS agent that leverages 'step-wise reinforcement learning and self-evolving training' for robust, personalized, and interactive performance. This directly showcases RL's role in building adaptable AI agents that learn from interaction. For HCI, the application of RL to achieve a 'warm, collaborative partner' emphasizes learning user preferences and adapting behavior over time, crucial for building intuitive and trustworthy human-agent systems that improve with user engagement and feedback.",
        "url": "https://arxiv.org/pdf/2510.19386"
      }
    ]
  },
  {
    "topic": "Explainable AI",
    "papers": [
      {
        "title": "What Questions Should Robots Be Able to Answer? A Dataset of User Questions for Explainable Robotics",
        "relevance": "This foundational paper introduces a dataset of user questions for household robots, moving beyond 'why' questions to a broader spectrum of user information needs. It is directly relevant to XAI by informing *what* types of explanations users actually desire. From an HCI perspective, this dataset is invaluable for designing robot explanation strategies that align with user expectations, build trust, and facilitate intuitive human-robot interaction by enabling robots to address diverse queries about their capabilities, actions, and hypothetical behaviors.",
        "url": "https://arxiv.org/pdf/2510.16435"
      },
      {
        "title": "Steering Autoregressive Music Generation with Recursive Feature Machines",
        "relevance": "This paper introduces MusicRFM, a framework for 'fine-grained, interpretable control' over music generation models using 'concept directions' that correspond to musical attributes. This is a direct contribution to XAI, as it allows users to understand and manipulate the model's internal representations in semantically meaningful ways (e.g., specific notes or chords). From an HCI perspective, this provides a powerful interactive control mechanism that enhances user agency and interpretability, moving beyond black-box generation towards a collaborative creative process.",
        "url": "https://arxiv.org/pdf/2510.19127"
      },
      {
        "title": "Language Models are Injective and Hence Invertible",
        "relevance": "This paper proves that transformer language models are injective and invertible, meaning input text can be exactly reconstructed from hidden activations. It introduces SipIt for practical input reconstruction. This fundamental property has direct implications for XAI by establishing a deeper level of transparency within LLMs. From an HCI perspective, understanding what information is perfectly preserved or transformed within a model's layers can inform the design of novel interpretability tools, helping users develop more accurate mental models of AI behavior and increasing trust in their processing capabilities.",
        "url": "https://arxiv.org/pdf/2510.15511"
      }
    ]
  }
]
```