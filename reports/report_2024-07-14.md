## Research Question: How are users involved in the evaluation of AI systems?

- *Relevant Paper*: **Is Your Model Really A Good Math Reasoner? Evaluating Mathematical Reasoning with Checklist**
    - *Why it's relevant*: This paper proposes MATHCHECK, a comprehensive checklist for evaluating mathematical reasoning abilities of AI systems. This checklist aims to reflect real-world user experience and evaluate model robustness across diverse tasks, providing insights into the usability and reliability of AI systems for mathematical reasoning.
    - *Read more*: https://arxiv.org/pdf/2407.08733
- *Relevant Paper*: **GTA: A Benchmark for General Tool Agents**
    - *Why it's relevant*: This paper proposes GTA, a benchmark for evaluating General Tool Agents, which includes real user queries and real-world tools. This framework considers user experience and practical application scenarios, providing valuable insights into the usability and capabilities of AI systems in real-world settings.
    - *Read more*: https://arxiv.org/pdf/2407.08713

## Research Question: What are the novel prompt engineering techniques that improve the performance of AI systems?

- *Relevant Paper*: **Multimodal Self-Instruct: Synthetic Abstract Image and Visual Reasoning Instruction Using Language Model**
    - *Why it's relevant*: This paper introduces a novel technique for generating synthetic abstract images and visual reasoning instructions using language models, which can be used to improve the performance of AI systems in various visual reasoning tasks.
    - *Read more*: https://arxiv.org/pdf/2407.07053
- *Relevant Paper*: **MAVIS: Mathematical Visual Instruction Tuning**
    - *Why it's relevant*: This paper proposes MAVIS, a novel instruction tuning paradigm for multi-modal large language models specifically designed to improve their mathematical problem-solving abilities in visual contexts. This method includes a specialized vision encoder and a carefully curated dataset with complete chain-of-thought rationales.
    - *Read more*: https://arxiv.org/pdf/2407.08739
- *Relevant Paper*: **Skywork-Math: Data Scaling Laws for Mathematical Reasoning in Large Language Models -- The Story Goes On**
    - *Why it's relevant*: This paper introduces Skywork-MathQA, a dataset of 2.5M instances used to train the Skywork-Math model series, which demonstrates the importance of data scaling for improving the mathematical reasoning capabilities of LLMs. The paper also provides practical takeaways for enhancing math reasoning in LLMs.
    - *Read more*: https://arxiv.org/pdf/2407.08348
- *Relevant Paper*: **This&That: Language-Gesture Controlled Video Generation for Robot Planning**
    - *Why it's relevant*: This paper presents a novel method for robot planning that combines language-gesture conditioning for video generation and a behavioral cloning design, enabling unambiguous task communication and controllable video generation for robots.
    - *Read more*: https://arxiv.org/pdf/2407.05530
- *Relevant Paper*: **SEED-Story: Multimodal Long Story Generation with Large Language Model**
    - *Why it's relevant*: This paper proposes SEED-Story, a method leveraging a Multimodal Large Language Model (MLLM) to generate extended multimodal stories. It uses multimodal attention sink mechanism for efficient autoregressive story generation with up to 25 sequences.
    - *Read more*: https://arxiv.org/pdf/2407.08683
- *Relevant Paper*: **AgentInstruct: Toward Generative Teaching with Agentic Flows**
    - *Why it's relevant*: This paper introduces AgentInstruct, an agentic framework for automatically creating diverse and high-quality synthetic data for post-training language models. It demonstrates the utility of this approach by teaching language models various skills, resulting in significant improvements on several benchmarks.
    - *Read more*: https://arxiv.org/pdf/2407.03502
- *Relevant Paper*: **CosmoCLIP: Generalizing Large Vision-Language Models for Astronomical Imaging**
    - *Why it's relevant*: This paper introduces CosmoCLIP, a framework for fine-tuning pre-trained CLIP model using astronomical image-text data, enabling better generalization across in-domain and out-of-domain tasks in astronomy.
    - *Read more*: https://arxiv.org/pdf/2407.07315
- *Relevant Paper*: **LLaVA-NeXT-Interleave: Tackling Multi-image, Video, and 3D in Large Multimodal Models**
    - *Why it's relevant*: This paper introduces LLaVA-NeXT-Interleave, which tackles Multi-image, Multi-frame (video), Multi-view (3D), and Multi-patch (single-image) scenarios in Large Multimodal Models, demonstrating the importance of interleaved data format for enhanced capabilities.
    - *Read more*: https://arxiv.org/pdf/2407.07895

## Research Question: How can the human-in-the-loop approach improve the model training process?

- *Relevant Paper*: **Video-STaR: Self-Training Enables Video Instruction Tuning with Any Supervision**
    - *Why it's relevant*: This paper introduces Video-STaR, a video self-training approach that enables the utilization of any labeled video dataset for video instruction tuning. The model cycles between instruction generation and finetuning, improving general video understanding and adapting to novel tasks with existing supervision.
    - *Read more*: https://arxiv.org/pdf/2407.06189
- *Relevant Paper*: **MiraData: A Large-Scale Video Dataset with Long Durations and Structured Captions**
    - *Why it's relevant*: This paper presents MiraData, a high-quality video dataset with long durations, detailed structured captions, and high motion intensity, providing valuable data for training models capable of generating complex and consistent video content. The paper also introduces MiraBench for evaluating video generation models.
    - *Read more*: https://arxiv.org/pdf/2407.06358
- *Relevant Paper*: **VIMI: Grounding Video Generation through Multi-modal Instruction**
    - *Why it's relevant*: This paper addresses the lack of visual grounding in text-to-video diffusion models by constructing a large-scale multimodal prompt dataset and implementing a two-stage training strategy. The proposed VIMI model demonstrates multimodal understanding and generates grounded videos with consistent temporal coherence.
    - *Read more*: https://arxiv.org/pdf/2407.06304
- *Relevant Paper*: **Graph-Based Captioning: Enhancing Visual Descriptions by Interconnecting Region Captions**
    - *Why it's relevant*: This paper proposes a new annotation strategy called graph-based captioning (GBC) that describes images using a labelled graph structure with various node types. GBC enhances the representation of complex scenes by encoding hierarchical information, benefiting downstream models for visual understanding.
    - *Read more*: https://arxiv.org/pdf/2407.06723
- *Relevant Paper*: **CrowdMoGen: Zero-Shot Text-Driven Collective Motion Generation**
    - *Why it's relevant*: This paper introduces CrowdMoGen, a zero-shot text-driven framework for crowd motion generation, leveraging the power of Large Language Model (LLM) for planning and generating crowd motions based on textual descriptions. This approach provides a scalable and generalizable solution for various crowd motion generation tasks.
    - *Read more*: https://arxiv.org/pdf/2407.06188
- *Relevant Paper*: **AgentInstruct: Toward Generative Teaching with Agentic Flows**
    - *Why it's relevant*: This paper introduces AgentInstruct, an agentic framework for automatically creating diverse and high-quality synthetic data for post-training language models, enabling the teaching of new skills and behaviors to language models.
    - *Read more*: https://arxiv.org/pdf/2407.03502
- *Relevant Paper*: **TheoremLlama: Transforming General-Purpose LLMs into Lean4 Experts**
    - *Why it's relevant*: This paper proposes TheoremLlama, a framework for transforming general-purpose LLMs into Lean4 experts by utilizing a bootstrapped dataset and training approaches for formal theorem proving. This human-in-the-loop approach leverages the NL reasoning ability of LLMs for formal reasoning.
    - *Read more*: https://arxiv.org/pdf/2407.03203

## Research Question: What are the latest applications of generative AI in user interface design and engineering?

- *Relevant Paper*: **RodinHD: High-Fidelity 3D Avatar Generation with Diffusion Models**
    - *Why it's relevant*: This paper presents RodinHD, a model for generating high-fidelity 3D avatars from portrait images, which can be used in user interface design and engineering for creating realistic virtual representations of users.
    - *Read more*: https://arxiv.org/pdf/2407.06938
- *Relevant Paper*: **Still-Moving: Customized Video Generation without Customized Video Data**
    - *Why it's relevant*: This paper introduces Still-Moving, a framework for customizing a text-to-video model without requiring customized video data. This can be applied to user interface design and engineering for creating customized video content based on user preferences or specific requirements.
    - *Read more*: https://arxiv.org/pdf/2407.08674
- *Relevant Paper*: **VEnhancer: Generative Space-Time Enhancement for Video Generation**
    - *Why it's relevant*: This paper presents VEnhancer, a framework for enhancing the quality of AI-generated videos by increasing their spatial and temporal resolution. This can be used for user interface design and engineering to create more immersive and engaging user experiences.
    - *Read more*: https://arxiv.org/pdf/2407.07667
- *Relevant Paper*: **This&That: Language-Gesture Controlled Video Generation for Robot Planning**
    - *Why it's relevant*: This paper introduces a method for robot planning using language-gesture controlled video generation. This can be applied to user interface design and engineering for creating intuitive interfaces for interacting with robots.
    - *Read more*: https://arxiv.org/pdf/2407.05530

## Research Question: How to make it easier to explain AI system’s behavior?

- *Relevant Paper*: **Gradient Boosting Reinforcement Learning**
    - *Why it's relevant*: This paper introduces GBRL, a framework that extends the advantages of Gradient Boosting Trees (GBT) to the reinforcement learning domain. GBTs are known for their interpretability and lightweight implementations, which can make it easier to explain AI system behavior.
    - *Read more*: https://arxiv.org/pdf/2407.08250
- *Relevant Paper*: **From Loops to Oops: Fallback Behaviors of Language Models Under Uncertainty**
    - *Why it's relevant*: This paper analyzes fallback behaviors of language models under uncertainty and categorizes them as sequence repetitions, degenerate text, and hallucinations. Understanding these behaviors can help explain the model's decision-making process and improve its reliability.
    - *Read more*: https://arxiv.org/pdf/2407.06071
- *Relevant Paper*: **Do Vision and Language Models Share Concepts? A Vector Space Alignment Study**
    - *Why it's relevant*: This paper investigates the relationship between representations learned by language models and vision models, providing insights into the underlying mechanisms of concept representation and potential for explaining AI system behavior.
    - *Read more*: https://arxiv.org/pdf/2302.06555
- *Relevant Paper*: **How do you know that? Teaching Generative Language Models to Reference Answers to Biomedical Questions**
    - *Why it's relevant*: This paper introduces a biomedical retrieval-augmented generation (RAG) system that enhances the reliability of generated responses by referencing relevant abstracts from PubMed. This approach increases transparency and accountability by providing evidence for generated answers.
    - *Read more*: https://arxiv.org/pdf/2407.05015

## Research Question: No relevant paper was found for this question. 
