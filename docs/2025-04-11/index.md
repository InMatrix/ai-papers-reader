---
layout: default
title: 2025-04-11
permalink: /2025-04-11/
---

# 2025-04-11

## Generative AI for Assisting Software Developers

### T1: Tool-integrated Self-verification for Test-time Compute Scaling in Small Language Models

**Relevance:** This paper focuses on improving the performance of small language models by integrating external tools, such as code interpreters, for verification tasks. This is highly relevant because it directly addresses the challenges of using generative AI to assist software developers by leveraging tools to enhance the reliability and accuracy of generated code or suggestions, which is crucial for practical applications. This could lead to more robust and trustworthy AI-powered coding assistants.

ðŸ’¡ **[Summary](2504.04718/)** ðŸ“„ **[Full paper](https://arxiv.org/pdf/2504.04718)**

### SkillWeaver: Web Agents can Self-Improve by Discovering and Honing Skills

**Relevance:** While not directly about code generation, SkillWeaver's ability to learn and refine skills autonomously through web interaction has implications for creating AI agents that can assist developers. Imagine an agent that learns to use various software libraries and APIs, then uses this knowledge to complete tasks for the developer. The skills learned in SkillWeaver can be leveraged to automate tasks in software development.

ðŸ’¡ **[Summary](2504.07079/)** ðŸ“„ **[Full paper](https://arxiv.org/pdf/2504.07079)**

## AI Agents

### SkillWeaver: Web Agents can Self-Improve by Discovering and Honing Skills

**Relevance:** SkillWeaver introduces a framework where agents autonomously discover, execute, and distill practice experiences into APIs, enabling self-improvement. This directly addresses the AI agent research area by providing a mechanism for agents to learn and refine skills in complex web environments. This is highly relevant to HCI as it explores how agents can independently acquire capabilities, potentially leading to more adaptive and useful interactive systems.

ðŸ’¡ **[Summary](2504.07079/)** ðŸ“„ **[Full paper](https://arxiv.org/pdf/2504.07079)**

### DiaTool-DPO: Multi-Turn Direct Preference Optimization for Tool-Augmented Large Language Models

**Relevance:** This paper presents DiaTool-DPO, a method to enhance dialogue capabilities of Tool-Augmented Language Models (TA-LLMs). By modeling interactions as Markov Decision Processes and optimizing dialogue flows using Direct Preference Optimization, this work significantly improves the ability of TA-LLMs to handle incomplete or out-of-scope requests. The ability of AI agents to handle multi-turn interactions is important in HCI as this enables the agents to interact with the users more naturally.

ðŸ’¡ **[Summary](2504.02882/)** ðŸ“„ **[Full paper](https://arxiv.org/pdf/2504.02882)**

### A Unified Agentic Framework for Evaluating Conditional Image Generation

**Relevance:** This paper introduces CIGEval, a framework that utilizes LLMs as agents to evaluate conditional image generation. It highlights the potential for automating the assessment of AI-generated content with human-level reliability. While focused on image generation, the agentic evaluation approach is valuable for developing agents with the capability to evaluate their own performance.

ðŸ’¡ **[Summary](2504.07046/)** ðŸ“„ **[Full paper](https://arxiv.org/pdf/2504.07046)**

## Prompt Engineering Techniques

### Self-Steering Language Models

**Relevance:** This paper introduces DisCIPL, a method for 'self-steering' LMs. A planner model generates a task-specific inference program that is executed by a population of follower models. The Planner model essentially crafts a sophisticated and dynamic 'prompt' or series of instructions for the follower models, which can be seen as an advanced form of prompt engineering.

ðŸ’¡ **[Summary](2504.07081/)** ðŸ“„ **[Full paper](https://arxiv.org/pdf/2504.07081)**

## Human-in-the-loop Machine Learning

### Efficient Reinforcement Finetuning via Adaptive Curriculum Learning

**Relevance:** AdaRFT dynamically adjusts the difficulty of training problems based on the model's recent reward signals, providing an optimal difficulty range for the model. This adaptive approach, while not directly involving human feedback in real-time, mimics a human tutor who adjusts the curriculum based on the learner's progress. This relates to the core idea of human-in-the-loop learning, where the system adapts to facilitate effective learning and collaboration. The findings are relevant to creating ML systems that can adaptively learn from human input.

ðŸ’¡ **[Summary](2504.05520/)** ðŸ“„ **[Full paper](https://arxiv.org/pdf/2504.05520)**

### COIG-P: A High-Quality and Large-Scale Chinese Preference Dataset for Alignment with Human Values

**Relevance:** This paper presents a method for automatically creating preference datasets, COIG-P, by using LLMs to generate and score chosen-rejected response pairs. While the creation process lacks direct human annotators, the dataset aims to align LLMs with human values. The reward model trained on the dataset can then be used in a Reinforcement Learning from Human Feedback (RLHF) setting, enabling the AI to better respond to human needs.

ðŸ’¡ **[Summary](2504.05535/)** ðŸ“„ **[Full paper](https://arxiv.org/pdf/2504.05535)**

## Techniques for Explaining AI Behavior

### OLMoTrace: Tracing Language Model Outputs Back to Trillions of Training Tokens

**Relevance:** OLMoTrace provides a tool to trace language model outputs back to their training data, offering insights into the origins of model responses. This addresses the need for explainability by allowing users to see the context from which the model learned specific patterns or facts. By providing this traceability, the system supports understanding the basis for AI decisions and predictions.

ðŸ’¡ **[Summary](2504.07096/)** ðŸ“„ **[Full paper](https://arxiv.org/pdf/2504.07096)**

### Why Reasoning Matters? A Survey of Advancements in Multimodal Reasoning (v1)

**Relevance:** This survey provides an overview of reasoning techniques in both textual and multimodal LLMs. It highlights the challenges of multimodal reasoning, such as handling conflicting information across modalities. By understanding the limitations of existing reasoning models, researchers can develop more transparent and interpretable AI systems.

ðŸ’¡ **[Summary](2504.03151/)** ðŸ“„ **[Full paper](https://arxiv.org/pdf/2504.03151)**

