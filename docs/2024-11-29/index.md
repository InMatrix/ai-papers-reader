---
layout: default
title: 2024-11-29
permalink: /2024-11-29/
---

# 2024-11-29

## Generative AI for Assisting Software Developers

No paper recommendations for this topic.

## AI Agents

### Large Language Model-Brained GUI Agents: A Survey

**Relevance:** This survey paper extensively covers LLM-powered GUI agents, which are a prime example of sophisticated AI agents. These agents perceive their environment (the GUI), reason about tasks (user instructions), plan actions (sequences of GUI interactions), and execute them autonomously. The survey highlights advancements in creating agents that interact with digital tools and environments, aligning directly with AI agent research.

ðŸ’¡ **[Summary](2411.18279/)** ðŸ“„ **[Full paper](https://arxiv.org/pdf/2411.18279)**

### ShowUI: One Vision-Language-Action Model for GUI Visual Agent

**Relevance:** ShowUI presents a vision-language-action model designed as a GUI visual agent. This agent interacts with a GUI, understands visual and textual inputs, and performs actions within the GUI.  The innovations such as UI-Guided Visual Token Selection and Interleaved Vision-Language-Action Streaming are significant advancements in enabling agents to efficiently perceive, reason and act within complex digital environments, thus relevant to AI agent research.

ðŸ’¡ **[Summary](2411.17465/)** ðŸ“„ **[Full paper](https://arxiv.org/pdf/2411.17465)**

## Prompt Engineering Techniques

### MolReFlect: Towards In-Context Fine-grained Alignments between Molecules and Texts

**Relevance:** MolReFlect uses a teacher-student framework and in-context learning to improve the alignment between molecules and their textual descriptions. The 'in-context selective reflection' and 'chain-of-thought' approaches are sophisticated prompt engineering techniques that improve the model's ability to understand and generate complex relationships, mirroring the principles of few-shot learning and chain-of-thought prompting.

ðŸ’¡ **[Summary](2411.14721/)** ðŸ“„ **[Full paper](https://arxiv.org/pdf/2411.14721)**

## Human-in-the-loop Machine Learning

### Interactive Medical Image Segmentation: A Benchmark Dataset and Baseline

**Relevance:** This paper introduces IMed-361M, a benchmark dataset for interactive medical image segmentation, a clear example of human-in-the-loop ML.  The model uses human input (clicks, bounding boxes, text prompts) to improve segmentation accuracy. The creation of the dataset itself involved human verification, demonstrating a human-centered approach to model development and evaluation.

ðŸ’¡ **[Summary](2411.12814/)** ðŸ“„ **[Full paper](https://arxiv.org/pdf/2411.12814)**

### SketchAgent: Language-Driven Sequential Sketch Generation

**Relevance:** SketchAgent uses human-computer interaction to create sketches. Users interact conversationally with the model, providing language-driven instructions to iteratively refine the sketch.  This exemplifies human-in-the-loop learning, where human feedback (the conversational interaction) guides the model's creation process and improves the quality of the output.

ðŸ’¡ **[Summary](2411.17673/)** ðŸ“„ **[Full paper](https://arxiv.org/pdf/2411.17673)**

## Techniques for Explaining AI Behavior

No paper recommendations for this topic.

