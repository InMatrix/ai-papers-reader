---
layout: paper
pdf_url: https://arxiv.org/pdf/2506.03817
permalink: 2025-06-06/2506.03817/
title: Active Learning's Achilles Heel&#58; A Massive Study Uncovers Hyperparameter
  Chaos
---



Active learning (AL), a technique designed to minimize the amount of labeled data needed for machine learning, is often touted as a cost-effective solution. However, a new study from researchers at Technische Universität Dresden and Technion - Israeli Institute of Technology reveals a major challenge: the vast and largely unexplored hyperparameter space surrounding AL. This complexity, the study argues, leads to contradictory and often irreproducible results, hindering AL's widespread adoption in real-world applications.

The researchers embarked on a massive experimental grid search, testing over 4.6 million hyperparameter combinations. This is, by far, the most comprehensive AL study to date. Their work analyzed the impact of various hyperparameters on AL performance, revealing surprising insights and offering practical recommendations for future research.

"Annotating data is a time-consuming and costly task, but it is inherently required for supervised machine learning," the authors note. AL attempts to address this by strategically selecting the most informative unlabeled samples for expert annotation. However, the study highlights that setting up an effective AL system is far from straightforward.

One of the key challenges is the lack of consensus on which AL strategy is the "best." Previous surveys have yielded conflicting recommendations, with some favoring uncertainty-based strategies while others find non-uncertainty-based methods to be superior. This inconsistency stems from the fact that AL performance is heavily influenced by numerous hyperparameters, including the dataset, evaluation metrics, and specific implementation details.

To illustrate the importance of hyperparameters, consider the batch size – the number of samples selected for annotation in each AL iteration.  The study found that large batch sizes tend to produce similar results, as do small batch sizes. However, large batch sizes can differ significantly from small ones. Therefore, selecting the right batch size can be vital for the overall performance. The researchers recommend including at least one small and one large batch size in experiments to better understand the effect. 

Another crucial, and somewhat unexpected, finding pertains to the *implementation* of AL strategies. Even when using the same underlying strategy, different implementations (e.g., using different AL libraries) can lead to markedly different results. This implies that seemingly minor coding choices can have a substantial impact on AL performance.  "The specific implementation of AL strategies can impact performance more than the underlying strategy itself," the study warns. This underscores the importance of carefully documenting and controlling all aspects of the AL implementation.

The study also delves into the complexities of evaluating AL performance. Traditional machine learning metrics, such as accuracy and F1-score, can be misleading when applied to AL. The authors advocate for using a class-weighted F1-score in combination with the complete arithmetic mean of all AL cycles. This aggregation method captures both the overall performance and the progress made throughout the AL process.

Ultimately, the researchers offer practical recommendations for designing reproducible AL experiments.  They emphasize the importance of using a diverse set of datasets, and they recommend including at least 4,000 randomly drawn hyperparameter combinations to ensure robust results.

While acknowledging the challenges, the authors remain optimistic about the potential of AL. "Our hopefully most significant contribution to the AL research field is that in the end, it does not matter which hyperparameters you evaluate. The results will be reproducible and comparable to other works if you sample a few hyperparameter combinations from an extensive hyperparameter grid." By shining a light on the often-overlooked complexities of hyperparameter tuning, this study paves the way for more reliable and trustworthy AL research in the future.