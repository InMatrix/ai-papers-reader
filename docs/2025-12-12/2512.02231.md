---
layout: paper
pdf_url: https://arxiv.org/pdf/2512.02231
permalink: 2025-12-12/2512.02231/
title: New Benchmark Reveals Major Gap in AI’s Audiovisual Speech Understanding
---



Multimodal Large Language Models (MLLMs), touted for their ability to interpret video, audio, and text simultaneously, still struggle to master a fundamental human skill: integrating what they see with what they hear, especially when focusing on speech.

A new benchmark, **AV-SpeakerBench**, introduced by researchers from the University of Wisconsin-Madison, Kookmin University, and Carnegie Mellon University, reveals that even leading MLLMs remain significantly behind human performance in fine-grained, speaker-centric audiovisual reasoning.

The paper, titled "See, Hear, and Understand: Benchmarking Audiovisual Human Speech Understanding in Multimodal Large Language Models," addresses a critical failing in existing video benchmarks. Most current tests are often "unimodal solvable"—meaning the correct answer can be guessed using only visual cues (like counting people) without needing the audio stream.

AV-SpeakerBench shifts the focus from scene-level understanding to **speaker-centric reasoning**, consisting of 3,212 multiple-choice questions curated from real-world videos involving multiple visible speakers.

### Why Fusion is Essential

The core principle of the new benchmark is "fusion-grounded question design," where solving the question explicitly requires linking auditory events (speech content, timing, attributes) to visual identities (who is speaking, their clothing, or their actions).

For instance, instead of asking "What color is the car?", AV-SpeakerBench poses questions like:

1.  **Speaker Recognition:** "Who says, 'Oh, I see what's going on?'" requiring the model to identify the spoken utterance and correctly attribute it to the visually distinct individual (e.g., "The man in the black suit").
2.  **Temporal Grounding:** "After the man in the blue sweater puts on the headphones, how many times is 'I remember it' mentioned?" forcing the model to align a spoken phrase count to a precise visual anchor in the video timeline.
3.  **Paralinguistic Attributes:** "Among those who speak, who has the lowest pitch?" demanding the model integrate the speaker's visual identity with the fine-grained acoustic properties of their voice.

### Performance Falls Short

Human annotators achieved an overall accuracy of 93.74% on the benchmark, confirming the questions are naturally solvable through combined sight and sound. In stark contrast, the strongest tested model, Google’s proprietary **Gemini 2.5 Pro**, achieved 73.04%. The recently released Gemini 3 Pro showed improvement, reaching 77.62%, but a substantial gap remains.

Analysis showed that the performance discrepancy stems primarily from weak audiovisual fusion capabilities. While Gemini 2.5 Pro consistently gained 10-20% accuracy when audio was provided (compared to vision-only), advanced open-source models like Qwen3-Omni-30B showed only modest gains, suggesting they often fail to leverage or even misuse the audio stream.

Furthermore, a study of model failures highlighted that most errors cluster around two areas: **audio perception** (mishearing or misinterpreting acoustic details) and **temporal grounding** (failing to correctly align the timing of speech events with visual actions).

The researchers conclude that AV-SpeakerBench establishes a rigorous foundation necessary to push future MLLMs toward genuine audiovisual understanding, moving beyond simple visual perception toward true integration of who is speaking, what is said, and when.