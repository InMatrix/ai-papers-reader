---
layout: paper
pdf_url: https://arxiv.org/pdf/2512.07222
permalink: 2025-12-12/2512.07222/
title: AI Models Gain 'Free Robustness' by Ignoring Filler Words
---



A team of researchers has introduced a novel defense mechanism for Vision-Language Models (VLMs) that solves a long-standing trade-off between model performance and security. By simply reducing the attention paid to common function words—such as "the," "is," or "to"—the new method drastically increases robustness against adversarial attacks without sacrificing accuracy or incurring high computational costs.

The technique, dubbed Function-word De-Attention (FDA), challenges the conventional wisdom that VLMs must be adversarially trained on corrupted data to resist malicious perturbations. Instead, the authors hypothesized that function words, which are ubiquitous but carry minimal semantic information (unlike content words like "noun" or "verb"), act as weak links that attackers exploit to confuse the model.

### The Function Word Vulnerability

Adversarial attacks on VLMs typically involve subtly altering an image or text input—changes invisible to humans—to force the model into making a mistake, such as retrieving the wrong image or misidentifying an object.

The researchers confirmed their hypothesis by observing how attention scores shift during a successful attack. Before an attack, function words show low similarity to corresponding image regions. However, after a targeted attack, the input image exhibited a higher similarity score toward function words than to content words in over 80% of test cases. This indicates that attackers exploit these low-specificity words to align misleading adversarial noise with image features.

The impact is intuitive: in one example visualized using attention maps, a VLM was asked to process an image showing a female student performing a kick. When the image was perturbed by adversarial noise, the VLM’s attention—guided by the distraction of the function words—partially shifted away from the student and toward a nearby male instructor. By simply masking or de-attending the function words, the VLM successfully "looked back at" the female student.

### Differential Subtraction for Robustness

To implement this defense, FDA operates like a "differential amplifier" within the VLM’s fusion encoder. It works in parallel with the standard cross-attention calculation (which aligns visual and textual features). FDA calculates a *distraction attention* score based only on the function words and the input image.

This distraction score is then *subtracted* from the original attention score. This differential subtraction forces the VLM to rely less on the noisy, vulnerable alignments created by the function words, thus improving vision-language alignment (VLA) and overall stability.

The results, tested extensively across models like ALBEF, TCL, and BLIP on tasks including image retrieval and visual grounding, demonstrated unprecedented gains. FDA achieved an average Attack Success Rate (ASR) drop of between 18% and 53% on retrieval tasks and over 90% on visual grounding tasks. Crucially, these massive robustness gains came with a negligible clean performance drop—often less than 1%, and in some cases, even a slight clean performance gain.

This "free robustness" is a significant breakthrough, offering a straightforward, plug-and-play defense that bypasses the computational complexity and performance degradation associated with traditional adversarial fine-tuning methods.