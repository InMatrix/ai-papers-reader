---
layout: paper
pdf_url: https://arxiv.org/pdf/2512.08923
permalink: 2025-12-12/2512.08923/
title: Same Content, Different Answers&#58; Study Reveals Cross-Modal Inconsistency
  in Top AI Models
---



New research leveraging specialized "stress tests" has revealed a fundamental flaw in state-of-the-art Multimodal Large Language Models (MLLMs): a significant inability to reason consistently when the same semantic information is provided across different input formats, such as native text versus rendered images.

While MLLMs are designed to integrate vision and language in a shared internal space, this study found that models often produce conflicting answers depending on whether the prompt is given as plain text, an image of that text, or a mix of both. This phenomenon, dubbed cross-modal inconsistency, suggests that even when a model can successfully read text presented visually (via Optical Character Recognition, or OCR), it often fails to reason about that information as effectively as when receiving native text.

To measure this vulnerability systematically, researchers introduced the **Render-Equivalence Stress Tests (REST and REST+)** benchmarks. The tests present semantically identical problems—including mathematical system-of-equations puzzles (SOEBENCH)—in three distinct modalities: text-only, image-only (where the text is rendered as an image), and mixed (context as image, question as text).

For example, a model might be asked to solve a simple linear system like "A + 3B = 6" and "3B + C + A = 10." If the model is asked this question three times—once as plain text, once as a high-resolution image of the equations, and once with the equations in an image and the final question in text—a cross-modal inconsistency occurs if it provides a correct answer for one format but fails on another.

Evaluating 15 leading MLLMs, the study found substantial inconsistency across the board. Even top performers like GPT-5-mini and Claude Haiku achieved consistency scores (Render-Equivalence Rate, RER) slightly above 90%, meaning they still fail to consistently answer 8-10% of solvable questions across all input types. Many other models scored far lower, with some exhibiting consistency as low as 6.6%.

A critical finding was that this failure is not merely a consequence of poor OCR performance. Even when models correctly recognized and transcribed the text within an image, their reasoning accuracy often remained inferior to their performance on native text, confirming that the text modality is inherently preferred.

The more challenging REST+ benchmark explored the impact of visual characteristics, showing that inconsistency levels were exacerbated by factors like low image resolution. Interestingly, text presented in certain colors (such as red or yellow) sometimes yielded higher image accuracy than black text, suggesting that subtle visual cues can play an outsized role in reasoning success.

Mechanistically, the inconsistency was traced to the degree of alignment between modalities in the model’s internal embedding space, often referred to as the "modality gap." Models exhibiting higher consistency on the REST benchmarks also demonstrated greater cosine similarity between their internal representations of the same concept presented as text versus as an image.

In conclusion, the research suggests that MLLMs have not achieved true semantic equivalence between text and visual inputs. While they can perform multimodal tasks, their inherent reliance on text leads to highly format-dependent reasoning, highlighting an untapped potential that current models fail to capture consistently.