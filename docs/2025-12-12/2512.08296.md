---
layout: paper
pdf_url: https://arxiv.org/pdf/2512.08296
permalink: 2025-12-12/2512.08296/
title: New Science of AI Scaling Reveals Multi-Agent Systems Aren't Always Better,
  Performance Hinges on Task Structure
---



A landmark study challenges the prevailing heuristic that simply adding more language model (LLM) agents improves performance, establishing the first quantitative scaling principles for multi-agent systems (MAS). Researchers from Google Research, Google DeepMind, and MIT conducted a controlled evaluation across 180 configurations, revealing that the effectiveness of agent teams is governed by measurable trade-offs between coordination architecture and task complexity, not just the underlying LLM capability.

The study, which examined five canonical agent architectures (Single-Agent, Independent, Centralized, Decentralized, and Hybrid) across four diverse benchmarks—financial analysis, web navigation, game planning, and workplace execution—found MAS performance can range wildly, from an +80.9% improvement to a -70% degradation, depending on task alignment.

### Three Pillars of Agentic Scaling

The research identified three dominant scaling effects that predict whether coordination succeeds or fails:

**1. The Tool-Coordination Trade-off:** Tasks requiring extensive tool use suffer disproportionately from multi-agent overhead. In tool-heavy environments (like the WorkBench benchmark with 16 tools), multi-agent architectures incurred a 2–6x efficiency penalty compared to single-agent systems. The coordination tax—the resource consumed by inter-agent communication—compounds as environmental complexity increases, making simpler architectures paradoxically more effective.

**2. The Capability Saturation Ceiling:** Coordination yields diminishing returns once a Single-Agent System (SAS) baseline exceeds approximately 45% accuracy. Beyond this empirical threshold, the overhead cost associated with coordination begins to outweigh the potential for marginal improvement, suggesting that agents should only be employed when the task is difficult enough to warrant the complexity tax.

**3. Topology-Dependent Error Amplification:** The choice of architecture dictates how errors propagate. Architectures without validation mechanisms, such as Independent systems (where agents work in parallel and aggregate isolated outputs), amplified errors by a factor of 17.2x. Conversely, Centralized coordination, which employs an orchestrator to enforce validation bottlenecks and cross-check reasoning steps, successfully contained error amplification to 4.4x.

### Task Decomposability is Key

The critical factor determining MAS success is task decomposability. When tasks can be naturally broken into parallel subtasks, coordination provides genuine value.

For example, on the Finance Agent benchmark, which involves analyzing revenue trends and market comparisons, a Centralized architecture achieved an +80.9% performance gain. Here, specialized agents could independently research different parallel information streams before the orchestrator synthesized the findings.

However, on the PlanCraft benchmark, which requires strictly sequential constraint satisfaction (e.g., crafting an item where the inventory state changes with every step), all multi-agent variants degraded performance by 39% to 70%. The coordination overhead in this sequential domain fragmented the agents' limited reasoning capacity, consuming valuable token budget on unnecessary communication rather than problem-solving.

By synthesizing these findings into a predictive framework using empirical coordination metrics (including efficiency, overhead, and error amplification), the researchers achieved a cross-validated variance explanation of R²=0.524. The resulting model provides the first quantitative criterion for selecting an agent architecture based on measurable task properties, achieving 87% accuracy in predicting the optimal architecture for unseen configurations. The team confirmed that these principles generalize to frontier models like GPT-5.2 in out-of-sample validation.

The results offer practitioners a data-driven path away from architectural guesswork, confirming that effective multi-agent deployment relies on matching the coordination structure to the inherent structure of the task.