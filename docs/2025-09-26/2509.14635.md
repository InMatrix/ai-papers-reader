---
layout: paper
pdf_url: https://arxiv.org/pdf/2509.14635
permalink: 2025-09-26/2509.14635/
title: New Benchmark Forces LLMs to Tackle Real-World Code Complexity
---



In a significant step toward developing truly intelligent software engineering tools, researchers have introduced a new benchmark and an advanced agent framework designed to test large language models (LLMs) on the full complexity of real-world codebases.

Traditionally, LLMs have been evaluated using benchmarks focused on isolated functions or small code snippets. However, effective software development requires understanding vast, interconnected repositories—tracing dependencies across multiple files, comprehending architectural designs, and performing multi-hop reasoning.

To address this gap, researchers from Shanghai Jiao Tong University developed **SWE-QA**, a repository-level code question answering (QA) benchmark. SWE-QA comprises 576 high-quality, expert-verified question-answer pairs derived from an analysis of 77,100 developer questions found in GitHub issues across 12 popular open-source Python projects (including Django, pytest, and scikit-learn).

The benchmark questions are classified into a two-level taxonomy based on developer intent:

*   **What/Why (Conceptual):** Questions about definitions, concepts, and design rationales.
*   **Where/How (Procedural/Locational):** Questions requiring multi-hop tracing of data flow, locating features, or explaining implementation steps.

To effectively tackle these challenges, the team also proposed the **SWE-QA-AGENT**, an autonomous agent framework built on the ReAct (Reasoning and Acting) paradigm. The agent is equipped with specialized tools—such as `ReadFile`, `GetRepoStructure`, and `SearchContent`—allowing it to iteratively reason, navigate the repository structure, and progressively refine its context before synthesizing an answer.

This iterative, tool-augmented approach proves essential for solving complex queries. For instance, if a developer asks, "How does SQLFluff implement its plugin system for custom rules?", a standard retrieval method (RAG) might only retrieve a single function snippet. The SWE-QA-AGENT, however, performs a multi-step search. It identifies the core component (`get_rules()`), infers the use of the `pluggy` library, and then traces the initialization and registration flow across different configuration files, providing a complete architectural explanation.

Empirical evaluations across six advanced LLMs revealed that simply prompting the models (even with standard RAG) yielded poor results, underscoring the necessity of providing grounded context. The best performance was achieved when combining the SWE-QA-AGENT framework with a powerful proprietary model, Claude 3.7 Sonnet, scoring 47.82 overall.

The agent specifically boosted scores in "Completeness" and "Reasoning," demonstrating its superior ability to handle complex, multi-hop queries compared to simpler retrieval methods.

However, the results also highlighted persistent weaknesses. Models excelled at conceptual "What" and "Why" questions, which often rely on documentation or explicit comments. They struggled significantly with procedural "How" and locational "Where" queries, which require deeply tracing code paths and cross-file dependencies—the exact challenges faced in real-world development.

By establishing SWE-QA, the researchers have created a vital new standard that mandates a genuine architectural understanding of code. This benchmark will be critical for driving future research into LLM agents capable of sophisticated, repository-level software engineering tasks.