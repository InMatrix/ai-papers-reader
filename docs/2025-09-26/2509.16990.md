---
layout: paper
pdf_url: https://arxiv.org/pdf/2509.16990
permalink: 2025-09-26/2509.16990/
title: Reinforcement Learning Turbocharges Voice AI, Boosting Generative Speech Understanding
---



IBM researchers have unveiled a significant advancement in training Speech-Aware Large Language Models (SALLMs), leveraging a cutting-edge reinforcement learning (RL) technique called Group Relative Policy Optimization (GRPO) to dramatically improve performance on open-ended, generative speech tasks.

SALLMs—the foundational models behind sophisticated voice AI that can handle both spoken audio and complex text reasoning—traditionally rely on Supervised Fine-Tuning (SFT). While effective, this approach often limits the model's ability to provide novel, high-quality, open-ended responses. The new work, detailed in a recent paper, demonstrates that training SALLMs using GRPO, paired with verifiable rewards like the BLEU score, surpasses SFT results across key generative benchmarks.

Prior RL methods used to enhance SALLMs often focused on simplified tasks, such as selecting the correct answer from a multiple-choice list, relying on binary (right/wrong) rewards. In contrast, the IBM team focused on tasks requiring genuine text generation, which better reflect real-world, chat-oriented applications.

To achieve this, the researchers incorporated the GRPO algorithm, an efficient, on-policy RL method known for its success in improving LLM reasoning. Crucially, they utilized metrics traditionally associated with translation, like the BLEU score, as a dynamic reward signal.

### Intuition: Rewarding Quality, Not Just Correctness

Imagine using an SALLM for a Spoken Question Answering (SQA) task. If you speak the prompt, “What are the top three reasons the Roman Empire declined?” the model must generate a coherent, multi-part answer. A binary reward system would fail to grade the quality, fluency, and detail of that generative output.

By using BLEU score as a reward, the model is penalized or rewarded based on how closely its generated text matches a known ground-truth reference, allowing for nuance. If the model generates, "War, corruption, and economic trouble," and the reference answer is similar, the BLEU score yields a high reward, training the model to prioritize high-quality, relevant output in subsequent generations.

The research evaluated the GRPO approach on two major open-format challenges: Spoken Question Answering (SQA) using the LibriSQA dataset and Automatic Speech Translation (AST) from English to German using the CoVoST2 dataset.

### Significant Gains Over Standard Tuning

The empirical results were compelling. On SQA, the SALLM (specifically the Granite Speech 2B model) trained with GRPO achieved a 9.8% improvement in BLEU score compared to the model fine-tuned with standard SFT. For the larger 8B parameter model, the GRPO boost was similarly strong, demonstrating scalability.

Similar gains were observed in translation. For AST (English-to-German), GRPO outperformed SFT by 3.2% in BLEU score on the 2B model, proving that the technique robustly enhances generative capabilities beyond just answering questions.

The researchers also experimented with a “Mixed-Policy GRPO,” incorporating the ground-truth answer as an 'off-policy' sample during training, suggesting future avenues for leveraging existing high-quality data to further anchor and stabilize generative model performance.

This work marks a clear step toward more capable, voice-enabled AI systems, showing that RL algorithms like GRPO are highly effective tools for extracting superior generative abilities from SALLMs in complex, open-ended scenarios.