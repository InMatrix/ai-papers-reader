---
layout: paper
pdf_url: https://arxiv.org/pdf/2509.20317
permalink: 2025-09-26/2509.20317/
title: New LLM Technique SIM-CoT Achieves Explicit Reasoning Performance with 2.3x
  Greater Efficiency
---



Large Language Models (LLMs) often rely on Chain-of-Thought (CoT) prompting to solve complex problems like mathematics and programming. While the traditional "explicit" CoT approach—where the model verbalizes every intermediate step—yields high accuracy, it generates lengthy outputs that dramatically increase inference time and computational cost.

To solve this, researchers developed "implicit" CoT, which compresses reasoning into a small number of continuous latent tokens. Implicit methods are highly efficient but have consistently lagged behind their explicit counterparts in accuracy. Now, researchers at Fudan University and the Shanghai AI Laboratory have developed Supervised Implicit Chain-of-Thought (SIM-CoT), a new training paradigm that stabilizes implicit reasoning, closes the performance gap, and offers unprecedented interpretability.

The key challenge SIM-CoT addresses is "latent instability." Previous implicit CoT models suffered catastrophic failure when researchers tried to scale the number of implicit tokens used for complex tasks. Analysis revealed that without fine-grained supervision, the latent tokens quickly degraded, becoming "semantically homogeneous."

For instance, a reasoning task might require both numbers and mathematical operators (like `+` and `*`). In failed implicit models, the internal representations collapsed, losing critical semantic diversity and encoding only numerical values. A successful model's latents maintain a large "distance" from each other to represent distinct reasoning steps, but unstable models allow these latents to converge, making complex, multi-step logic impossible.

SIM-CoT resolves this by introducing step-level implicit supervision during training. Existing implicit methods only supervise the final answer or the overall reasoning trajectory. SIM-CoT, however, employs a temporary, "plug-and-play" auxiliary decoder that aligns *each* implicit latent token with its corresponding explicit reasoning step.

Imagine an LLM solving a math word problem requiring four steps:
1. `(Calculate the discount)`
2. `(Find the remaining amount)`
3. `(Determine the final price)`
4. `(State the answer)`

During training, SIM-CoT uses the auxiliary decoder to ensure that latent token 1 captures the semantic information equivalent to step 1, latent token 2 captures step 2, and so on. This forced alignment prevents the latent space from collapsing into a homogeneous blob of numbers, ensuring stable, diverse, and logically structured internal reasoning.

Crucially, the auxiliary decoder is discarded entirely at inference time. This means SIM-CoT retains the token efficiency of fast implicit methods, but with the robustness of explicit CoT.

The results are significant: On the GPT-2 model solving math problems (GSM8K-Aug benchmark), SIM-CoT not only outperformed previous implicit methods like Coconut and CODI, but also surpassed the strong explicit CoT baseline by 2.1% accuracy while delivering a staggering **2.3 times greater token efficiency**. The gains were consistent when scaling up to larger models in the LLaMA 3 series.

Furthermore, the reusable decoder provides a mechanism for interpretability. By projecting the implicit latent tokens back onto the explicit vocabulary, researchers can now visualize the model’s internal thought process. For a complex watermelon farm problem, SIM-CoT's latents were decoded into clear, sequential expressions: `0.3*120=36`, followed by `(120-36)*3/4=63`, confirming that the model encodes semantically meaningful, ordered reasoning steps in its compact, efficient latent space. SIM-CoT thus successfully bridges the gap between fast, implicit thought and robust, structured reasoning.