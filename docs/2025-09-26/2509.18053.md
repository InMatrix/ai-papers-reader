---
layout: paper
pdf_url: https://arxiv.org/pdf/2509.18053
permalink: 2025-09-26/2509.18053/
title: AI Driving System Solves Critical Occlusion Problem with Novel ‘Graph-of-Thoughts’
  Reasoning
---



Autonomous vehicle research has introduced a significant breakthrough in cooperative driving safety, addressing the crucial challenge of occluded objects—items hidden from a vehicle’s view by large obstacles like trucks or buildings. Researchers have unveiled V2V-GoT (Vehicle-to-Vehicle Cooperative Autonomous Driving with Multimodal Large Language Models and Graph-of-Thoughts), a novel reasoning framework that significantly reduces collision risk by systematically integrating shared sensor data.

Current state-of-the-art autonomous vehicles often rely only on their immediate sensors (LiDAR and cameras). This localized approach breaks down in complex urban scenarios, such as when a large semi-trailer obscures a pedestrian or another car approaching an intersection. V2V-GoT leverages Vehicle-to-Vehicle (V2V) communication, pooling perception features from multiple Connected Autonomous Vehicles (CAVs) into a central Multimodal Large Language Model (MLLM).

The core innovation is the MLLM’s use of a "Graph-of-Thoughts" (GoT) framework. Unlike simpler methods where an MLLM gives a direct answer, the GoT forces the model through a structured, step-by-step chain of logic, similar to a human making a driving decision. The answer from a "parent" question serves as the explicit context for a "child" question, ensuring comprehensive reasoning.

This structured approach is made possible by introducing two specialized reasoning steps: **Occlusion-Aware Perception** and **Planning-Aware Prediction**.

For example, in a dense traffic scenario, the ego vehicle (CAV\_EGO) asks the MLLM: "What are the notable objects invisible to me near my planned trajectory?" Data from a cooperative vehicle (CAV\_A) on the far side of a large van might reveal a hidden scooter. The MLLM processes this shared information through a series of perception questions (Q1–Q4) to identify the "invisible object" (the scooter).

Next, the MLLM executes **Planning-Aware Prediction** (Q5–Q7). The prediction model uses the newly identified scooter as context and also factors in the planned trajectories of all other nearby CAVs. This allows the system to determine not just where the scooter is, but where it is likely to move, based on the actions of all surrounding vehicles.

Finally, the GoT feeds the refined prediction into the planning steps (Q8–Q9) to generate a Suggested Trajectory and Action Classification (e.g., "Suggested speed: slow, Suggested steering: straight"). This deliberate sequencing—moving from raw data fusion to perception, then prediction, and finally planning—is crucial for avoiding last-second critical errors.

The researchers validated V2V-GoT on a newly curated dataset, V2V-GoT-QA, built upon real-world cooperative driving scenarios. Experimental results demonstrate that the V2V-GoT model significantly outperforms existing baseline methods, achieving the lowest L2 distance errors and collision rates in cooperative planning tasks. The ablation studies confirmed that removing either the occlusion-aware perception or planning-aware prediction steps resulted in diminished performance, underscoring the critical importance of structured GoT reasoning for safe, cooperative autonomous driving.