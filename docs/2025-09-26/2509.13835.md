---
layout: paper
pdf_url: https://arxiv.org/pdf/2509.13835
permalink: 2025-09-26/2509.13835/
title: AI Models Harbor Significant Bias Against German Dialects, Amplifying Negative
  Stereotypes
---



A new study analyzing popular Large Language Models (LLMs) reveals that artificial intelligence systems systematically discriminate against German dialect speakers, mirroring and even amplifying negative human stereotypes.

The research, published on arXiv, tested major LLM families, including Llama, Gemma, and the proprietary GPT-5 Mini, finding that models consistently associate dialect speakers with undesirable traits such as being “uneducated,” “careless,” and “closed-minded.”

Dialects remain a significant part of German culture, spoken by over 40% of the population. However, sociolinguistic research shows that dialect speakers often face real-world penalties, including earning less and experiencing discrimination in hiring. The central question for the researchers was whether this prejudice is being captured and reinforced by the foundation models now driving much of global technology.

### The Job Assignment Test

To quantify the inherent bias, researchers devised two distinct tests: an implicit association task and a decision-making task. The decision task was crucial for simulating real-world outcomes.

In this task, LLMs were asked to recommend job placements for two fictitious writers—one who spoke standard German and one who used a regional dialect (such as Bavarian or Alemannic). The models consistently made stereotypical choices.

For instance, when asked to recommend a writer for the high-education job of "Psychiatrist" versus the vocational job of "Plasterer" or "Farmworker," LLMs overwhelmingly assigned the standard German writer to the specialist roles. Llama-3.1 70B, a leading model, systematically linked dialect speakers to occupations requiring lower educational levels, reflecting a strong *uneducated* bias.

### Explicit Labeling Worsens Discrimination

The study also introduced a critical distinction between two forms of bias exposure:

1.  **Dialect Usage Bias:** Providing the LLM with an actual passage written in a German dialect (e.g., a Bavarian sentence about history) compared to the standard German translation.
2.  **Dialect Naming Bias:** Explicitly labeling the writer in the prompt as a "German dialect speaker" versus a "standard German speaker."

Contrary to previous studies on other demographics, which suggested that explicit labels might reduce bias through model alignment, the German dialect bias was significantly **amplified** when the linguistic demographic was explicitly named.

“Explicitly identifying individuals as speakers of dialects amplifies bias even more than implicit cues like dialect usage,” the authors note, suggesting that LLMs display explicit discriminatory tendencies against this linguistic group.

### Biases Grow with Model Size

The findings raise particular concern because the researchers observed that larger, more sophisticated LLMs within the same family (e.g., the 70 billion parameter versions compared to the 8 billion parameter versions) exhibited stronger, more pronounced biases. The maximum dialect naming bias for the trait *uneducated* reached 0.98 (out of a possible 1.0) in models like GPT-5 Mini and Gemma-3 12B, indicating an almost perfect correlation with the negative stereotype.

Furthermore, a robustness check confirmed that LLMs are not simply treating dialect text as "noisy" or erroneous input; the bias is targeted at the dialectal form itself, independent of the text's content. The study highlights an urgent need for safety efforts, which often focus on factors like sexism or racism, to be broadened to address the pervasive and overlooked issue of dialect bias in artificial intelligence systems.