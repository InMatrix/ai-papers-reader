---
layout: paper
pdf_url: https://arxiv.org/pdf/2509.14745
permalink: 2025-09-26/2509.14745/
title: Autonomous AI Coding Agents Prove Highly Effective on GitHub, But Human Oversight
  Remains Critical
---



A new empirical study tracking the performance of autonomous AI coding agents on GitHub reveals that these tools are becoming powerful software development teammates, successfully submitting pull requests (PRs) that are accepted by human maintainers over 83% of the time.

The research, conducted by an international team from institutions including Nara Institute of Science and Technology and Queen's University, analyzed 567 PRs generated by Claude Code, an agentic coding tool by Anthropic, across 157 diverse open-source projects. Agentic coding differs from typical LLM usage (like ChatGPT) by enabling the AI to autonomously plan, execute, test, and submit complex development tasks with minimal human intervention.

While highly successful, the findings underscore that human developers still play a vital role in ensuring quality and adherence to project standards.

### Agents Excel at Maintenance and Quality

The study found that AI agents are strategically deployed for different tasks than their human counterparts. While both Agentic-PRs (APRs) and Human-PRs (HPRs) focus on bug fixes and new features, the agents overwhelmingly concentrate on non-functional improvements crucial for code health.

Specifically, APRs were found to focus far more often on:

1.  **Refactoring and Code Structure:** 24.9% of APRs targeted refactoring (restructuring code for maintainability without changing external behavior), compared to only 14.9% of HPRs.
2.  **Testing:** Contributions to testing were substantially more frequent in APRs (18.8% vs. 4.5% for HPRs). For example, agents were observed significantly improving test coverage, such as raising one project’s coverage from 70% to 94% by systematically adding comprehensive test suites for previously untested code paths.
3.  **Documentation:** APRs addressed documentation updates in 22.1% of cases, suggesting agents are key in maintaining textual artifacts like READMEs and code comments.

Furthermore, 40% of Agentic-PRs were multi-purpose (e.g., combining a feature implementation with necessary test updates simultaneously), showcasing the agent's ability to deliver robust, cohesive changes in a single submission.

### High Acceptance, But Trust Issues Persist

Project maintainers accepted and merged 83.8% of Agentic-PRs, a rate lower than the 91.0% observed for Human-PRs but still indicative of wide real-world adoption. When accepted, 54.9% of Agentic-PRs were merged without any further revision, confirming that agents can frequently produce integration-ready code.

For the PRs that were rejected, the reasons were often structural or procedural, not technical. Rejections were typically driven by project context, such as the changes being implemented by another developer, the submission being overly large, or the PR becoming obsolete due to evolving requirements.

However, when revisions were required (45.1% of accepted PRs), human intervention was necessary to address critical quality gaps. The most common targets for revision were:

*   **Bug fixes (45.1%):** The agent-generated code often required functional fixes, particularly involving critical error handling that the AI had overlooked.
*   **Documentation updates (27.4%):** Reviewers ensured documentation and comments accurately matched the new code.
*   **Refactoring (25.7%) and Style (22.1%):** These revisions were necessary to align AI-generated code with the project’s specific architectural decisions and coding conventions (e.g., standardizing variable naming or indentation).

The study suggests that developers can reduce friction by ensuring they give agents small, well-defined tasks and by explicitly embedding project-specific style and architecture rules—such as via a dedicated guidelines file—to ensure AI outputs adhere to human expectations.