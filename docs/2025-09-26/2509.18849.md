---
layout: paper
pdf_url: https://arxiv.org/pdf/2509.18849
permalink: 2025-09-26/2509.18849/
title: Dynamic Advantage Mixing Stabilizes Foundation Model Reasoning
---



Foundation models (FMs) excel at complex reasoning—from solving geometry problems to understanding emotional cues—a capability often enhanced through Reinforcement Learning (RL) techniques like Group Relative Policy Optimization (GRPO). However, researchers have identified a critical flaw in how GRPO evaluates potential reasoning paths, leading to unstable and sometimes inaccurate performance.

A new paper proposes the **Mixed Advantage Policy Optimization (MAPO)** method, an easy yet effective strategy that dynamically adjusts how reasoning trajectories are scored. MAPO addresses two fundamental issues—Advantage Reversion and Advantage Mirror—that plague existing RL training methods for language models.

In GRPO, the "advantage function" is used to rank a group of generated reasoning paths (trajectories). A fixed formulation based on statistical normalization (z-score) is typically used, which assumes uniform applicability across all scenarios.

The authors of MAPO found that this fixed approach fails because different queries exhibit varying degrees of **Trajectory Certainty**, defined by the consistency of successful outcomes across multiple rollouts.

For instance, consider a highly successful batch of reasoning attempts with reward scores like [0.9, 1.0, 1.0, 1.0]. Because the scores are very tight, the standard deviation ($\sigma$) is tiny. The resulting *Advantage Reversion* problem means the trajectory with the 0.9 reward receives an exaggerated negative advantage (an overly harsh penalty) simply because the variance is near zero.

Conversely, the *Advantage Mirror* problem occurs when two semantically opposite batches—say, a nearly flawless one [0.9, 1.0, 1.0, 1.0] and a nearly failed one [0, 0.1, 0.1, 0.1]—yield the exact same set of normalized advantage scores. The model cannot distinguish between easy successes and difficult failures, hindering effective learning.

MAPO overcomes this by introducing a mixture approach driven by certainty.

First, for high-certainty samples (where variance instability is high), the team developed the **Advantage Percent Deviation (APD)**. APD replaces the unstable variance normalization with a relative normalization based on the mean reward ($\frac{r_i - \mu}{\mu}$). This provides a stable, proportional measure of deviation, eliminating the risk of near-zero division.

Second, MAPO uses **Trajectory Certainty Reweight (TCR)** to dynamically combine the standard, variance-sensitive advantage function and the new mean-based APD. When the trajectory certainty is low (i.e., the model is highly uncertain, success probability $p \approx 0.5$), the standard deviation-based advantage is prioritized, encouraging exploration. When certainty is high ($p \approx 0$ or $p \approx 1$), the APD takes over, ensuring stable refinement.

Tested on multimodal reasoning tasks, including geometry (Geo3K) and emotional analysis (EmoSet), MAPO consistently outperformed existing methods like GRPO and DAPO. By adaptively configuring the advantage function based on the characteristics of each sample, MAPO provides a more reliable optimization signal, leading to more stable and accurate reasoning performance without requiring additional architectures or complex hyper-parameter tuning.