---
layout: paper
pdf_url: https://arxiv.org/pdf/2407.04842
permalink: 2024-07-12/2407.04842/
title: New Benchmark Reveals Flaws in AI Judges, Crowns GPT-4o as Current Best Critic
---



Text-to-image (T2I) systems like Stable Diffusion and DALL-E 3 rely heavily on automated judges—known as Multimodal Reward Models (MRMs)—to align their output with human preferences regarding safety, quality, and prompt adherence. However, a new study by researchers introducing the **MJ-BENCH** benchmark reveals that many of these judges are inadequately evaluated, leading to persistent issues in generative AI, including hallucination and bias.

The MJ-BENCH platform, detailed in a recent paper, aims to provide the first comprehensive evaluation of these AI judges across four critical dimensions: **text-image alignment, safety, image quality, and generation bias.**

The need for strict evaluation is critical because if the judge is flawed, the resulting image model will be misaligned. For instance, in terms of **alignment**, a model must handle complex instructions like generating "a blue cat and a red car." An inadequate judge might approve an image showing a "red cat and a blue car"—a factual contradiction or hallucination.

The researchers tested a wide array of judges, from smaller, pre-trained scoring models (like HPS-v2.1 and PickScore) to large Vision-Language Models (VLMs) such as GPT-4o, Gemini Ultra, and various open-source models.

### Closed-Source VLMs Lead the Pack

The evaluation results showed that proprietary, closed-source VLMs generally provided better feedback across all metrics, with **GPT-4o emerging as the top-performing judge** on average, demonstrating high accuracy in discerning preferred images.

However, the study uncovered a nuanced split in capabilities between different model types:

1.  **Scoring Models vs. VLMs:** Smaller CLIP-based scoring models proved highly effective at assessing objective metrics like text-image alignment and image quality, likely due to their extensive pre-training on visual-text data.
2.  **Reasoning Superiority:** VLMs, which incorporate stronger reasoning capabilities often employing a "Chain-of-Thought" analysis, vastly outperformed smaller models on complex tasks like **safety** and **bias.** For example, judging "evasive" prompts—where users attempt to circumvent restrictions (e.g., generating nudity under the guise of "European artistic style")—requires sophisticated contextual understanding that only VLMs could reliably handle.

### The Problem with Numerical Ratings

A significant finding related to model consistency concerned the feedback format. The research demonstrated that VLM judges provided more accurate and stable feedback when using **natural language Likert scales** (e.g., "Extremely Poor" to "Outstanding") rather than trying to quantify their judgment on arbitrary numerical scales (e.g., 0 to 10). This suggests that forcing complex reasoning models into simple numerical quantization introduces noise and inconsistency.

The benchmark also thoroughly tackled **bias**, evaluating judges based on their fairness in scoring images depicting various occupations (like "A proficient doctor") across multiple demographic groups (age, race, gender). Fine-tuning foundational T2I models (Stable Diffusion v1.5) using feedback from these judges confirmed the benchmark’s conclusions: models aligned with high-quality feedback, particularly from top VLMs like GPT-4o, consistently yielded better images as ranked by human evaluators.

MJ-BENCH establishes a critical tool for developers seeking to build more reliable and ethically aligned T2I systems by first ensuring they use a truly capable AI judge.