---
layout: paper
pdf_url: https://arxiv.org/pdf/2407.05530
permalink: 2024-07-12/2407.05530/
title: New AI Framework Lets Robots Understand "Put This to There" with Human Gestures
---



Researchers have unveiled a novel robotic learning framework, dubbed **This&That**, that radically improves how humans communicate complex tasks to robots. By combining simple deictic language (like "this" and "that") with pointing gestures, the system uses an advanced video generation model to create unambiguous plans, leading to exceptional performance in robot manipulation tasks.

Current state-of-the-art robotic systems often struggle with natural language instructions that are either too verbose or rely on vague directional words. For example, telling a robot to “Give me the blue glass located on the third row of the wooden cabinet” is inefficient, but simply saying “Give me that” while pointing is clear to a human—a clarity robots have historically lacked.

The This&That framework solves this by integrating a two-pronged conditioning approach into a video diffusion model (VDM). This VDM, built atop a large-scale text-to-video model and fine-tuned on robotics data, generates an envisioned video of the task completion.

The core breakthrough is the dual conditioning: the system accepts both a text prompt ("Put this near there") and 2D spatial coordinates indicating the target and destination (the "this" and "there"). This geometric anchor instantly resolves scene ambiguity.

For instance, in a challenging scenario involving moving one green cube onto another identical green cube, language-only models frequently fail to identify which cube is the target. However, This&That achieves high success because the accompanying gesture visually isolates the intended objects, ensuring the resulting video plan precisely matches the user's intent.

Once the clear video plan is generated, it is fed into a behavioral cloning architecture called Diffusion Video to Action (DiVA). DiVA acts as the execution module, translating the planned visual sequence into low-level robot end-effector movements. The researchers highlight that using the predicted video as a dense, step-by-step goal sequence provides a far more robust planning guide than relying on a single final goal image.

In comprehensive evaluations, the framework proved highly effective. In user studies, videos generated by This&That aligned with human intent with an average success rate of 91.7%, significantly outperforming language-only baselines which struggled, especially in tasks like stacking and folding.

Furthermore, in simulated robot experiments—including challenging "out-of-distribution" scenarios featuring identical blocks—This&That achieved a pick-and-place success rate of 93%, demonstrating its robustness against ambiguity where competing video-based planners performed poorly.

The researchers assert that this breakthrough in clear, interpretable instruction, leveraging the universal language of pointing and deictic words, represents a foundational step toward more intuitive and reliable human-robot collaboration across a wide range of tasks.