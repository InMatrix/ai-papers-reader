---
layout: paper
pdf_url: https://arxiv.org/pdf/2407.07565
permalink: 2024-07-12/2407.07565/
title: New Research Reveals Code LLM Performance Scores Are Severely Inflated by Test
  Data Contamination
---



A new paper from researchers at Cohere warns that the widely-used benchmarks for evaluating large language models (LLMs) on code generation are fundamentally contaminated, leading to inflated performance scores and a misleading perception of recent progress.

The research, titled “On Leakage of Code Generation Evaluation Datasets,” argues that decades of reliance on standard metrics like HumanEval and MBPP have broken the core assumption of LLM testing: that the test data is genuinely unseen. The team introduced a new, contamination-free, and significantly more difficult benchmark—the **Less Basic Python Problems (LBPP)**—revealing that leading state-of-the-art (SOTA) models often perform up to 43% worse on the genuinely novel problems.

“When this evaluation data contaminates model training, the validity of the metrics as a measure of generalization capability becomes unreliable,” the authors state.

### The Leaky Pipeline of Code LLMs

The paper identifies three primary pathways through which contamination occurs, making it almost impossible to trust current leaderboard rankings.

The most obvious source is **direct data leakage**. HumanEval and MBPP tasks, due to their popularity, are extensively scraped into training corpora. The researchers demonstrated this by searching GitHub, finding that the median HumanEval prompt has been hit 99 times, often as exact duplicates or slight variations, making automatic deduplication filters ineffective.

A more insidious form of leakage occurs through **synthetic data generation**. Modern LLMs are increasingly trained on massive synthetic code datasets, sometimes comprising hundreds of thousands of examples. While synthetic data is meant to increase scale and complexity, the study found high semantic similarity between popular synthetic training sets (like `evol-instruct`) and the public benchmarks.

For instance, if HumanEval asks a model to "write a function to find the perimeter of a square," a highly similar problem exists in the synthetic data, even if the surrounding text or specific variable names are different. When a model is trained using this data, its high score reflects memorization, not generalization.

### A New Standard for Code Evaluation

To combat this, the researchers curated the LBPP dataset, a collection of 161 Python code completion problems designed to be similar in format to HumanEval but substantially more complex and guaranteed to be unseen.

The prompts were created by human annotators with competitive programming experience and filtered using a rigorous process: annotators were barred from using any web source or LLM for inspiration and had to ensure the problems were insoluble by strong internal models.

The result is a test that genuinely pushes model limits, focusing on domains like complex programming concepts, bit arithmetic, and graph-oriented algorithms. Sample LBPP problems include tasks like writing a Monte Carlo function to compute card draws or determining unique combinations for word anagrams.

When SOTA models were tested on LBPP, the illusion of mastery vanished. Models that scored highly on the contaminated benchmarks—with some achieving over 90% on HumanEval—saw their Pass@1 scores plummet. For example, some leading LLMs performed 43% worse on LBPP than on HumanEval, and 27% worse compared to MBPP.

The findings motivate an urgent shift away from older, compromised metrics. LBPP offers a new, untainted measure, providing a clearer signal of actual coding capability and offering a genuine stress test for future LLM development cycles.