---
layout: paper
pdf_url: https://arxiv.org/pdf/2407.03651
permalink: 2024-07-12/2407.03651/
title: New Test Reveals Top LLMs Suffer From "Working Memory" Loss, Proposes Simple
  Fix
---



Despite large language models (LLMs) now boasting context windows capable of ingesting the equivalent of massive books—up to two million tokens—their ability to reliably use all that information has remained uncertain. New research from Snorkel AI and the University of Wisconsin-Madison confirms that many leading LLMs suffer a severe "lost-in-the-middle" effect, often forgetting critical details buried deep within documents.

To address this reliability gap, researchers developed the Snorkel Working Memory Test (SWiM), a new evaluation framework designed to benchmark LLMs on real-world documents and use cases. They also introduced "medoid voting," a straightforward, training-free algorithmic correction that boosted retrieval accuracy by up to 24% in experiments.

### Moving Beyond the "Haystack"

Traditional long-context evaluations, such as the popular "needle in a haystack" (NIAH) test, measure if a model can retrieve a synthetic piece of information hidden within irrelevant text. The SWiM framework argues that this doesn't capture the complexity of production systems that deal with large volumes of complex documents like financial reports or legal briefs.

SWiM automatically generates real-world Question-Answering (QA) tasks on user-provided documents and specifically tests two crucial factors: robustness to irrelevant data (distractor documents) and the effect of position.

Testing eight cutting-edge models—including GPT-4 and Claude 3 Opus—the SWiM team found that performance degrades significantly when the answer document is placed between 25% and 75% depth in the context window.

For example, if an LLM is tasked with analyzing a 100-page corporate filing, it might perfectly retrieve a figure from the introduction (page 5) or the conclusion (page 95). However, if that identical figure is placed on page 50, the model’s accuracy plummets, confirming the "lost-in-the-middle" phenomenon across the industry’s strongest models.

"While NIAH may serve as a quick initial check, our results strongly suggest that SWiM is necessary prior to any development work for specific applications," the authors stated, noting that models that aced the NIAH test showed poor real-world retrieval.

### Medoid Voting: A Simple Correction

To counteract this positional bias, the researchers proposed Medoid Voting. This simple, inference-time correction is based on the idea that since we don’t know where the LLM is most likely to "pay attention," we should give the model several chances by shuffling the context order.

When Medoid Voting is applied, the system:
1.  Runs the query multiple times (e.g., three to five runs).
2.  Randomly shuffles the order of documents in the input context for each run.
3.  Generates an answer for each run.
4.  Selects the "medoid" response—the answer that is most similar to all other generated answers in the embedding space.

The results demonstrated immediate and substantial gains. On the tested models, Medoid Voting delivered a 17.3 percentage point accuracy lift on GPT-4-Turbo and a 24.2 percentage point lift on GPT-3.5-Turbo-16k, effectively smoothing out the "lost-in-the-middle" effect.

The researchers conclude that for real-world document analysis applications, relying solely on high context window counts is insufficient. The combination of the SWiM framework for robust, application-specific evaluation and the Medoid Voting approach offers production teams an effective path toward maximizing the reliability of long-context LLMs.