---
layout: paper
pdf_url: https://arxiv.org/pdf/2407.06192
permalink: 2024-07-12/2407.06192/
title: New Benchmark Reveals Major Flaws in Vision Language Models’ Multi-Object Reasoning
---



Large Vision-Language Models (LVLMs) like GPT-4V and LLaVA frequently suffer from "object hallucination"—the act of generating objects that are not present in an image or misidentifying existing ones. While this flaw is known, new research demonstrates that the problem is significantly exacerbated when these models are asked to recognize multiple objects simultaneously, revealing deep-seated reliance on linguistic shortcuts rather than visual facts.

Researchers introduced the **Recognition-based Object Probing Evaluation (ROPE)**, an automated protocol designed to test LVLMs' multi-object recognition capabilities under controlled conditions. Unlike previous tests that rely on simple text descriptions or single yes/no questions, ROPE uses visual prompts (bounding boxes) to uniquely pinpoint objects, mitigating textual ambiguity and forcing the model to identify five specified objects concurrently.

The core finding is stark: models suffer more hallucinations when tasked with multi-object recognition than when focusing on single objects. For instance, in one test case featuring a kitchen counter, a model might correctly identify a bounding box pointing to a whisk when asked about it individually. However, when simultaneously asked to name the whisk, a knife, a lemon, a jar, and an apple, the same model might incorrectly identify the whisk as a "fork." This suggests a breakdown in visual attention or reasoning when the cognitive load increases.

Furthermore, LVLMs struggle disproportionately when the requested object classes are diverse or "heterogeneous." Performance sharply drops compared to tasks where all queried objects belong to the same category (e.g., identifying five apples).

The study offers critical evidence that LVLMs frequently exploit language biases rather than performing true visual grounding. In adversarial tests designed to detect shortcut behavior, models were asked to identify a sequence of five objects: the first four were identical (e.g., 'apple'), while the fifth was different (e.g., 'pear'). Researchers found that models repeatedly predicted the fifth object as the class of the first four, scoring nearly zero accuracy on the outlier. This indicates the models were using the sequence of preceding text tokens as a shortcut, effectively repeating the dominant answer instead of visually checking the final object.

Analysis of the hallucinatory factors showed that models are more likely to fail when they exhibit high uncertainty (high *object token entropy*) and when the object being queried has low *semantic salience* within the image relative to other instances of its class. The visual modality contribution—the weight the model assigns to visual information—was consistently low during hallucinations, suggesting LVLMs default to linguistic context when uncertain.

These findings suggest that simply increasing the scale of the base language model or relying on current grounded instruction tuning is insufficient to resolve multi-object hallucination. Future development of robust LVLMs must prioritize complex multi-object instructions and training datasets with more balanced object distributions to ensure models reason from visual cues rather than linguistic patterns.