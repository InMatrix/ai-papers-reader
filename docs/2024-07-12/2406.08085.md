---
layout: paper
pdf_url: https://arxiv.org/pdf/2406.08085
permalink: 2024-07-12/2406.08085/
title: New AI Model Mimics Human Memory to Understand Hour-Long Video Streams in Real
  Time
---



In a major step toward practical, real-time artificial intelligence, researchers from Tsinghua University and ByteDance have unveiled Flash-VStream, a novel video-language model capable of processing and instantly answering questions about extremely long video streams, such as live surveillance feeds or continuous robotics footage.

Existing large video-language models (VLLMs) excel at analyzing finite, *offline* videos. However, they struggle profoundly with the "dynamic" nature of online streaming: they must continuously encode visual data while simultaneously being ready to answer user queries at any moment. The core bottleneck is memory; these models attempt to save redundant visual tokens for every frame, leading to prohibitive GPU memory (VRAM) consumption and inference latency that scales up drastically with video length.

Flash-VStream resolves this challenge by simulating the memory mechanism of the human brain. The system is designed with two asynchronous processes—one continuously perceives and memorizes the stream, and a separate one handles user questions and retrieval.

The heart of the innovation is the Spatial-Temporal-Abstract-Retrieved (STAR) memory mechanism. Instead of storing every pixel detail, STAR compresses visual information into four distinct components:
1. **Spatial Memory:** Holds detailed, recent visual information (short-term memory).
2. **Temporal Memory:** Integrates dynamic data over time, crucial for long-term retention of key events, using weighted clustering to condense information.
3. **Abstract Memory:** Synthesizes high-level semantic concepts, creating an actionable synopsis of the whole video.
4. **Retrieved Memory:** Recalls precise spatial details related to the most important temporal events.

This design allows the model to "forget" redundant frames while remembering the necessary context, much like a person only recalls the most important parts of a long movie or conversation.

To appreciate the difference, consider a two-hour-long first-person video stream. If a conventional model is asked, "What color was the car driven five minutes ago?" the model would often need to re-encode all the visual information from scratch, leading to unacceptable delays, sometimes exceeding 10 seconds. In contrast, Flash-VStream remains responsive, answering within one second regardless of stream duration, thanks to its ability to quickly retrieve compressed information from its STAR memory.

In laboratory tests, Flash-VStream demonstrated significant computational efficiency gains. Compared to state-of-the-art VLLMs, Flash-VStream achieved the lowest inference latency, remaining constant even after processing thousands of frames, and drastically reduced VRAM usage.

Furthermore, the researchers introduced VStream-QA, a new benchmark specifically tailored for this challenging setting. VStream-QA features videos ranging from 30 to 60 minutes—significantly longer than existing benchmarks—with question-answer pairs linked to specific timestamps. This ensures the model is tested on its ability to use visual information *prior* to the query time, accurately simulating real-time interaction. On this demanding new benchmark, Flash-VStream achieved state-of-the-art accuracy, confirming the superiority of its memory-based approach for continuous, real-world video analysis.