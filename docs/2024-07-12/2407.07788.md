---
layout: paper
pdf_url: https://arxiv.org/pdf/2407.07788
permalink: 2024-07-12/2407.07788/
title: BiGym Benchmark Challenges Robot Learning with Human-Realistic, Dual-Arm Household
  Tasks
---



A new benchmark environment called BiGym has been introduced to dramatically raise the difficulty bar for training mobile humanoid robots in complex household scenarios. BiGym is designed specifically for mobile bi-manual (two-armed) manipulation, featuring 40 diverse tasks based on the popular Unitree H1 humanoid platform operating in highly realistic simulated kitchens and homes.

Developed by researchers at the Dyson Robot Learning Lab, BiGym aims to close the gap between controlled academic tests and the messy reality of real-world robotics.

The primary innovation of BiGym is its data source: unlike previous benchmarks that often rely on synthetic, planner-generated trajectories, BiGym uses human-collected demonstrations. These trajectories are gathered by tele-operating the Unitree H1 robot via Virtual Reality (VR), resulting in noisy, multi-modal, and realistic movement data.

"The trajectories of BiGym are noisy, multi-modal, but smooth in general, but the motion planner generated trajectories of RLBench are either straight lines or unnatural," the authors note, highlighting the crucial difference in data quality.

### Complexity and Long-Horizon Challenges

The 40 tasks in BiGym span a vast range of complexity. Simple tasks involve foundational skills, such as `reach_target_single`, where the robot uses a specified hand to touch a colored marker. However, the benchmark quickly scales up to long-horizon household chores that require meticulous sequential planning and coordination.

For example, the task `dishwasher_unload_plate_long` demands a sequence of sub-tasks: the robot must locate the plate, open the dishwasher door, pull out the lower tray, grasp and place the plate into a wall cabinet, and finally, close both the door and the cabinet. This high degree of difficulty, coupled with a sparse reward (a "1" only given upon full completion), makes exploration extremely challenging for existing algorithms.

Furthermore, BiGym’s complex task space introduces multi-modality. For a task like moving a cup to a drawer, the robot might choose to open the drawer with one arm while holding the cup with the other, or perform the entire action using a single arm and careful base movements. The humanoid is equipped with three RGB-D cameras (head and wrists) and offers two flexible action modes: a resource-intensive *whole-body* mode (controlling locomotion and arms simultaneously) and a simplified *bi-manual* mode (focusing on upper-body manipulation with a fixed base controller).

### Benchmarking Reveals AI Limitations

To validate the benchmark, the researchers tested seven state-of-the-art algorithms, including imitation learning (IL) methods like Action Chunking Transformers (ACT) and Diffusion Policies, and demo-driven reinforcement learning (RL) methods.

The results confirm BiGym's challenging nature. While the most advanced IL algorithms—which use powerful architectures like transformers—achieved perfect success rates on simpler tasks (e.g., 100% on `dishwasher_close_trays`), all algorithms struggled severely with long-horizon and highly mobile tasks.

The complex multi-step unloading tasks and interactions with rigid objects, like stacking blocks or moving two plates simultaneously, resulted in 0% success rates across almost all tested RL baselines. The findings suggest that current robot learning models lack the robust memory mechanisms needed to track objects accurately while the robot base is moving, a critical requirement for functional mobile humanoids.

BiGym is poised to serve as a vital testing ground, driving future research in hierarchical planning, multi-modal policy representations, and effective belief estimation necessary to build intelligent, general-purpose household assistance robots.