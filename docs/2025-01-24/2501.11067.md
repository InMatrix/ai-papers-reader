---
layout: paper
pdf_url: https://arxiv.org/pdf/2501.11067
permalink: 2025-01-24/2501.11067/
title: IntellAgent&#58; A New Multi-Agent Framework for Evaluating Conversational
  AI
---



Conversational AI is rapidly evolving, with large language models (LLMs) powering increasingly sophisticated systems capable of autonomous planning, execution, and refinement.  However, evaluating these complex agents has been a major challenge. Traditional methods often rely on static benchmarks with coarse-grained metrics, failing to capture the intricacies of real-world interactions.  A new open-source framework, IntellAgent, aims to address this gap, offering a scalable, comprehensive, and reproducible approach to evaluating conversational AI.  Developed by Plurai, the system is detailed in a recent paper posted to arXiv.

IntellAgent moves beyond static benchmarks by automatically generating diverse, synthetic scenarios that realistically simulate multi-turn dialogues, API integrations, and policy adherence.  The key to its effectiveness lies in its use of a "policy-driven graph model." This model represents various policies (rules the AI must follow, like "never reveal PII") as nodes in a graph, with edges representing the likelihood of different policy interactions in a conversation.  The system then uses this graph to sample realistic policy combinations, creating scenarios with varying complexity levels.

Imagine a chatbot designed for airline bookings.  IntellAgent could generate scenarios involving multiple intertwined policies:  the user might want to change their flight, which triggers policies related to fare differences, cancellation fees, and availability, potentially also triggering policies related to rebooking with different airlines and handling customer service.  A simple benchmark might only test a single policy, like refund policies; IntellAgent's multi-policy approach would assess the AI's robustness and capabilities in handling interactions between several policy constraints in a more natural context.

The system then uses AI agents to simulate realistic interactions between a user and the chatbot being evaluated. This fine-grained simulation allows IntellAgent to provide detailed diagnostic reports, pinpointing specific areas where the AI falls short.  For instance, IntellAgent might flag a weakness in the AI's ability to correctly handle refund policies when a flight change also necessitates using a different airline.  This level of detail far exceeds what existing benchmark tools offer.

IntellAgent's open-source design promotes reproducibility and collaboration. The framework’s modular nature allows for seamless integration of new domains, policies, APIs, and even different LLMs.  This adaptability is critical for the ongoing evolution of the field.  The researchers demonstrate IntellAgent's effectiveness by comparing its results to existing conversational AI benchmarks like the T-bench, showing a strong correlation between the two despite IntellAgent relying entirely on synthetic data.  This adds confidence in the validity of the new framework.

The paper’s results highlight the IntellAgent’s ability to uncover significant performance variations across different models and policy categories, empowering users to make informed decisions about which LLM or chatbot configuration best meets their specific needs.  IntellAgent promises to be a valuable tool for advancing conversational AI research and deployment by providing a more accurate and informative means to evaluate agent performance and guide future development.  The framework’s code is publicly available on GitHub, encouraging wider adoption and community contribution.