---
layout: paper
pdf_url: https://arxiv.org/pdf/2506.16141
permalink: 2025-06-27/2506.16141/
title: New Approach Boosts Multimodal AI's Reasoning and Consistency
---



A recent research paper introduces GRPO-CARE, a novel framework designed to improve the reasoning abilities of multimodal large language models (MLLMs), particularly in complex video understanding tasks. The study also presents SEED-Bench-R1, a new benchmark specifically created to evaluate these advanced AI models.

Current reinforcement learning (RL) techniques, like outcome-supervised GRPO, have shown promise in enhancing the "Chain-of-Thought" (CoT) reasoning of LLMs. However, applying these methods to MLLMs, which process both text and visual information, has been a challenge. The researchers observed that existing RL methods, while improving the accuracy of answers, often lead to inconsistencies between the model's reasoning process and its final output. For example, an AI might correctly identify that water is running in a sink but then incorrectly suggest leaving it on after washing a cloth, contradicting the logical next step of turning off the tap.

To address this, the paper proposes GRPO-CARE. This framework enhances RL by introducing a "consistency-aware" reward system. Instead of solely rewarding correct final answers, GRPO-CARE also provides a bonus for reasoning that is logically coherent with that answer. This is achieved by using a reference model, which is a slightly older version of the main AI model, to assess how likely a given reasoning path is to lead to the correct answer. By rewarding consistency alongside accuracy, GRPO-CARE encourages MLLMs to generate more reliable and interpretable reasoning.

The researchers developed SEED-Bench-R1 to rigorously test their approach. This benchmark features a hierarchical structure with three levels of increasing difficulty, designed to assess how well MLLMs generalize to new environments and tasks. Level 1 includes tasks similar to the training data, Level 2 features new environments, and Level 3 involves diverse tasks and environments, mimicking real-world complexity.

Experiments conducted on SEED-Bench-R1 demonstrated that GRPO-CARE significantly outperforms standard GRPO and other baseline methods. Specifically, GRPO-CARE achieved a 6.7% performance boost on the most challenging tasks and a 24.5% improvement in the consistency rate between reasoning and answers. The paper also highlights that GRPO-CARE trained models show strong transferability to other video understanding benchmarks, indicating the robustness and generalizability of their method.

The study's findings suggest that the new framework not only improves the accuracy of multimodal AI but also makes its decision-making process more transparent and logical, paving the way for more sophisticated and trustworthy AI systems in various real-world applications.