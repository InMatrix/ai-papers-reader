---
layout: paper
pdf_url: https://arxiv.org/pdf/2507.15974
permalink: 2025-07-25/2507.15974/
title: The AI Safety Paradox&#58; When Asking LLMs to ‘Think Harder’ Makes Them Less
  Secure
---



A new study by researchers from Princeton, NVIDIA, Carnegie Mellon, and Google DeepMind has uncovered a critical paradox in the safety engineering of large language models (LLMs): increasing a model's "thinking time" to improve performance can actively undermine its security, exposing it to new adversarial attacks.

The practice, known as inference-time scaling, encourages LLMs to generate longer intermediate reasoning chains (often called Chain-of-Thought) before producing a final answer. While this technique generally boosts the quality of complex reasoning, the paper demonstrates that the security benefits are entirely dependent on how the model is deployed.

When reasoning steps are kept hidden from the user—the current standard practice for proprietary models like OpenAI’s and Google’s—the researchers confirm that extended computation reliably improves robustness against threats like **Prompt Injection** and **Prompt Extraction**. For example, a model given a larger "thinking budget" (e.g., increasing tokens from 100 to 16,000) is better able to identify and ignore malicious instructions embedded in a low-priority data block, adhering instead to its primary safety directives.

### The Inverse Scaling Law

However, the team identified a novel and worrying **inverse scaling law** when these intermediate reasoning steps are explicitly exposed to the adversary, a configuration common in some open-source models and commercial APIs. In this scenario, allocating *more* compute consistently *degrades* robustness across all measured threat models.

The core intuition is simple: lengthening the reasoning chain fundamentally expands the attack surface. If an attacker is trying to steal a secret key embedded in the system prompt (a **Prompt Extraction** attack), increasing the model's "thought budget" from 100 to 16,000 tokens gives the malicious token more opportunities to appear during the extended internal generation process. The study found that for certain models, robustness against extraction plummeted from nearly 90% to below 20% when reasoning chains were exposed and lengthened.

Furthermore, exposed reasoning chains drastically amplify the risk from **Harmful Requests**. While a shorter chain might refuse a request like “Explain how to manufacture a bomb,” a longer, exposed chain might reveal step-by-step unsafe instructions during its intermediate thoughts, even if the final output attempts to hide them.

### New Threats from Tool Use

The research also warns that simply hiding the reasoning chain is insufficient to guarantee safety, especially in the emerging era of reasoning agents integrated with external tools and APIs.

Even when internal thoughts are invisible, longer reasoning chains inherently increase the chance that a prompt injection attack will trigger an unintended external tool call. For instance, an attacker could embed a subtle command to execute a malicious API action (like initiating a transfer or deleting a file) within a document the LLM is asked to summarize. The extended reasoning process, necessary for complex problem-solving, gives the model more chances to misinterpret that command as a legitimate instruction and execute the malicious tool call during its internal deliberation.

The findings urge AI developers and practitioners to carefully consider these trade-offs, concluding that increased inference-time computation, while beneficial for performance, introduces serious, overlooked security vulnerabilities that must be balanced against deployment risk.