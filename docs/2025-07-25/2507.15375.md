---
layout: paper
pdf_url: https://arxiv.org/pdf/2507.15375
permalink: 2025-07-25/2507.15375/
title: AI Models Learn to Multitask&#58; ‘STITCH’ Enables Language Models to Think
  and Talk Simultaneously
---



Spoken Language Models (SLMs) are rapidly becoming the face of conversational AI, but they face a fundamental constraint when handling complex requests: they can’t think while they talk.

In a preprint released today, researchers introduced STITCH (Simultaneous Thinking and Talking with Chunked Reasoning), a novel generation framework that allows SLMs to perform internal, unspoken reasoning—much like humans—without suffering crippling delays. The breakthrough significantly boosts accuracy on complex tasks while maintaining the ultra-low latency necessary for real-time dialogue.

### The Latency Problem

Current SLMs typically operate in two modes for complex tasks like mathematical problem-solving. They either respond instantly without internal reasoning, often leading to poor or incorrect answers, or they adopt a “Think Before Speaking” (TBS) approach, where the model generates a complete internal Chain-of-Thought (CoT) before producing any audio.

While TBS improves accuracy by over 16% on reasoning benchmarks like MultiArith and GSM8K, the latency penalty is severe. Since the CoT reasoning can be arbitrarily long, the user is left waiting silently until the entire thought process is finished.

STITCH overcomes this by breaking the reasoning process into small, manageable chunks and interleaving them with the spoken response.

“When a chunk of audio is played to the user, the model continues to generate the next unspoken reasoning chunk, achieving simultaneous thinking and talking,” the authors explain.

### Leveraging Audio Playback Time

The key insight is exploiting the temporal asymmetry between token generation and audio synthesis. When an SLM generates a chunk of text and speech tokens, it takes only a fraction of a second. However, synthesizing and playing that speech audio to the user might take two seconds or more.

STITCH leverages this "free time." For instance, if an SLM generates around 80 tokens per second, and a two-second audio chunk requires only 39 tokens to create, the model has time to generate over 120 tokens of unspoken reasoning *before* it needs to prepare the next speech segment. This allows the model to think and refine its next steps while the user is listening to the current output.

The team presented two variants: STITCH-R (Reasoning First), which starts with a small thinking chunk, and the highly efficient STITCH-S (Speaking First).

### Zero-Latency Reasoning

STITCH-S is the most exciting development, as it immediately begins speaking (generating the first chunk of the response) and then weaves the reasoning process into the audio playback periods.

In evaluations against models unable to generate reasoning, STITCH-S achieved an accuracy boost of over 15% on math reasoning datasets—crucially, matching the non-reasoning models' initial latency profile. This means the model can now deliver superior answers for reasoning-intensive questions without adding any perceivable delay to the conversation's start.

The researchers also demonstrated flexibility in deployment. The length of the partial reasoning chunks can be adjusted during inference time without retraining the model, allowing developers to balance latency and computational resources according to the hardware environment.

By introducing this human-like ability to simultaneously process thoughts and deliver responses, STITCH sets a new standard for efficient, high-performing spoken language models.