---
layout: paper
pdf_url: https://arxiv.org/pdf/2507.15024
permalink: 2025-07-25/2507.15024/
title: AI Models Learn to Give Useful Feedback with New “RefCritic” System
---



Large Language Models (LLMs) are increasingly tackling complex reasoning tasks, from advanced mathematics to intricate code generation. But as these solutions grow more sophisticated, auditing their long, intricate chains of thought has become nearly impossible for humans. While AI "critic" models are designed to fill this gap, a new study reveals that standard training methods produce critics that are often superficially accurate but practically useless for improvement.

Researchers have introduced RefCritic, a novel framework that uses reinforcement learning (RL) guided by a unique dual-reward system to train critics capable of generating deep, actionable feedback. RefCritic dramatically enhances the ability of policy models (the LLMs solving the problem) to refine their own incorrect solutions.

The inadequacy of previous critic training stems from Supervised Fine-Tuning (SFT). While SFT teaches a critic to correctly classify an output as "right" or "wrong," it doesn't teach it *why* the error occurred or how to communicate the fix effectively. For instance, a conventional SFT critic might observe an incorrect math solution and merely state: "Incorrect. Recheck your algebraic substitution." This is accurate, but it offers no real guidance.

RefCritic solves this limitation by implementing a "long chain-of-thought" structure and optimizing it with two distinct reward signals. The first reward is the basic binary score for **Judgment Accuracy** (0/1 for correctly identifying the solution's validity). Crucially, the second reward is the **Refinement Effectiveness**.

This second reward creates an explicit feedback loop: the critic is rewarded only if its critique leads the original policy model to generate a *measurably better* revised solution. In practice, this means RefCritic is incentivized to output highly specific advice, such as: "Error detected in Step 3. You mistakenly inverted the determinant calculation, resulting in the sign error for variable $Z$."

When tested on challenging mathematical benchmarks like the AIME25 (American Invitational Mathematics Examination), RefCritic demonstrated significant superiority. In the crucial "Refinement after Critique" setting, policy models using RefCritic’s guidance saw Pass@1 accuracy gains of 6.8% and 7.2% over models using standard SFT critiques, confirming that the feedback was truly actionable.

Furthermore, RefCritic proved highly adept at identifying precise errors, even without explicit step-level training. On the ProcessBench dataset, designed to test error localization in mathematical reasoning steps, RefCritic achieved an impressive 77 F1 score, demonstrating its deep, nuanced understanding of solution integrity.

The framework’s benefits also scale effectively. When used to filter out low-quality solutions in a "Majority Vote with Critique" setting, RefCritic consistently outperformed baseline filtering methods, showing a 3.6 percentage point improvement on AIME25 when evaluating larger candidate pools. By explicitly linking the quality of critique to measurable improvements in model performance, RefCritic represents a significant step toward developing LLMs that can truly "teach" themselves to reason better.