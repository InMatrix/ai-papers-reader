---
layout: paper
pdf_url: https://arxiv.org/pdf/2507.15245
permalink: 2025-07-25/2507.15245/
title: LLM Agents Revolutionize Academic Search, Outperforming Google Scholar with
  Human-Like Exploration
---



New research introduces SPAR (Scholar PAper Retrieval), a modular, multi-agent framework designed to overcome the limitations of traditional and existing AI-enhanced academic search engines. Leveraging the power of large language models (LLMs) to mimic how human researchers explore literature, SPAR achieves dramatic performance improvements, recording up to a 56% increase in performance (F1 score) over previous state-of-the-art agent systems.

Traditional search platforms, such as Google Scholar, excel at simple keyword matching but falter when faced with complex, multi-intent queries like: “Show some cutting-edge technological advancements on how to improve the generalization ability of machine learning models across multiple domains.” Existing LLM retrieval pipelines often remain rigid and lack the iterative reasoning needed for deep scholarly discovery.

SPAR addresses this gap by orchestrating five specialized LLM agents across four core stages, centering its retrieval strategy on a concept called the **Reference Chain (RefChain)**.

RefChain simulates the common research behavior of locating a highly relevant paper (Paper A) and then recursively exploring its reference list to uncover foundational or closely related works (Papers B, C, and D). This approach goes far beyond simple keyword matches, dramatically improving document recall.

The sophisticated search process begins with the **Query Understanding Agent**, which interprets the user's intent, identifies the specific academic domain (e.g., biomedicine or computer science), and refines ambiguous phrasing into a set of precise, targeted search queries.

Next, the **Iterative Retrieval** phase coordinates multi-source searching across platforms like OpenAlex and Semantic Scholar, alongside the RefChain expansion. To ensure the search doesn't become too noisy or tangential, SPAR uses a key constraint: it only expands the reference chain one layer deep, prioritizing precision and efficiency.

To maintain diversity and depth, the **Query Evolver Agent** monitors the best-retrieved documents. If a top-ranked paper discusses a novel Reinforcement Learning (RL) algorithm, the Evolver generates new queries focused specifically on that algorithm’s "convergence limitations" or "real-world applications," broadening the search trajectory intelligently.

Finally, the **Reranker Agent** optimizes the final results, not just by relevance, but by incorporating crucial academic factors: **publication authority** (venue prestige, author h-index) and **temporal relevance** (prioritizing recent papers when the query requests "cutting-edge" work).

To rigorously test this framework, the researchers also introduced **SPARBench**, a new, expert-annotated benchmark featuring 50 realistic, complex queries spanning computer science and biomedicine.

On this challenging benchmark, SPAR consistently outperformed all manual search engines and LLM-assisted pipelines. It achieved the highest F1 score (0.3015), demonstrating its superior ability to balance high recall (finding all relevant papers) with high precision (minimizing irrelevant results), validating the synergy of its modular, agent-based design.