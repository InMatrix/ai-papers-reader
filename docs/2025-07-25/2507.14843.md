---
layout: paper
pdf_url: https://arxiv.org/pdf/2507.14843
permalink: 2025-07-25/2507.14843/
title: The Invisible Leash&#58; RL-Trained Reasoning Models Prioritize Precision Over
  Discovery
---



Reinforcement Learning with Verifiable Rewards (RLVR), the powerful technique behind recent breakthroughs in AI reasoning models like DeepSeek-R1, acts primarily as a “precision enhancer” rather than a tool for genuinely expanding a model's underlying reasoning capacity.

A new empirical study reveals that while the common practice of RLVR reliably boosts the accuracy of the best answer (`pass@1`), it achieves this by systematically collapsing the model’s distribution onto a narrower set of known, high-reward solutions. This optimization creates an “invisible leash,” confining the trained policy to the initial reasoning support established by the base model.

### The Shrinkage of Empirical Support

The research investigates the concept of "empirical support," defining it as the set of correct answers that a model assigns a non-negligible probability to, making them reachable through finite sampling. The findings show that RLVR operates largely as a support-constrained optimization mechanism.

Across multiple large language models and reasoning benchmarks (including math and non-math tasks), the models exhibited high **Support Retention Rate (SRR)**—retaining nearly all solutions the base model knew—but near-zero **Net Discovery Rate (NDR)**, meaning they rarely found solutions previously inaccessible.

Critically, the study found that **empirical support shrinkage** consistently outweighed expansion. For instance, the ProRL-1.5B model lost 175 previously accessible correct completions while gaining only 48 new ones, resulting in a net shrinkage. This behavior is starkly illustrated in tasks like Leg Counting, where the RLVR-trained model narrowed its viable solution paths, excluding alternative correct methods available to the original base model.

This systematic narrowing explains a paradoxical trade-off: RLVR models consistently achieve higher single-sample accuracy (`pass@1`) because they concentrate probability mass on the single best-known path. However, when evaluated with large sampling budgets (`pass@k` for high *k*), which measure the total diversity of accessible solutions, the base model often outperforms the RL-trained version due to its broader coverage. For example, on AIME2024, the base model’s `pass@8192` score (93.3%) surpassed ProRL’s (83.3%).

### The Entropy-Reward Trade-off

The precision-diversity trade-off is further quantified by analyzing output entropy. The research identified a decoupling between local uncertainty and global diversity:

1.  **Token-Level Entropy (Local Uncertainty):** RLVR models often show *increased* token-level entropy, suggesting they take longer, more uncertain steps during reasoning, perhaps generating more elaborate Chain-of-Thought sequences.
2.  **Answer-Level Entropy (Global Diversity):** Despite the noisy local steps, RLVR systematically *reduces* answer-level entropy. This indicates that while the generation process appears more stochastic, these paths ultimately converge onto a smaller, less diverse set of final answers.

This "local stochasticity without global exploration" confirms that RLVR is primarily reweighting existing pathways rather than discovering fundamentally new reasoning structures. The fundamental constraint arises because standard RL updates rely on gradients derived from sampled outputs; if a correct solution is assigned zero probability by the base model, RLVR can never sample it or assign it positive mass.

The rare instances where expansion does occur are attributed to the RLVR model recomposing subskills the base model already possessed (e.g., modular tasks like Boxnet and Arc 1D), or correcting prompt-format misalignments—not genuine novel reasoning beyond the base model’s support.

Breaking this invisible leash and extending the true reasoning horizon of LLMs, the authors conclude, will require future algorithmic innovations such as explicit exploration mechanisms or hybrid strategies that inject probability mass into currently underrepresented solution regions.