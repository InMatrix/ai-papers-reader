---
layout: paper
pdf_url: https://arxiv.org/pdf/2502.07445
permalink: 2025-02-14/2502.07445/
title: The Chameleon Effect&#58; New Study Reveals Top LLMs Overfit to Benchmark Phrasing,
  Undermining High Scores
---



A groundbreaking new study from researchers at Ben Gurion University, Tel Aviv University, and Harvard University suggests that the impressive high scores achieved by Large Language Models (LLMs) on standard public benchmarks may be dangerously misleading. The paper introduces a novel framework showing that many state-of-the-art LLMs—including large, high-performing models—rely heavily on superficial, dataset-specific phrasing rather than genuine language understanding, leading to significant performance drops when test questions are slightly reworded.

The researchers dubbed this phenomenon the "Chameleon Effect," as LLMs appear to blend seamlessly into specific testing environments only to fail when the surface patterns change.

To systematically detect this vulnerability, the team developed the Chameleon Benchmark Overfit Detector (C-BOD). This meta-evaluation tool works by taking questions from established benchmarks, such as the widely used MMLU (Massive Multitask Language Understanding) dataset, and applying parameterized transformations to rephrase the prompts. Crucially, the process ensures that the core semantic meaning and the correct multiple-choice answers remain identical.

This controlled distortion, governed by a "distortion parameter" ($\mu$), acts as a stress test for generalization.

To illustrate, the C-BOD framework might take an original question that an LLM correctly answers: "Why is the Mars Exploration Rover Spirit currently tilted towards the north?" When perturbed by C-BOD, the question might become: "What caused the Mars Exploration Rover Spirit to tilt in a northerly direction?" Despite the meaning being preserved, models frequently failed to select the correct answer on the rephrased version, indicating a dependence on the exact original wording.

The extensive validation, involving 32 leading open-weight LLMs from families like Gemma, Llama, and Mistral, revealed widespread fragility. On the MMLU benchmark, models that showed a statistically significant performance gap experienced an average accuracy drop of 2.75% under modest rephrasing ($\mu=0.5$). Over 80% of the tested models exhibited a statistically significant performance decline.

The most counterintuitive finding challenges current wisdom on scaling: the models that performed best on the original, canonical benchmarks were often the most vulnerable to simple textual perturbations. Larger models, which theoretically possess greater generalization capabilities, demonstrated a stronger tendency to overfit. For instance, models in the Gemma family showed a clear, progressive increase in performance drop corresponding to higher parameter counts. This suggests that the capacity to memorize and exploit surface-level patterns scales with model size.

Conversely, some smaller models, such as Llama 1B, showed minimal or statistically insignificant drops, potentially because they lack the capacity to memorize the specific benchmark cues necessary for overfitting.

The findings underscore a critical paradox: high scores on existing LLM leaderboards may be masking underlying vulnerabilities. By offering a robust, dataset-agnostic method to quantify reliance on prompt structure, C-BOD provides a necessary safeguard. The researchers are urging the community to pivot evaluation practices away from solely tracking peak accuracy toward prioritizing model resilience and true generalization ability. The team has released the rephrased benchmark datasets and code to facilitate the immediate adoption of more robust evaluation standards.