---
layout: paper
pdf_url: https://arxiv.org/pdf/2502.06884
permalink: 2025-02-14/2502.06884/
title: AI Models Learn When to Stay Silent&#58; New Policy Uses Reinforcement Learning
  to Detect Hallucinations and Manage Risk
---



In a significant step toward making powerful Large Language Models (LLMs) and Vision-Language Models (VLMs) safer for critical applications, researchers have introduced a novel framework that allows these models to dynamically assess their own uncertainty and proactively abstain from high-risk predictions.

The new method, dubbed the Conformalized Abstention Policy (CAP), merges reinforcement learning (RL) with statistical uncertainty tools to overcome the rigidity of existing risk management systems. Current state-of-the-art uncertainty methods, such as Conformal Prediction (CP), rely on static, predetermined thresholds. While these static guarantees ensure reliability, they often lead to suboptimal outcomes: either the model must predict confidently and risk errors (hallucinations), or it must produce an overly broad set of possible answers, reducing practical utility.

"As LLMs move into high-stakes domains like autonomous driving or healthcare diagnostics, we need them not just to be accurate, but to be reliably aware of their own limitations," explains Sina Tayebati, the lead author on the paper.

CAP addresses this challenge by teaching the model how to choose among three outcomes dynamically based on the complexity and context of the input data:

1.  **Single Best Prediction:** When confidence is high (e.g., identifying a common object in an image).
2.  **Set Prediction:** When uncertainty is moderate, providing a small set of plausible answers (e.g., suggesting "{A, B}" for an ambiguous medical finding).
3.  **Abstain:** When uncertainty is extreme, allowing the model to defer the decision to a human expert.

The system uses reinforcement learning to treat the thresholds that define these three regimes as adaptive "actions." The RL agent learns to adjust these boundaries to minimize prediction set size and maximize accuracy, all while adhering to strict statistical coverage guarantees—ensuring the correct answer is captured in the prediction or abstention set at least 90% of the time.

The results, tested across diverse LLM and VLM benchmarks including multi-modal reasoning and scientific QA, demonstrate substantial improvements over static baselines like Least Ambiguous Classifiers (LAC) and Adaptive Prediction Sets (APS).

In a critical finding for safety, CAP boosted the Area Under the Receiver Operating Characteristic (AUROC) metric—a key measure for hallucination detection—by up to 22.19%. This means the model became significantly better at distinguishing confidently asserted but incorrect claims from correct predictions.

Furthermore, CAP reduced the Expected Calibration Error (ECE) by a massive 70% to 85% compared to standard conformal baselines. ECE measures how well a model’s confidence aligns with its actual accuracy. By drastically lowering ECE, the framework provides far more trustworthy uncertainty estimates.

For instance, if a VLM in an autonomous system encounters an object that could be an ambiguous traffic sign, CAP would learn the precise threshold at which it should abstain and flag the input for human review, rather than forcing a low-confidence guess. This dynamic balance yielded up to 21.17% improvement in uncertainty-guided selective generation (AUARC), quantifying the system’s effectiveness at only predicting when it is reasonably certain.

The researchers conclude that by coupling the statistical rigor of conformal prediction with adaptive, policy-based learning, CAP offers a flexible and effective solution for developing robust risk management strategies essential for deploying large foundation models in sensitive real-world settings.