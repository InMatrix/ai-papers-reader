---
layout: paper
pdf_url: https://arxiv.org/pdf/2502.03492
permalink: 2025-02-14/2502.03492/
title: Reinforcement Learning Trains LLMs to Deliver Expert Critiques, Revolutionizing
  Self-Correction
---



A new framework, dubbed Critic Training via Reinforcement Learning (CTRL), has demonstrated a scalable method for training specialized AI "critics" that can provide highly effective, actionable feedback to guide the iterative refinement of large language model (LLM) outputs, particularly in complex coding tasks.

The research, detailed in a new paper, addresses a critical bottleneck in the quest for autonomous AI: while LLMs possess the capability for self-reflection and self-correction, attempts to refine outputs often fail or degrade quality because the feedback generated is vague, inaccurate, or leads to compounding errors. The CTRL framework solves this by decoupling the critic model—the component providing feedback—from the generator model—the component performing the task (like writing code).

Instead of relying on costly human annotations, CTRL trains the critic using reinforcement learning, optimizing it to generate feedback that maximizes the correction performance of the generator model.

### From Vague Analysis to Actionable Fixes

The core innovation is turning the critique process into a measurable optimization problem. The critic is taught not just to spot errors (discrimination) but to provide actionable suggestions (critiquing) that successfully drive the original solution toward a correct output.

To illustrate this, consider a complex programming challenge requiring the LLM to find the $k^{th}$ nearest obstacle to an origin point, a task often solved with heap data structures. An initial, incorrect LLM attempt might mistakenly use a minimum heap (min-heap) and try to access the $k^{th}$ element directly using array indexing.

A typical weak self-critique might vaguely flag the approach as "inefficient." In contrast, a CTRL-trained critic provides a structured analysis: it identifies the precise flaw ("heaps do not maintain sorted order beyond the root element") and provides concrete **Improvement Suggestions**, such as switching to a maximum heap (max-heap) of size $k$. This targeted guidance enables the generator to produce a correct solution efficiently, often in a single revision cycle.

### Weak Critics Guide Stronger Generators

The results across challenging programming benchmarks like CodeContests and LiveCodeBench are substantial. On CodeContests, the CTRL framework achieved up to 106.1% relative improvement in Pass@1 success rates compared to zero-shot generation. Crucially, the system enables significant test-time scaling, minimizing the number of revision iterations needed and mitigating the error accumulation that plagues baseline methods.

Furthermore, the research establishes a promising paradigm known as "weak-to-strong generalization." While the critic was trained using a Qwen2.5-Coder model as the generator, it demonstrated remarkable efficacy in guiding stronger, state-of-the-art models like GPT-4o. When paired with GPT-4o, the CTRL critic still delivered a 23.5% relative Pass@1 improvement on CodeContests, showing that specialized, well-trained critique models can effectively supervise more capable systems.

This decoupling of the critic role from the generator not only enhances the performance of AI systems on complex tasks but also provides a robust, scalable path toward building more reliable, autonomously self-improving LLMs.