---
layout: paper
pdf_url: https://arxiv.org/pdf/2502.08047
permalink: 2025-02-14/2502.08047/
title: AI Agents Tackle the Real World&#58; New Benchmark Tests Dynamic Desktop GUI
  Automation
---



A universal AI agent framework designed with "critical thinking" has demonstrated a significant leap in its ability to navigate complex and unpredictable desktop applications, according to a new paper from the National University of Singapore.

Researchers introduced **WorldGUI**, a comprehensive benchmark specifically engineered to test GUI (Graphical User Interface) automation agents in dynamic, real-world scenarios. While previous benchmarks often test agents starting from a software application's default, pristine state, WorldGUI simulates the chaotic context of daily human-computer interaction, where interfaces are rarely predictable.

The core innovation of WorldGUI lies in its use of "pre-actions," which instantiate tasks across 10 widely used desktop and web applications—including Microsoft PowerPoint, VSCode, Excel, and Adobe Acrobat—with diverse initial states.

For instance, if a task involves formatting text in a Word document, a conventional benchmark might start with a blank screen. WorldGUI, however, might use pre-actions to start the task with the necessary "Layout" tab already open, or, conversely, with an unrelated dialog box blocking the screen. This contextual variability challenges the agent to adapt its plan dynamically, rather than relying on a fixed, step-by-step script. The benchmark features 611 tasks, including dynamic augmentations that require immediate replanning.

To address the rigor of the new benchmark, the team also proposed **WorldGUI-Agent**, a framework built on the principle of critical thinking. The agent incorporates a three-module verification process:

1.  **Planner-Critic (Post-Planning Critique):** After generating an initial plan to complete a task (e.g., "format the whole document into two columns"), this module assesses the plan's accuracy and self-corrects based on the current environment screenshot.
2.  **Step-Check (Pre-Action Validation):** Before executing any subtask, this module verifies if the step is still necessary. If a pre-action has already opened the "Table Layout" tab, the Step-Check module identifies this intermediate state and skips the redundant click, preventing an error (like inadvertently closing the tab again).
3.  **Actor-Critic (Post-Action Evaluation):** This module ensures that every executed action (like a mouse click or keystroke) was successful, iteratively making corrections until the subtask is achieved.

Experimental results show that this integrated verification approach is highly effective. WorldGUI-Agent, using Claude-3.5 as its backbone, achieved a 36.0% overall success rate on the dynamic WorldGUI benchmark, outperforming the previous state-of-the-art model (Claude-3.5 Computer Use) by 12.4%.

However, the results also highlight the immense difficulty of truly mastering desktop automation in dynamic environments. Even the best AI agent's 36.0% success rate falls far short of the 85.3% success rate achieved by human experts performing the same augmented tasks. Furthermore, agents generalized poorly to augmented tasks, showing drastically reduced performance when faced with varied initial context compared to standard task setups.

The findings confirm that dynamic augmentation and adaptive planning are crucial frontiers in the race to build truly reliable GUI agents capable of assisting humans across the messy reality of everyday digital work.