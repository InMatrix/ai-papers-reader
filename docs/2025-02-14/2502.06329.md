---
layout: paper
pdf_url: https://arxiv.org/pdf/2502.06329
permalink: 2025-02-14/2502.06329/
title: New Benchmark Exposes Dangerous Reliability Gaps in Financial LLMs
---



**Financial language models (LLMs) used for critical tasks like risk analysis are failing simple real-world tests of reliability, often opting to fabricate answers rather than admit when they lack sufficient information.**

This is the key finding from researchers who have introduced a rigorous new evaluation tool, **FailSafeQA**, designed to test LLM resilience against the inevitable errors, linguistic inaccuracies, and context gaps inherent in human interaction within the financial sector.

As financial services increasingly rely on LLMs to process vast, long-context documents like 10-K annual reports, the consequences of missteps—or "hallucinations"—are profound. The FailSafeQA benchmark evaluates models in two crucial dimensions: Robustness and Context Grounding.

### Testing User Errors and Degraded Context

The first dimension, **Robustness**, measures an LLM’s ability to handle variations in user input without compromising accuracy. Researchers simulated common user interface failures in two categories:

1.  **Query Failure:** The original question is intentionally perturbed. For example, if a user asks for the "aggregate market value," the query might be entered with a common typo: "What was the *agregate* market *valiu* of Common Stock?" LLMs should still provide the correct financial figure.
2.  **OCRed Context:** The input document itself contains errors, mimicking low-quality Optical Character Recognition (OCR) typical of paper contracts scanned back into digital form. Despite corrupted text, the model must extract the correct facts.

The second dimension, **Context Grounding**, is critical for preventing fabrication. It tests whether an LLM can correctly identify when a query is unanswerable based on the provided documents and appropriately refuse to respond. This was tested using two challenging scenarios:

1.  **Missing Context:** Simulating a file upload failure, where the user provides a query but no financial report context. The LLM must decline the question.
2.  **Irrelevant Context:** Providing a query about, for instance, third-quarter earnings, but linking it to a document about a completely unrelated new product launch.

### The Robustness-Grounding Trade-off

Across 24 models tested, the results revealed a striking trade-off: highly robust models were often the worst at Context Grounding.

The most robust model tested, **OpenAI 03-mini**, handled input perturbations efficiently. However, this resilience came at a cost: it was found to fabricate information in 41% of the unanswerable test cases, indicating a high risk of dangerous hallucinations when context was missing or irrelevant.

Conversely, models focusing on precision demonstrated a better capacity for truthful refusal. The specialized model **Palmyra-Fin-128k-Instruct** demonstrated the highest Context Grounding score, effectively refusing to answer when context was absent or inadequate. However, it still struggled to maintain robust predictions, failing in 17% of cases where inputs were perturbed.

The researchers introduced the **LLM Compliance Score** to quantify this necessary balance between Robustness (answering correctly despite noise) and Context Grounding (refusing to answer when necessary). Text generation tasks, such as generating a blog post summarizing financial details, were found to be especially vulnerable to context issues, suggesting developers may need to structure these tasks as multiple steps rather than single, long-context prompts.

The findings highlight that even the best-performing LLMs still have significant room for improvement before they can be deemed dependably fail-safe in high-stakes financial applications. FailSafeQA is now available as a public dataset to aid in developing truly compliant financial AI.