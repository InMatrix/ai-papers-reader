---
layout: paper
pdf_url: https://arxiv.org/pdf/2502.07776
permalink: 2025-02-14/2502.07776/
title: AI's Invisible Security Flaw&#58; LLM Response Times Leak Private User Data
---



New research has revealed that a common performance optimization used by Large Language Model (LLM) APIs—prompt caching—inadvertently creates a "side channel" allowing attackers to infer other users' private prompts and even deduce proprietary architectural details of the models themselves.

Prompt caching is a technique LLM providers use to speed up service and reduce computational load. When a user submits a prompt, the model caches the intermediate computation (known as the key-value or KV cache) for that prompt's prefix. If a subsequent request shares that same prefix, the model skips the initial steps, resulting in a significantly faster response time—the cache hit.

Researchers developed a rigorous statistical audit using timing attacks to detect and quantify this mechanism across 17 major, real-world LLM API providers, including services from OpenAI, Anthropic, and Google. They found that these timing differences—a few hundred milliseconds faster for a cache hit versus a cache miss—can be measured reliably, introducing a critical risk of information leakage.

### The Global Cache Problem

The core danger arises when the prompt cache is shared across different users. If an attacker sends a prompt and receives an unusually fast response, they can infer that another, unrelated user recently sent a prompt starting with the same prefix.

Consider a concrete example: A victim user sends the sensitive legal query, "The contract clearly specifies that Section 3..." to an API. This phrase is cached. Later, an attacker sends the phrase, "The contract clearly specifies that Section 3 is null and void." Because the second prompt shares a long prefix with the cached prompt, the attacker receives a nearly instant response. This timing discrepancy informs the attacker that the "victim" recently discussed the exact starting phrase, "The contract clearly specifies that Section 3," revealing the topic of discussion and potentially sensitive information.

The audit confirmed prompt caching in 8 out of 17 providers. Most alarmingly, 7 of these providers, including major players like OpenAI (for certain models), Deep Infra, and Perplexity, were found to employ **global cache sharing**. This level of sharing means the cache is available to *all* users of the API, regardless of organization, presenting the highest possible risk for privacy leakage.

### Unmasking Model Architecture

Beyond user privacy, the audit technique also revealed previously undisclosed technical details about proprietary models. The researchers leveraged the fact that cache hits based on matching prefixes but differing suffixes are only possible in **decoder-only Transformer** architectures.

By detecting this exact pattern of prefix caching in OpenAI’s proprietary **text-embedding-3-small** API, the researchers provided empirical evidence that the model utilizes a decoder-only architecture—a technical specification previously kept private by OpenAI. For proprietary models, this leakage constitutes a loss of intellectual property.

### Responsible Disclosure and Mitigation

The research team performed responsible disclosure to all affected providers in October 2024, giving them time to address the issues before public release of the findings. At least five providers have since made changes to mitigate these vulnerabilities, such as disabling global cache sharing or updating documentation.

The findings underscore the need for greater transparency around LLM API infrastructure. Experts suggest that the most effective mitigation for privacy risk is enforcing **per-user caching**, ensuring that an attacker can only benefit from caching their own previously sent prompts. Alternatively, providers must aggressively mask timing differences to eliminate the side channel entirely, even if it sacrifices some performance benefit.