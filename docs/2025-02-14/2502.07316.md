---
layout: paper
pdf_url: https://arxiv.org/pdf/2502.07316
permalink: 2025-02-14/2502.07316/
title: CODEI/O&#58; New Training Method Condenses Complex Code Logic into Universal
  Reasoning Skills for LLMs
---



In a significant stride toward enhancing the reasoning capabilities of Large Language Models (LLMs), researchers have introduced CODEI/O, a novel training approach that systematically extracts and condenses foundational reasoning patterns embedded within diverse software codebases.

Current LLMs often excel at narrow tasks like code generation or elementary math but struggle with complex, generalized reasoning—such as logical deduction or scientific inference—due to sparse and fragmented training data. CODEI/O addresses this by flipping the script on traditional code training: instead of teaching models to generate raw code, it trains them to predict either the inputs or the outputs of executable functions, providing the entire explanation as a natural language Chain-of-Thought (CoT).

This approach transforms raw, contextually-grounded code into a reasoning playground, forcing the model to articulate the procedural logic behind the code in plain English.

The core methodology involves curating hundreds of thousands of high-quality Python functions from diverse sources, including complex algorithms, STEM problems, and logic puzzles. These functions are converted into a standardized format. The model is then given the function's code, a textual query describing its purpose, and either a test input or a ground-truth output. The task is to deduce the missing piece.

Consider the classic coin change problem, which requires finding the minimum number of coins needed to reach a target amount using a given set of denominations.

In an *output prediction* task, the model is given the function and the input (`Amount: 25`, `Coins: [1, 4, 7]`). The model must then reason through the state-space exploration (e.g., trying 3 coins of 7, then 1 coin of 4) to conclude the minimum count is 4.

Crucially, CODEI/O also enforces *input prediction*. Given the function and the desired output (e.g., 4), the model must reverse-engineer a feasible input, such as finding a new amount and set of coins (`Amount: 13`, `Coins: [1, 2, 5]`) where the optimal solution requires exactly four coins. This inverse task is essential for decoupling core logical flows—like decision tree traversal and modular decomposition—from syntactic restraints.

The researchers demonstrated that models trained with CODEI/O exhibit universal and balanced performance improvements across 14 major benchmarks. Testing four different base models (ranging from Qwen 2.5 Coder 7B to Gemma 2 27B), the approach delivered gains not only on code-related tasks but also on general domains including symbolic, math, numerical, and commonsense reasoning (such as Winogrande and DROP).

An enhanced version, CODEI/O++, further improves robustness by incorporating multi-turn revision. If an initial prediction is incorrect, the model receives execution feedback and is prompted to revise its natural language CoT explanation, demonstrating that verification and self-correction based on logical execution rigor are key drivers of enhanced reasoning ability.

By distilling complex procedural logic into scalable, verifiable natural language CoT examples, CODEI/O offers a powerful and cost-effective intermediate training stage, successfully leveraging the structured nature of code to boost general intelligence in LLMs.