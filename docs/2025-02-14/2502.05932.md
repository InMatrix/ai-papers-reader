---
layout: paper
pdf_url: https://arxiv.org/pdf/2502.05932
permalink: 2025-02-14/2502.05932/
title: AI Agents Master Human-Like Learning&#58; New Framework Stores and Blends Skills
  in Parameter Space
---



Autonomous agents—from robotics to complex decision-making systems—have long struggled with a fundamental inefficiency: whenever they encounter a new task, they often must learn it from scratch, frequently forgetting previously mastered skills in the process. This "tabula rasa" paradigm contrasts sharply with human intelligence, which excels at reusing and composing existing knowledge.

A new framework, Parametric Skill Expansion and Composition (PSEC), offers a breakthrough solution by enabling agents to efficiently evolve their capabilities and dynamically program new skills by leveraging a manageable, expandable skill library. Developed by researchers from Tsinghua University and the National University of Defense Technology, PSEC achieves this through innovative use of parameter-efficient finetuning and context-aware composition.

### Plug-and-Play Skill Modules

The core mechanism of PSEC is the utilization of Low-Rank Adaptation (LoRA) modules. Instead of retraining an entire neural network for every new skill—which is computationally expensive and risks "catastrophic forgetting"—PSEC encodes each skill as a distinct, low-rank LoRA module, essentially a small, plug-and-play add-on to a frozen base policy network.

This setup facilitates what the authors call **Skill Expansion**. For example, if an agent is initially trained to "Stand" (the base policy), and then needs to learn to "Walk," the "Walk" skill is added as a small LoRA module. This modular structure minimizes the number of new parameters needed, significantly reducing training time and memory, while ensuring the underlying ability to "Stand" is never forgotten.

The true innovation lies in **Skill Composition**, where PSEC blends these discrete LoRA modules directly within the network’s parameter space. Prior methods often attempted to compose skills late in the process—by mixing the final actions or the noisy inputs—limiting their ability to utilize shared knowledge between skills. PSEC, in contrast, merges the actual weight matrices (parameters) of the LoRA modules early on, allowing for far more expressive and complementary combinations.

### Context-Aware Dynamic Blending

To ensure flexible adaptation to real-time challenges, PSEC introduces a small context-aware network that dynamically determines the optimal blending weights for each skill module based on the agent’s current state.

This dynamic blending is crucial for complex tasks like multi-objective control, such as autonomous driving in the DSRL benchmark (Safe Offline Reinforcement Learning). Here, the agent must constantly balance conflicting goals: maximizing progress (Reward Policy) while minimizing collision risk (Safety Policy).

PSEC trains separate LoRA modules for the Reward and Safety policies. When the agent is driving smoothly on a straight road, the context-aware network assigns a high weight to the Reward Policy module. However, the moment the agent approaches an obstacle or a tight curve, the network instantly shifts the weight almost entirely to the Safety Policy module (as shown in the paper’s visualizations), guaranteeing a safe response.

Across rigorous tests on the DeepMind Control Suite and D4RL benchmarks, PSEC demonstrated superior performance in multi-objective composition, continual policy shifts (e.g., learning to stand, then walk, then run), and adapting to dynamic shifts (e.g., sudden changes in robot friction or gravity). The framework showed it can achieve superior task performance using only 7.58% of the parameters compared to training from scratch, marking a significant step toward developing highly efficient and versatile autonomous agents.