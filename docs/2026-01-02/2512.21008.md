---
layout: paper
pdf_url: https://arxiv.org/pdf/2512.21008
permalink: 2026-01-02/2512.21008/
title: GateBreaker Attack Framework Reveals Critical Safety Flaw in Leading AI Models
---



A new, lightweight attack framework dubbed "GateBreaker" has exposed a fundamental security vulnerability in state-of-the-art Mixture-of-Experts (MoE) Large Language Models (LLMs), including those from major developers like OpenAI, Microsoft, and Alibaba. The research demonstrates that the safety mechanisms designed to prevent harmful output in these highly efficient AI architectures are not uniformly distributed but are instead concentrated in a tiny fraction of the models' internal wiring, making them susceptible to precise, inference-time "surgical strikes."

MoE models, such as Mixtral and DeepSeek-MoE, achieve massive scale while remaining computationally affordable by utilizing a "gate" mechanism to activate only a sparse subset of specialized subnetworks, or "experts," for any given input token. Researchers hypothesized that this modular design creates unique failure modes, suggesting that if a harmful prompt avoids the few experts responsible for safety checks, it could bypass alignment entirely.

GateBreaker operates on the reverse principle, viewing the safety alignment—the behavioral constraints added post-training (e.g., via RLHF)—as a localized internal "gatekeeper." The framework works in three stages without needing to retrain the model. First, it performs *gate-level profiling* to identify the specific "safety experts" most frequently activated by malicious inputs. Second, *expert-level localization* pinpoints the most critical "safety neurons" within these experts that correlate strongly with refusal behavior. Finally, *targeted safety removal* selectively disables (masks) these few neurons during inference, effectively hollowing out the safety function.

This minimal intervention creates a "safety trap": malicious tokens are still routed to the correct safety experts, but those experts have been stripped of their ability to enforce refusal, causing the model to generate harmful content fluently.

The attack proved highly effective across eight modern MoE LLMs, increasing the average Attack Success Rate (ASR) from a baseline of 7.4% to 64.9% against malicious prompts. Crucially, this dramatic failure was achieved by modifying fewer than 3% (2.6% on average) of the neurons in the targeted expert layers, demonstrating extreme precision and negligible impact on the model’s general utility on benign tasks.

The vulnerability is also transferable. In one-shot transfer attacks, safety neurons identified in a source model (e.g., GPT-OSS-20B) could be successfully used to attack related variants trained for different domains (like math or marketing), boosting ASR from 17.9% to 67.7% without any new profiling. Furthermore, GateBreaker generalized beyond text, successfully compromising MoE Vision Language Models (VLMs), raising the ASR on unsafe image inputs from 20.8% to 60.9%.

These findings confirm that safety alignment in current MoE architectures is concentrated and fragile, existing as small, localized structural modifications. The research highlights an urgent, overlooked security gap, suggesting that future MoE models must implement safety redundancy by explicitly distributing refusal behavior across a wider range of experts and layers to prevent these surgical attacks.