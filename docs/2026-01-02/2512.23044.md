---
layout: paper
pdf_url: https://arxiv.org/pdf/2512.23044
permalink: 2026-01-02/2512.23044/
title: New AI Agent, Video-Browser, Cracks the Code on Open-Web Video Research
---



Autonomous AI agents, which have fundamentally reshaped information seeking by actively navigating the open web, have long faced a critical blind spot: video. Unlike static text or images, video is the web’s most dynamic and information-dense modality, posing an enormous computational challenge for AI systems that need to seek, retrieve, and reason over visual evidence.

A new paper formally defining the challenge of Agentic Video Browsing introduces a novel architecture, **Video-Browser**, that addresses this bottleneck. By employing a three-stage "Pyramidal Perception" strategy, the agent achieves a substantial accuracy increase while drastically cutting down on the prohibitive token costs traditionally associated with processing long-form visual content.

### The Trade-off: Precision vs. Cost

Current agent paradigms are caught between a difficult trade-off.

On one hand, **Direct Visual Inference**—streaming raw video frames directly to a large language model (LLM)—offers the highest potential accuracy but incurs a "context explosion," making it unscalable for open-ended web research across multiple long videos. On the other, relying solely on **Text-centric Summarization** (using transcripts and subtitles) is cheap but suffers from a critical "modality gap," missing subtle, fine-grained visual details essential for verification.

To resolve this dilemma, Video-Browser treats web videos like a pyramid, allocating expensive processing power only when necessary.

The process involves three stages:
1.  **Semantic Filter:** The agent first uses cheap textual metadata (titles, snippets) to quickly prune irrelevant search results, incurring zero visual inference cost.
2.  **Sparse Localization:** For promising videos, the agent performs a sparse scan, combining the full transcript with a fixed set of sparse frames to identify relevant temporal windows—the "glimpses" where the evidence might reside.
3.  **Zoom-in:** Only on these prioritized temporal windows does the agent perform high-fidelity **Targeted Visual Decoding**, scrutinizing the dense visual stream to verify fine-grained details.

### Benchmarking Real-World Complexity

To test this approach, the researchers introduced **Video-BrowseComp**, a rigorous benchmark featuring 210 open-ended questions designed to enforce mandatory video dependency. These tasks scale from Level 1 (Explicit Retrieval) to Level 3 (Multi-Source Reasoning), where the agent must aggregate fragmentary evidence across disparate videos, much like a real researcher.

For example, a traditional text-only search agent might fail a Level 2 question like identifying the color of a specific prop, such as the pen cap worn by the character Walter Mitty in a film. Textual summaries focus on the plot, not the prop’s color. Video-Browser, however, succeeds by using its **Zoom-in** stage to verify the detail in a close-up frame, confirming the cap is red, a crucial plot detail easily missed by text.

The system also excels in efficiency. In one complex task requiring the final score of the game that led to Charles Barkley losing a famous bet to Yao Ming, Video-Browser used its planner to iteratively refine search queries and, once relevant clips were found, precisely zoomed into the crucial timestamp (e.g., 20.2s - 31.5s) where the score (20 points) was stated in the video’s recap.

Video-Browser (leveraging GPT-5.1) achieved a state-of-the-art overall accuracy of 26.19%—a 37.5% relative improvement over direct visual inference baselines. Crucially, this performance came with a **58.3% reduction in total token consumption**, confirming that Pyramidal Perception successfully balances the precision needed for fine-grained verification with the scalability required for open-web research.