---
layout: paper
pdf_url: https://arxiv.org/pdf/2512.24873
permalink: 2026-01-02/2512.24873/
title: From Prompts to Projects&#58; New AI Ecosystem Builds Reliable Autonomous Agents
---



A team of researchers has unveiled a comprehensive new infrastructure designed to overcome the key bottlenecks hindering the deployment of autonomous AI agents in real-world, complex tasks like software engineering. The innovation, called the Agentic Learning Ecosystem (ALE), serves as the foundational bedrock for **ROME (ROME is Obviously an Agentic ModEl)**, an open-source agent model that achieves state-of-the-art performance, rivaling models several times its size.

Unlike traditional large language models (LLMs) that generate one-shot responses, agentic crafting requires the AI to operate in dynamic environments, taking actions, observing outcomes, and iteratively refining solutions over multiple turns. Until now, the lack of a scalable, end-to-end ecosystem has stifled development.

The ALE addresses this gap by unifying the entire pipeline, from data generation to deployment, through three synergistic components:

1.  **ROLL (Reinforcement Learning Optimization for Large-Scale Learning):** A highly efficient framework for training the agent policy.
2.  **ROCK (Reinforcement Open Construction Kit):** A secure, sandboxed execution environment.
3.  **iFlow CLI:** An agent framework that manages context and provides the user-facing interface.

The core functionality of ALE rests on the secure isolation provided by ROCK. For instance, when an agent is tasked with fixing a complex software bug, ROCK spins up a secure, sandboxed environment. This allows the agent to run code, compile tests, and invoke system tools without the risk of accidentally corrupting real production systems or incurring security risks—acting like a virtual mechanic’s bay where mistakes are confined.

Grounded in this ecosystem, the ROME model employs a novel training technique called **Interaction-Perceptive Agentic Policy Optimization (IPA)**. This algorithm fundamentally changes how the agent learns from its multi-turn interactions. Instead of assigning credit or blame based on individual tokens (which often leads to unstable learning over long tasks), IPA operates on "semantic interaction chunks."

For example, when an agent executes a three-step action—*Reason about the bug, run a diagnostic command, and receive the system output*—IPA treats this functional unit as a single chunk for credit assignment. This mirrors how a manager might evaluate an engineer: rewarding the success of a coherent subgoal rather than grading every word typed in between the commands. This approach improves stability and efficiency across long-horizon tasks.

Trained on over one million trajectories generated within the ALE, ROME (a 30B Mixture-of-Experts, or MoE, model) demonstrates exceptional scaling efficiency. Empirical results show ROME substantially outperforms other normal-sized models and even rivals larger agents exceeding 100 billion parameters. ROME achieved an accuracy of 57.40% on SWE-bench Verified (a benchmark for resolving real-world GitHub issues) and 24.72% on Terminal-Bench 2.0.

The researchers also introduced a new, more rigorous benchmark, Terminal Bench Pro, to ensure stricter evaluation against contamination and complexity. Even on this tougher standard, ROME demonstrated strong generalization. The model has already been successfully integrated into production via iFlow CLI, validating the practicality of the ALE approach in building robust, high-performance agents for the next era of AI.