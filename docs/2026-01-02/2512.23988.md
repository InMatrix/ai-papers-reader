---
layout: paper
pdf_url: https://arxiv.org/pdf/2512.23988
permalink: 2026-01-02/2512.23988/
title: Scientists Uncover ‘Thought Vectors’ in LLMs, Gaining Real-Time Control Over
  Reasoning
---



A new study from Google DeepMind introduces an unsupervised framework that allows researchers to peer deep inside the "black box" of large language models (LLMs) and identify the specific internal directions—or vectors—that govern complex thought processes like reflection and planning.

The research, detailed in a paper titled *Fantastic Reasoning Behaviors and Where to Find Them*, proposes a framework called **RISE** (Reasoning behavior Interpretability via Sparse auto-Encoder). RISE moves beyond older methods that required human labeling to define concepts (like "happy" or "sad"), instead allowing the AI itself to discover its own fundamental reasoning components.

### Unsupervised Discovery of Atomic Thoughts

While LLMs have shown remarkable reasoning skills, often relying on "Chain-of-Thought" (CoT) prompts to break down complex problems, the exact mechanisms governing these steps have remained opaque. Previous work in mechanistic interpretability typically relied on supervised methods, where researchers had to pre-define behaviors like 'reflection' and manually label data.

The RISE team sidestepped this limitation by employing Sparse Auto-Encoders (SAEs) trained directly on the raw, unlabeled hidden states generated as an LLM processes a reasoning trace. The resulting SAEs learned a dictionary of **Reasoning Vectors**—linear directions in the model’s activation space—where each vector corresponds to a distinct, disentangled cognitive behavior.

Visualization confirmed that these vectors weren't random noise; they clustered into semantically coherent groups representing human-interpretable behaviors such as **reflection** (self-checking previous steps) and **backtracking** (abandoning a path for an alternative solution).

### Steering the Model’s Mind

The most compelling finding is the ability to causally manipulate the model’s reasoning style in real-time without any additional training.

By targeting a specific reasoning vector—say, the direction corresponding to "reflection"—and slightly amplifying or suppressing the activity along that vector during inference, researchers could dynamically steer the model’s behavior.

For instance, when given a complex geometry problem (like converting rectangular coordinates (0, 3) to polar coordinates), the baseline model might spontaneously perform **four steps of reflection** ("Wait, let me double-check...").

*   Applying a **positive intervention** to the reflection vector caused the model to amplify its self-checking, resulting in **six or more steps of reflection**, producing a longer, more verbose reasoning trajectory.
*   Conversely, a **negative intervention** suppressed the vector, forcing the model to complete the task with only **two reflection steps**, yielding a shorter, more direct response.

Critically, these interventions successfully shaped the *style* of reasoning while consistently preserving the correctness of the final answer, demonstrating fine-grained control over internal cognition.

### Beyond Human Labels

The unsupervised approach also proved vital for discovering new, subtle behaviors difficult to define in human language. As a case study, the team used the framework to identify vectors related to the model’s **confidence** (linked to low entropy in predictions).

By boosting these confidence vectors, researchers could shift the LLM’s response style toward answers framed with greater assurance and conviction. Furthermore, RISE also revealed structural organizational principles within the model, identifying separate clusters of vectors corresponding to **response length**—distinguishing between long and short reasoning traces.

This work marks a significant advance in LLM interpretability, offering a scalable method for mapping the cognitive landscape of powerful AI models and enabling dynamic control over their core reasoning processes.