---
layout: paper
pdf_url: https://arxiv.org/pdf/2512.22255
permalink: 2026-01-02/2512.22255/
title: AI Training Breakthrough&#58; LLMs Learn Better From Flawed But Familiar Reasoning
---



A new study challenges the long-held assumption that correctness is the primary metric for high-quality language model training data. Researchers have found that large language models (LLMs) often achieve superior performance on complex reasoning tasks, like math and algorithmic problem-solving, when fine-tuned on synthetic Chain-of-Thought (CoT) traces generated by a stronger model—even if those traces lead to an incorrect final answer.

The paper, titled “Shape of Thought: When Distribution Matters More Than Correctness in Reasoning Tasks,” identifies that the *distributional closeness* of the training data to the model’s native output is a more critical factor than previously understood.

Traditionally, supervised fine-tuning (SFT) for reasoning relies on human-annotated, fully correct solutions (the "gold standard" or H data). However, the team demonstrated that synthetic traces, even those leading to incorrect final results (dubbed "W" traces, generated by a powerful teacher model), consistently outperformed human-written H data across models like Gemma, Llama, and Qwen, and benchmarks including MATH, GSM8K, and MBPP (code generation).

### The Intuition: Good Steps, Bad Outcome

The key insight is that synthetic traces, being machine-generated, inherently align better with the internal "language" or distribution of the LLM being trained. Furthermore, a trace that yields the wrong final answer is rarely useless noise; it often contains valid, reusable intermediate reasoning steps, spoiled only by a localized error, such as a single arithmetic mistake.

Conversely, a fully correct, human-written trace might be stylistically distant from the model’s natural output, making it harder to learn from. The researchers provide compelling examples where a model-generated trace with the correct final answer (G trace) used a fundamentally flawed logical approach that coincidentally arrived at the right number. Meanwhile, a 'W' trace for the same problem might follow a sound logical decomposition but suffer a minor, localized arithmetic error. The LLM benefits more from the useful intermediate structure of the W trace than the distributionally distant, albeit strictly correct, H trace.

### Aligning the Distribution

To prove that distributional alignment—the "shape of thought"—was the key, the researchers conducted a paraphrasing experiment. They used an LLM to rewrite the high-quality human (H) solutions, intentionally shifting their style closer to the model’s native distribution without altering the logical content. Fine-tuning models on these paraphrased H traces yielded significant performance improvements over using the original human data, bridging the gap between human and synthetic performance.

In an experiment testing the limits of this robustness, the models were fine-tuned on datasets that included increasingly high proportions of completely flawed CoT traces. The results showed remarkable tolerance: models trained on data consisting of up to 25% *fully* flawed reasoning still performed comparably to those trained entirely on human-written CoTs. Performance only began to degrade substantially once the error rate exceeded this moderate threshold.

This research implies that current strategies for curating reasoning datasets, which often rely solely on final-answer correctness, are discarding valuable training signals. For future LLM development, prioritizing data that is distributionally similar to the model’s internal processes may be as critical as ensuring factual accuracy.