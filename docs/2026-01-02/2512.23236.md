---
layout: paper
pdf_url: https://arxiv.org/pdf/2512.23236
permalink: 2026-01-02/2512.23236/
title: Meta Deploys AI Agents to Write and Optimize Code for Custom Accelerators,
  Achieving Up to 17x Speedup
---



Meta Platforms has unveiled KernelEvolve, an advanced agentic artificial intelligence framework designed to autonomously generate and optimize high-performance computational kernels—the low-level primitive functions underpinning deep learning recommendation models (DLRMs). Deployed across Meta’s massive fleet of heterogeneous AI accelerators, including NVIDIA GPUs, AMD GPUs, and Meta's proprietary Meta Training and Inference Accelerator (MTIA), the system addresses a critical scalability crisis in modern AI infrastructure.

The need for automated kernel generation stems from the "Curse of Dimensionality" in Meta's serving architecture. Recommendation models operate under strict sub-second latency constraints, but the combination of diverse model types, hundreds of unique operators, and heterogeneous hardware creates an optimization space too complex for manual development.

A key challenge highlighted by the research is kernel coverage for complex operators, particularly those handling data preprocessing (like feature engineering). If a critical preprocessing operator lacks an optimized native implementation on an AI accelerator, the production system is forced into a disaggregated architecture—using a separate CPU tier for preprocessing. This adds significant architectural overhead, incurring 10 to 20 milliseconds of latency, consuming a substantial portion of the sub-100ms budget required for ad serving.

KernelEvolve tackles this by automating the kernel lifecycle, reducing the development time for optimized kernels from weeks to mere hours. The system frames kernel optimization as a graph-based search problem, where an AI agent—referred to as a "universal operator"—iteratively refines kernel implementations. This process is dynamically guided by real-time execution feedback, including multi-level profiling and correctness validation.

Crucially, KernelEvolve uses a persistent knowledge base to inject hardware-specific constraints and optimization patterns into the agent's context. This mechanism proves vital for proprietary hardware like MTIA. Since the MTIA architecture is absent from standard LLM training data, KernelEvolve teaches the LLM Meta-specific programming idioms, memory access patterns, and even low-level compiler directives to ensure generated kernels exploit unique MTIA features, such as specialized function units (SFUs) and cross-Processing Element communication primitives.

The results demonstrate KernelEvolve’s dual value proposition: enabling model deployment on new hardware and delivering substantial performance gains. In production use cases, KernelEvolve-generated kernels achieved speedups ranging from 1.25x up to 17x over PyTorch baselines.

For instance, optimizing a 1D convolution kernel used in Convolutional Transformer models resulted in a 2.3x speedup on NVIDIA H100 GPUs through operator fusion—combining several operations into a single kernel launch, thereby eliminating redundant data movement. Furthermore, the memory-bound data preprocessing operator `MergeBucketizedDenseTransform` (MBDT), which maps continuous features to bin indices, achieved up to 9.25x speedup on MTIA v2i by fusing the entire bucketization pipeline and utilizing MTIA-specific vectorization.

By automating the synthesis of correct, high-performing kernels across its diverse accelerator fleet, KernelEvolve transforms kernel availability from a deployment bottleneck into an enabler, mitigating the high latency and cost penalties associated with manual development and disaggregated serving architectures.