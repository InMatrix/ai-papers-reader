---
layout: paper
pdf_url: https://arxiv.org/pdf/2601.14209
permalink: 2026-01-23/2601.14209/
title: LLMs Learn to Correct Their Own Mistakes, Boosting Math Reasoning by 14%
---



A new technique called Intervention Training (InT) allows Large Language Models (LLMs) to perform granular self-correction during complex reasoning tasks, solving a long-standing challenge in training advanced AI. Developed by researchers at Carnegie Mellon University and the University of Illinois Urbana-Champaign, InT enables LLMs to precisely identify and fix errors within their own long reasoning paths, leading to significant performance gains on difficult math benchmarks.

Standard reinforcement learning (RL) struggles with "credit assignment" in multi-step reasoning. When an LLM attempts a problem like an Olympiad-level math question and the final answer is incorrect, outcome-based RL punishes the entire sequence of hundreds of tokens uniformly. This penalizes correct intermediate steps alongside critical mistakes, preventing efficient learning.

The InT paradigm bypasses this issue by exploiting a crucial asymmetry: it is often easier for an LLM to verify a single reasoning step against a known reference solution than to generate the entire correct solution from scratch.

### The Self-Correction Mechanism

When an LLM generates a failed reasoning trace, InT instructs the model to act as a verifier. It compares its incorrect trace against an available reference solution and identifies the location of the *first critical error*.

Once the error location is pinpointed, the LLM then proposes a single-step **intervention**: a short, corrective natural language prompt designed to steer the reasoning trajectory back toward the correct path.

For example, if an LLM is solving a complex algebra problem and commits a fatal logical fallacy at step 50, InT isolates that mistake. The intervention might read: “Wait—consider a sequence that takes only finitely many values. Try to understand how the number of distinct blocks relates to the periodicity of the sequence.”

The model then generates a *counterfactual continuation* based on this correction. Researchers found that conditioning rollouts on these interventions increases the probability of success on previously failed problems by over 22 times compared to simply continuing from the error.

### Superior Initialization for RL

These successful intervention-guided traces are then used for supervised fine-tuning (SFT), providing a highly targeted learning signal. Instead of indiscriminately reinforcing entire successful rollouts (which are often "off-policy" and dissimilar to the base model’s behavior), InT reinforces only the corrected prefixes and the intervention step itself, effectively "patching" the base model's logic at the point of failure.

This patch provides a substantially improved initialization for subsequent RL training. By focusing learning on these precise error points, InT ensures the LLM internalizes the corrective pattern.

In rigorous testing on challenging benchmarks, including IMO-AnswerBench (curated by former International Math Olympiad medalists), an InT-trained 4-billion-parameter model achieved an accuracy score of 25.62%. This represents a nearly 14% improvement over a strong standard RL baseline and surprisingly outperformed much larger open-source models such as gpt-oss-20b.

The success of Intervention Training demonstrates a simpler, more scalable path to advanced LLM reasoning, leveraging the model’s internal verification skills to achieve fine-grained credit assignment without the computational expense of generating explicit step-level reward models or running massive branched rollouts.