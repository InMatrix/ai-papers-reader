---
layout: paper
pdf_url: https://arxiv.org/pdf/2601.12346
permalink: 2026-01-23/2601.12346/
title: New Benchmark Reveals AI Research Agents Struggle to Ground Claims in Visual
  Evidence
---



A new study by researchers from The Ohio State University, Amazon Science, and several other institutions has introduced MMDeepResearch-Bench (MMDR-Bench), the first end-to-end benchmark designed to test the ability of Deep Research Agents (DRAs) to synthesize long-form, citation-rich reports using multimodal evidence, including charts, diagrams, and photos.

While modern Large Language Models (LLMs) excel at processing text, this research highlights a critical vulnerability: the moment agents must interpret visual evidence—like a data chart or complex diagram—and align it strictly with their generated claims, fidelity often breaks down.

The MMDR-Bench consists of 140 expert-crafted tasks spanning 21 domains, forcing DRAs to engage in multi-step workflows. Tasks are packaged as image-text bundles across two regimes: lightweight "Daily" tasks and dense "Research" tasks. For instance, a *Daily* task might require a user to identify if a specific eyedrop shown in a picture is suitable for their dry-eye condition, requiring web search grounded by visual identification. In contrast, a *Research* task might demand extracting structural facts from a complex GPT architecture diagram, then synthesizing those facts with external research on memory complexity, all with stringent citations.

To grade these complex outputs, the team developed a rigorous, three-module evaluation pipeline. The **Formula-LLM Adaptive Evaluation (FLAE)** measures general report quality, assessing readability and insightfulness. However, the core innovation lies in the fidelity checks:

The **Trustworthy Retrieval-Aligned Citation Evaluation (TRACE)** audits whether claims are verifiably grounded in cited sources. Nested within TRACE is the crucial **Visual Evidence Fidelity (VEF)** metric. VEF is a strict PASS/FAIL check that ensures the agent’s textual claims align perfectly with a task-specific textualized visual ground truth. For example, if an image shows an investment return of "10.1%" on a chart axis, the DRA must not simply claim "10% growth"; VEF penalizes such misreadings of fine-grained literals.

Finally, the **Multimodal Support-Aligned Integrity Check (MOSAIC)** ensures consistency between textual claims that reference images and the visual artifacts themselves, routing the check differently for photos, data charts, and diagrams.

The systematic evaluation of 25 state-of-the-art LLMs and agentic systems, including multimodal and web-enabled models, revealed persistent trade-offs. The Gemini Deep Research agent (Gemini 3 Pro) ranked highest overall, driven by strong evidence coverage (TRACE) and competitive multimodal alignment (MOSAIC).

Crucially, the study found that merely adding vision capabilities does not guarantee success. Comparisons within model families showed that vision-enabled models experienced an increase in "Detail/Text Extraction (DTE)" errors, such as misreading small numerals or table cells. The data confirms a central bottleneck: strong writing quality (high FLAE scores) frequently diverged from faithful evidence use (lower TRACE/MOSAIC scores), proving that generating fluent, citation-rich prose does not guarantee reliable multimodal grounding for today’s deep research agents. Multimodal integrity remains the key technical frontier for verifiable AI research.