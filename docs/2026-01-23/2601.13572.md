---
layout: paper
pdf_url: https://arxiv.org/pdf/2601.13572
permalink: 2026-01-23/2601.13572/
title: Researchers Solve Reinforcement Learning Model Merging Paradox, Creating Super-Agent
  Generalists
---



A team of researchers has unveiled a new technique, Reinforced Agent Merging (RAM), that solves a critical bottleneck in artificial intelligence: combining specialized agent models trained via reinforcement learning (RL) without losing their unique, high-value skills.

Existing model merging methods, which combine multiple fine-tuned models into a single generalist, were primarily designed for models trained through supervised fine-tuning (SFT). When applied to modern RL-trained agentic models—specialized in domains like coding, tool use, or long-context memory—these methods led to significant performance degradation.

The root of the problem, according to the paper, lies in the fundamental difference between SFT and RL update vectors. SFT produces dense, globally comparable updates. In contrast, on-policy RL training induces *sparse* and *heterogeneous* task vectors, where specialized knowledge is encoded in a small, often non-overlapping set of parameters.

When standard averaging techniques are used on these sparse RL vectors, the unique, critical parameter updates are inadvertently divided by the total number of merged models—a phenomenon the researchers term "Signal Dilution." For example, a dedicated coding agent might only modify 3.2% of its total parameters to achieve expert performance; if that small, critical subset is averaged with zero updates from memory or tool-use agents, the coding skill is severely diluted and lost.

RAM addresses this by introducing a distribution-aware strategy that explicitly disentangles shared and unique parameter updates. The method first "probes" the reinforced task vectors, identifying which parameters are shared (updated by multiple agents) and which are unique (updated by only one).

In the final merge, RAM applies a selective approach: parameters shared across agents are averaged to build consensus on general skills, while unique, task-specific parameters are completely preserved. Crucially, RAM+ (the version incorporating a scaling factor) then rescales these unique parameters to compensate for any performance loss incurred by averaging even the shared regions.

Experiments across coding (CURE), tool-use (ToolRL), and long-context memory (MemAgent) domains demonstrate that RAM not only outperforms established merging baselines but achieves "synergistic potential."

The merged generalist model, in many cases, surpassed the performance of the original specialized agents on their own domain tasks. For instance, the RAM+ agent achieved a score of 82.03 on the challenging SQUAD 64K long-context memory task, beating the dedicated MemAgent specialist (77.34). This success confirms that combining the specialized behavioral knowledge—like the robust reasoning from the coding agent and the retrieval power of the memory agent—results in a superior unified model, a new state-of-the-art for merging reinforced agents.