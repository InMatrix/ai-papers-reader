---
layout: paper
pdf_url: https://arxiv.org/pdf/2601.13591
permalink: 2026-01-23/2601.13591/
title: DSAEval Benchmark Reveals Autonomous Data Agents Excel at Spreadsheets, Struggle
  with Vision
---



A new, comprehensive benchmark designed to test the real-world capabilities of Large Language Model (LLM)-based data science agents has exposed a critical performance gap: while agents are mastering structured data analysis, they still falter significantly when tackling unstructured domains like computer vision and natural language processing.

The benchmark, named DSAEval, was introduced by researchers to overcome the limitations of previous evaluations, which typically focused on simple, single-step tasks using tabular data. DSAEval challenges 11 advanced models, including proprietary and open-source systems, across 641 distinct data science problems spanning domains from Statistical Testing to Complex Deep Learning workflows.

### Mimicking Real-World Data Science

To rigorously simulate authentic data science projects, DSAEval incorporates four key innovations:

1.  **Broad Domain Coverage:** The tasks are grounded in 285 diverse datasets, including deep learning tasks that require GPU acceleration.
2.  **Multi-Query Interactions:** Unlike static tests, agents must complete a sequence of logically connected sub-tasks. For instance, an agent might first load and clean a dataset, then visualize the distribution, and finally perform feature engineering before modeling, mimicking an iterative workflow.
3.  **Multimodal Environment Perception:** Agents are required to interpret not just text and data tables, but also visual observations, such as generated plots and charts. This is crucial for diagnostics; an agent must be able to *see* a training loss curve or a scatter plot to identify non-linear relationships or errors.
4.  **Multi-Dimensional Evaluation:** Performance is assessed holistically across three weighted dimensions: **Reasoning Process** (soundness of methodology), **Code Steps** (correctness and completeness), and the **Final Result** (accuracy and quality of the report).

### Performance Hierarchy and Efficiency Trade-offs

The evaluation results established a clear performance hierarchy. Claude-Sonnet-4.5 emerged as the overall state-of-the-art model, achieving the highest weighted total score (8.164). However, this performance came at a cost.

When efficiency was measured, GPT-5.2 was identified as the most efficient model, achieving top-tier results with minimal average token consumption, suggesting decisive reasoning that avoids redundant self-correction steps. Conversely, the high-performing Claude-Sonnet-4.5 was found to be the least efficient, consuming significantly more tokens per task.

For budget-conscious practitioners, the MiMo-V2-Flash model was flagged as the most cost-effective choice, offering comparable performance to proprietary frontier models at a fraction of the price.

### The Multimodal Advantage

The study also quantified the importance of visual context. By comparing agents with multimodal perception (those that can interpret plots) against text-only baselines, researchers found consistent performance gains across all vision-related tasks, ranging from 2.04% to 11.30%.

"Access to visual outputs such as generated plots enables agents to identify patterns and anomalies that are difficult to infer from text alone," the authors noted, concluding that this feature reduces "hallucinations" and enhances evidence-based interpretations.

Despite these advancements, the benchmark highlighted significant challenges in unstructured data domains. Agents demonstrated robust capabilities in Data Analysis and routine Data Ingestion, but performance scores degraded noticeably in Computer Vision and Natural Language Processing, suggesting current LLMs function more reliably as data analysts than as fully autonomous deep learning engineers capable of complex training and optimization tasks. Future research, the authors suggest, must prioritize enhancing agent capabilities in these complex, unstructured areas.