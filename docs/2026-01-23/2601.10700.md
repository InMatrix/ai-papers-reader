---
layout: paper
pdf_url: https://arxiv.org/pdf/2601.10700
permalink: 2026-01-23/2601.10700/
title: New AI Benchmark, LIBERTY, Reveals Deep Flaws in LLM Explanations
---



In the high-stakes world of AI-driven decisions—from medical diagnosis to job screening—understanding *why* a Large Language Model (LLM) reaches a conclusion is paramount. Yet, evaluating the faithfulness of these explanations has been severely hampered by the lack of reliable ground truth.

Now, researchers have introduced LIBERTY (LLM-based Interventional Benchmark for Explainability with Reference Targets), a novel framework designed to rigorously test how well explanation methods capture the causal influence of high-level concepts like gender, experience, or race on model predictions.

The core innovation of LIBERTY is its use of explicit Structured Causal Models (SCMs) coupled with advanced generative LLMs (specifically GPT-40) to produce "structural counterfactuals." Previously, benchmarks relied on costly, imperfect human annotators to write counterfactuals (e.g., manually changing a job candidate’s race in a resume). LIBERTY treats the LLM as part of the data-generating process (DGP). When researchers intervene on a concept in the SCM, the change propagates causally, forcing the LLM to generate a corresponding counterfactual text that faithfully reflects the intervention.

For example, in the **CV Screening** dataset, the causal graph posits that a candidate's Gender, Race, and Age influence their Education and Work Experience, which in turn influence the predicted Quality label. If a researcher intervenes to change a candidate’s "Work Experience" from 5 years to 20 years, the SCM dictates how all related variables must shift, compelling the LLM to produce a structurally coherent, new personal statement that reflects that profound career change.

The benchmark spans three crucial real-world scenarios: the CV Screening task, **Workplace Violence Prediction** (based on HR interviews), and **Disease Detection** (analyzing clinical self-reports).

In addition to traditional error metrics, LIBERTY introduces a new evaluation standard called *Order-Faithfulness (OF)*, which measures whether an explanation accurately ranks concepts by their true causal importance. Using this benchmark, the team tested eight explanation methods against five major NLP models (including DeBERTa, T5, and GPT-40).

The results were sobering, revealing substantial headroom for improvement. The best-performing methods achieved an average Order-Faithfulness of only 0.74 (where 1.0 is perfect). Surprisingly, matching-based methods—which find similar examples rather than generating new text—outperformed explanations based on LLM-generated counterfactuals. This suggests that while LLMs are good at mimicking human-like textual edits (which worked well on older benchmarks), they struggle to generate text that aligns with the deep causal mechanisms defined by the SCM.

Furthermore, analyzing the *sensitivity* of the LLMs themselves, the researchers found that proprietary models like GPT-40 exhibit markedly reduced sensitivity to demographic concepts (Race, Gender, Age) compared to fine-tuned open-source models. This points strongly to the deliberate mitigation of bias during the proprietary LLMs' post-training alignment processes.

LIBERTY provides a much-needed, scalable, and rigorous foundation for developing the next generation of faithful, theory-grounded explainability methods, moving research beyond simple textual similarity toward true causal understanding.