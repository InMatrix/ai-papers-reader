---
layout: paper
pdf_url: https://arxiv.org/pdf/2601.11387
permalink: 2026-01-23/2601.11387/
title: Study Finds Easily Accessible Evidence is Key to Responsible AI Fact-Checking
---



New research into how humans evaluate claims supported by Artificial Intelligence (AI) suggests that easily accessible primary evidence is the most critical factor in mitigating user overreliance, regardless of the complexity or format of the AI’s explanation.

The study, conducted by researchers at the University of Copenhagen and other institutions, explored how non-expert users navigated fact-checking scenarios assisted by a Large Language Model (LLM). While much previous work has focused on optimizing AI explanations (e.g., providing summaries or articulating uncertainty) to prevent users from blindly following incorrect advice, this experiment demonstrated that providing immediate access to the underlying evidence documents proved a far more powerful tool for effective decision-making.

In a mixed-methods controlled experiment involving 208 crowdworkers, researchers presented claims alongside an AI verdict, a certainty score (like 76%), an explanation, and two evidence documents that participants could easily open and read within the interface. Participants were asked to decide whether to “Use AI Prediction” or “Do more research.”

The findings were striking: participants reported using the raw evidence most frequently—67.8% of the time overall—to validate the AI’s prediction. This reliance on evidence held true across all conditions, whether the AI's advice was correct or incorrect, or whether it provided a standard summary explanation or a detailed explanation of its uncertainty.

This level of engagement contrasts sharply with previous studies where users were provided only external hyperlinks to sources, which few people clicked (typically 10-28%). By embedding the evidence directly within the task interface, the researchers significantly reduced the friction required for users to inspect the facts.

This accessible evidence allows users to catch flaws even when the AI provides a seemingly plausible explanation. For instance, participants were presented with the claim that "A windmill could spin until it falls apart and never generate as much energy as was invested in building it.” The AI predicted "False" with low certainty (31%). By inspecting the evidence, users could see that the claim was based on a severely misrepresented quote, confirming that well-sited windmills *do* generate more energy than invested. The ability to verify the facts directly proved more useful than merely reading the AI’s low certainty score.

While evidence dominated, the natural language explanations still played a valuable secondary role. Participants reported that explanations were most useful when they helped expose "errors in the AI's logic," particularly if the explanation referenced evidence that seemed to contradict the final verdict.

The study also revealed an interesting dynamic related to subjective trust: participants who reported higher overall trust in the AI system were significantly more likely to rely on the AI's explanations and less likely to inspect the underlying evidence.

The results strongly suggest that for AI-assisted fact-checking and decision-support systems, design should prioritize making the raw evidence frictionlessly available, enabling users to maintain oversight and critically assess the AI’s reliability on a claim-by-claim basis.