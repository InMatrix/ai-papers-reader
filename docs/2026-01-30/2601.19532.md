---
layout: paper
pdf_url: https://arxiv.org/pdf/2601.19532
permalink: 2026-01-30/2601.19532/
title: Top LLMs Are Too Smart for Their Benchmarks, Study Finds
---



A new analysis of a leading mathematical reasoning benchmark reveals that the evaluation systems used to grade cutting-edge Large Language Models (LLMs) are fundamentally flawed. Researchers from Vrije Universiteit Brussel and Harvard found that as LLM capabilities approach saturation on challenging benchmarks, the limiting factor is no longer the model, but the incompetence of the LLM "judges" and errors embedded in the test data itself.

The findings, published in a paper titled "Benchmarks Saturate When the Model Gets Smarter Than The Judge," argue that evaluations must now be viewed as a "triplet" consisting of the dataset, the model, and the judge.

To test this interaction, the researchers meticulously audited and cleaned Omni-MATH, a challenging Olympiad-level math dataset. This resulted in the creation of Omni-MATH-2, where 14.6% of the original problems required editing to ensure LaTeX compilability, solvability, or verifiability. This included adding missing figures, clarifying ambiguous statements, and removing unsolvable clutter.

Beyond cleaning the data, the team measured "judge-induced noise" by comparing the evaluations of five state-of-the-art LLMs (including GPT-5, Gemini 3 Pro, and Claude Sonnet 4.5) using two different LLM judges: the original Omni-Judge and the more sophisticated GPT-5 mini.

The results were stark: the choice of judge significantly altered both the absolute accuracy scores and the relative rankings of the models. Gemini 3 Pro, for instance, jumped substantially in performance when graded by GPT-5 mini instead of Omni-Judge.

A targeted human audit of the disagreements confirmed the severity of the judge problem. On the cleaned exact-answer subset, the Omni-Judge was found to be mathematically incorrect in a staggering **96.4%** of the sampled disagreements. The primary failure mode was the judge’s inability to assess mathematical equivalence.

For example, when solving a geometry problem about a conical flask, a model might provide the correct answer as a fraction, $\frac{3+\sqrt{93}}{6}$. The original reference answer might be expressed in an algebraically equivalent but visually different form, $\frac{1}{2} + \frac{\sqrt{93}}{6}$. The older Omni-Judge often failed to recognize that these two forms were equal, marking the model’s correct answer as "Incorrect."

The study also highlighted a deeper flaw in handling "non-standard" problems—questions requiring a proof, an estimation, or referencing a missing image. In one case, a model faced a problem relying on an unattached figure. The model correctly identified that there was insufficient information and asked for the missing diagram. Both LLM judges, however, marked this reasoned abstention as "Incorrect," revealing they lack the ability to check if the problem itself is solvable.

Furthermore, judge disagreement was found to increase significantly with problem difficulty, meaning that the hardest questions are precisely where evaluation noise is highest.

The authors conclude that robust benchmarking requires evaluation designs that move beyond binary pass/fail outcomes, suggesting multi-judge frameworks and systems that allow for partial credit, uncertainty reporting, and explicit abstention. Without maintaining an "evaluator margin" over the models being tested, benchmarks will continue to provide unreliable and often misleading measures of true AI capability.