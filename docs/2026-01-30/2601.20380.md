---
layout: paper
pdf_url: https://arxiv.org/pdf/2601.20380
permalink: 2026-01-30/2601.20380/
title: OmegaUse Agent Achieves Record Performance in Autonomous Cross-Platform Task
  Execution
---



A new general-purpose GUI (Graphical User Interface) agent, OmegaUse, has set a new state-of-the-art benchmark for autonomously executing complex tasks across both desktop and mobile operating systems. Developed by researchers at Baidu, OmegaUse demonstrates the ability to perceive screen states, reason about high-level user goals, and execute a sequence of actions—such as clicking, typing, and scrolling—with high precision across diverse digital environments.

GUI agents have struggled to generalize reliably between vastly different interfaces, like Android apps and Windows desktop software. OmegaUse addresses this bottleneck through a unique combination of architecture and training innovations.

The agent is built on a parameter-efficient Mixture-of-Experts (MoE) backbone. This allows OmegaUse to retain the powerful reasoning capabilities typically found in massive models while reducing the computational overhead. This efficiency is critical for real-time operation in diverse computer-use and phone-use scenarios.

To ensure robust performance, the team engineered a sophisticated data construction pipeline. This pipeline goes beyond standard open-source datasets by integrating autonomous exploration (allowing the agent to discover interactions bottom-up) and taxonomy-guided generation (using expert knowledge to synthesize trajectories for specific, sophisticated tasks top-down).

This high-quality data feeds a decoupled two-stage training paradigm. First, Supervised Fine-Tuning (SFT) establishes the foundational grammar for interaction, teaching the model the basic syntax of actions like `click(x, y)`. Second, the model undergoes reinforcement learning using Group Relative Policy Optimization (GRPO) coupled with specialized rewards. For instance, an **Inside-of-Bounding-Box Reward** incentivizes the agent to predict a click point that falls precisely within the target UI element, rather than just near the boundary, guaranteeing spatial accuracy.

OmegaUse operates using a unified action space that harmonizes interaction primitives across terminals. This means the model can conceptually execute a high-level instruction, whether it translates into a mobile-specific action like `LongPress()` or a desktop command like `Hotkey(key=['ctrl', 'c'])`. This capability is essential for managing complex workflows that span multiple windows or devices. For example, an instruction like "Turn off Bluetooth" might require navigation (moving through menus) and grounding (precisely locating the toggle switch) regardless of whether the agent is on an Ubuntu desktop or an Android device.

The agent’s performance validated the new approach, achieving a state-of-the-art (SOTA) score of 96.3% on the ScreenSpot-V2 grounding benchmark and a leading 79.1% step success rate on the high-level AndroidControl navigation task.

Furthermore, the researchers introduced **OS-Nav**, a new specialized benchmark suite to rigorously test generalization in demanding environments, including **ChiM-Nav** (Chinese Android mobile applications) and **Ubu-Nav** (routine Ubuntu desktop interactions). OmegaUse excelled here too, demonstrating a 74.24% step success rate on ChiM-Nav and a strong 55.9% average success rate on Ubu-Nav, surpassing all existing open-source baselines in these challenging, localized ecosystems.