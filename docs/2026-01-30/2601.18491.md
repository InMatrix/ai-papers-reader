---
layout: paper
pdf_url: https://arxiv.org/pdf/2601.18491
permalink: 2026-01-30/2601.18491/
title: AgentDoG&#58; New AI Guardrail Offers Unprecedented Diagnostic Transparency
  for Autonomous Agents
---



Autonomous AI agents, capable of complex planning and tool use, are quickly becoming ubiquitous, yet the mechanisms meant to ensure their safety have struggled to keep pace. Traditional guardrails often provide only a simple "safe" or "unsafe" binary verdict, failing to diagnose *why* a complex, multi-step agent trajectory failed.

Researchers at the Shanghai Artificial Intelligence Laboratory have introduced AgentDoG, a Diagnostic Guardrail framework designed to provide fine-grained, transparent safety monitoring for these advanced AI systems. AgentDoG’s core innovation is a hierarchical, three-dimensional safety taxonomy and a novel Explainable AI (XAI) module that traces dangerous actions back to their root causes within the agent's decision-making process.

### The Three Dimensions of Agent Risk

To move beyond blunt safety labels, the AgentDoG framework organizes every potential agent failure along three orthogonal dimensions: Risk Source, Failure Mode, and Real-world Harm.

1.  **Risk Source (Where):** Where did the problem originate? This could be a malicious user instruction, an unreliable environmental observation, corrupted feedback from an external tool, or an internal logic failure within the model itself.
2.  **Failure Mode (How):** How did the agent exhibit the risk? This categorizes the behavior, such as Flawed Planning, Improper Tool Use (e.g., using incorrect parameters), or Unauthorized Information Disclosure.
3.  **Real-world Harm (What):** What is the eventual consequence? Harms range from financial and physical damage to security breaches, privacy violations, or psychological harm.

This granular classification is vital for real-world auditing. For example, if an AI agent reads a seemingly benign external document that secretly contains a hidden command ("Indirect Prompt Injection"—*Risk Source*), causing the agent to execute unauthorized file deletion ("Unconfirmed or Over-privileged Action"—*Failure Mode*), the ultimate consequence is a "Security & System Integrity Harm" (*Real-world Harm*). AgentDoG can label all three components, offering actionable feedback instead of just reporting a vague "unsafe" status.

### Explaining the "Why" Behind the Failure

The diagnostic power of AgentDoG is enhanced by its XAI attribution framework, which pinpoints the exact internal drivers of a decision. This is crucial for identifying subtle cognitive failures that might result in harm, even when the agent is trying to follow instructions.

In one test case, a financial analysis agent was asked to gauge market sentiment based on user feedback. It encountered a quote describing a new pricing model as a "Fantastic update" but noted that customers were "Paying more for fewer features." The agent failed to detect the evident sarcasm, misinterpreted the keywords "Fantastic update" as genuine praise, and recommended an unwise financial action (going long on the stock).

AgentDoG’s XAI module successfully traced this failure not just to the tool output step, but to the specific positive keywords within the text. By highlighting these shallow triggers and showing that the agent missed the deeper, sarcastic context, the diagnosis illuminates a vulnerability in the model's semantic reasoning, guiding developers toward specific alignment interventions.

Trained using a unique, taxonomy-guided data synthesis pipeline spanning over 10,000 distinct tools, AgentDoG significantly outperforms existing state-of-the-art guard models. Evaluation across standardized benchmarks, including the new trajectory-level ATBench, demonstrates superior accuracy in trajectory safety classification. More importantly, AgentDoG achieved an accuracy of 82.0% in identifying the precise Risk Source, proving the efficacy of its fine-grained, diagnostic approach—a substantial leap toward verifiable and accountable deployment of autonomous AI. All AgentDoG models and datasets are being openly released to facilitate further safety research.