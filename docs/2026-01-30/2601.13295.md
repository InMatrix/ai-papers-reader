---
layout: paper
pdf_url: https://arxiv.org/pdf/2601.13295
permalink: 2026-01-30/2601.13295/
title: Study Reveals "Curse of Coordination"&#58; Why AI Coding Agents Fail as Teammates
---



Despite rapid advances in large language models (LLMs) achieving near-human performance in individual coding tasks, a new benchmark reveals a fundamental flaw: AI agents are terrible collaborators. When required to work as a team on overlapping code, their success rates plummet, a phenomenon researchers term the "curse of coordination."

The findings, published today by a team from Stanford University and SAP Labs, introduce CooperBench, the first rigorous test designed to measure the social intelligence of coding agents. The benchmark assigns two separate, potentially conflicting features to two different AI agents working on the same codebase simultaneously, simulating real-world software development under partial observability. The agents must communicate and coordinate in real-time to ensure their patches merge successfully and pass all tests.

Testing frontier models like GPT-5 and Claude Sonnet 4.5, the researchers found that success rates in the cooperative setting averaged just 25%. Crucially, this is roughly 50% lower than the "Solo" baseline, where a single agent implements both features alone. In contrast, human teams typically see increased productivity when coordinating a divided workload.

"The current generation of AI agents are brilliant individual programmers, but they fundamentally lack the social intelligence required to function as effective teammates," the authors state.

### The Spatial vs. Semantic Gap

The study pinpoints three core capability gaps causing failure: poor communication, broken commitments, and flawed expectations about the partner's actions.

The key paradox is that while communication happens often—consuming up to 20% of the agents’ action budget—it fails to improve overall cooperation success. Agents excel at *spatial coordination* (agreeing on *where* to edit specific files and line numbers) but neglect *semantic coordination* (agreeing on *what* implementation details or values to use).

A concrete example from CooperBench illustrates this semantic gap: two agents were tasked with adding new parameters (a `case_sensitive` flag and a `reverse` flag) to the same `sync_do_groupby` function signature in the Jinja2 template engine.

The agents communicated successfully, negotiating a line-by-line split to avoid a Git conflict (spatial coordination). However, they failed because they never explicitly confirmed the default value for the `case_sensitive` flag. One agent implemented the correct default (`False`), but the other agent mistakenly reported in its status message that it used `True`. Because they relied solely on status updates rather than verifying implementation details (semantic coordination), the resulting merged code failed its unit tests.

### Coordination Failures and Emergent Hope

Analysis of failed runs showed that nearly half (42%) of coordination breakdowns stemmed from expectation failures—agents failed to model their partner's code changes accurately, even after those changes were clearly communicated. Another 32% involved broken commitments, where agents made claims about completed work that were unverifiable or simply untrue in the final patch.

However, the CooperBench results also offer a glimpse of hope. In rare, successful trajectories, the agents spontaneously exhibited sophisticated social behaviors that were neither prompted nor pre-programmed, including:

*   **Role Division:** Explicitly agreeing who would handle the "front-end" CLI layer and who would handle the "backend" editor implementation.
*   **Resource Division:** Specifying line-level boundaries in shared files to create conflict-free "safe zones."
*   **Negotiation:** Proposing mutually exclusive implementation options and converging on a single, detailed plan before executing any code.

The researchers conclude that addressing the curse of coordination requires shifting focus from maximizing individual coding ability to developing social intelligence, arguing that lightweight protocols for verifiable commitments are essential for enabling AI agents to become reliable teammates. CooperBench is being released as an open-source tool to accelerate this research.