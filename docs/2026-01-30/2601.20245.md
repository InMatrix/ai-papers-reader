---
layout: paper
pdf_url: https://arxiv.org/pdf/2601.20245
permalink: 2026-01-30/2601.20245/
title: Study&#58; AI Assistance Hinders Essential Debugging and Conceptual Skills
  in Novice Programmers
---



AI coding assistants, while providing a clear productivity boost in many professional tasks, come at a significant cost when novice technical workers are trying to acquire new skills, according to a new study from Judy Hanwen Shen and Alex Tamkin.

The research, detailed in a recent paper, found that reliance on AI assistance significantly impaired skill formation—specifically conceptual understanding, code reading, and debugging abilities—among software developers learning a new Python library. Crucially, the AI assistance did not provide a statistically significant average gain in speed, debunking the idea that AI serves as a straightforward shortcut to productivity when new knowledge is required.

To test this trade-off, the researchers conducted a randomized controlled trial involving 52 professional and freelance programmers. Participants were tasked with solving problems using *Trio*, a structured asynchronous Python library with unfamiliar concepts like "nurseries" and coroutines. The treatment group had access to an AI chat assistant (GPT-4o), while the control group relied only on documentation and web search.

The quantitative results were stark: the AI group scored 17% lower on a post-task knowledge quiz. While the AI assistant was capable of generating the complete and correct code, the average completion time across the entire AI group was not significantly faster than the non-AI group ($p=0.391$).

A deep qualitative analysis revealed why. For participants in the control group, the process of encountering and independently resolving errors—such as syntax errors or core Trio-specific issues like a *RuntimeWarning* when a coroutine was not awaited—was crucial for learning. These painful, high-cognitive-load moments cemented their understanding.

The AI users, however, skipped this essential learning phase. The study categorized their behavior into six patterns, finding a stark divide between high-scoring and low-scoring participants.

The lowest scorers often adopted "AI Delegation," asking the assistant to generate the entire solution and simply pasting the code, completing the task quickly but scoring poorly on the quiz (24%–39% quiz score).

The high scorers, conversely, demonstrated intense cognitive engagement. These high-scoring patterns included "Conceptual Inquiry" (only asking the AI conceptual questions, such as "Can you remind me what the different trio async operations are?”) or "Generation-Then-Comprehension" (generating code, then asking follow-up questions to check their understanding). These successful users leveraged AI to clarify concepts rather than delegating the task's completion.

The findings carry significant implications for the future of technical work, particularly in safety-critical domains where humans must ultimately supervise AI-generated outputs. The authors suggest that if developers are routinely inhibited from acquiring skills like debugging and conceptual mastery due to heavy AI reliance, they may not possess the competency needed to validate and oversee the increasingly complex code produced by generative models.

The researchers advise organizations to carefully integrate AI assistance into learning workflows, prioritizing interaction patterns that ensure cognitive effort and independent thinking, rather than merely optimizing for the fastest task completion time.