---
layout: paper
pdf_url: https://arxiv.org/pdf/2601.20757
permalink: 2026-01-30/2601.20757/
title: Persona Prompts Boost LLM Classification, But Undermine Core Reasoning
---



**Large Language Models (LLMs) tasked with sensitive jobs like detecting hate speech perform better when given specific demographic personas, but this classification boost comes at the cost of the quality and reliability of their explanations, according to new research.**

A study by researchers at Technische Universität Berlin and BIFOLD investigated the impact of "Persona Prompting" (PP)—instructing an LLM to respond as a specific individual, defined by attributes like age, political view, or education—on model performance and internal social reasoning. Evaluating three major LLMs (GPT-OSS-120B, Mistral-Medium, and Qwen3-32B) across tasks ranging from highly subjective hate speech detection (HateXplain) to standard sentiment analysis (SST-2), the team uncovered a critical trade-off in the application of personalized AI.

The core finding reveals that while persona prompts can improve classification accuracy on the most subjective task, they generally fail to improve "rationale quality"—the degree to which the LLM’s token-level explanations align with human annotations. On the HateXplain dataset, for instance, Mistral-Medium showed improved hate speech detection rates for many personas, yet rationale quality scores often degraded or showed no significant gain across all models.

### Models Resist Deeper Steering

The research confirms that LLMs hold deep-seated biases and exhibit a strong resistance to fundamental changes in reasoning, despite the prompt's explicit instruction to adopt a new perspective.

To test how reasoning changes, researchers exposed models to the controversial input: *"antifa are the real nazis man."* When prompted as a "Right-wing" individual, one LLM classified the comment as "Normal" speech, providing a rationale that framed it as harmless teasing. However, when the *same* model was prompted as a person with "Higher education," it classified the text as "Hate speech," using an analytical approach focused on identifying targets of derogatory stereotypes.

Despite these observable shifts in *style* and *conclusion*, the study found that LLMs failed to convincingly align simulated personas with real-world demographic groups. Inter-persona agreement remained high across most demographic categories, suggesting models are highly resistant to significant steering on subjective tasks.

### The Guardrail Problem

A persistent safety bias was also documented: all evaluated models showed a strong tendency to "over-flag" content as harmful. Across the hate speech detection task, neutral or offensive statements were frequently classified as the most severe category, "Hate speech," regardless of the persona used. This hyper-safety alignment suggests a rigid internal template that persona prompts struggle to override, prioritizing caution even when it reduces overall performance accuracy.

The findings urge caution for developers applying persona prompting to personalized systems. While PP may temporarily enhance label prediction in complex, socially sensitive domains, relying on the model’s generated explanations to understand *why* it made a decision risks amplifying unmitigated stereotypes and biases learned during pretraining. This calls for more rigorous auditing to ensure that personalized LLM outputs are both accurate and ethically sound.