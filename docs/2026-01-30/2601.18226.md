---
layout: paper
pdf_url: https://arxiv.org/pdf/2601.18226
permalink: 2026-01-30/2601.18226/
title: AI Agents Learn to Evolve Their Own Tools, Achieving New Performance Peaks
---



In a significant advance for Artificial General Intelligence (AGI), researchers have introduced an "In-Situ Self-Evolving" framework that allows language model-based agents to autonomously synthesize, refine, and consolidate their own functional toolsets while actively solving tasks.

The new system, dubbed Yunjue Agent, addresses a critical limitation of conventional AI: the reliance on static tool libraries and extensive offline training. In open-ended environments where tasks continuously drift and external supervision is scarce, Yunjue Agent leverages sequential interactions as a continuous stream of experience, enabling capability expansion during live execution.

The core insight of the framework is that **tool evolution** provides the most reliable pathway for growth. While judging the success of an abstract planning step is subjective, determining if a piece of generated code (a tool) executes successfully is objective and binary—it either runs or throws an exception. This robust, intrinsic feedback loop allows the agent to iteratively improve its functional primitives without human intervention.

### Parallel Evolution and Knowledge Crystallization

To efficiently manage the flood of newly synthesized capabilities, the team introduced a **Parallel Batch Evolution** strategy paired with an "absorbing mechanism."

Instead of generating a unique tool for every niche query, the agent processes tasks in batches. Upon completion, the "Aggregator" identifies functionally similar tools and the "Merger" consolidates them into a single, canonical utility for the global repository.

For example, if the agent initially synthesized a tool named `calculate_mean` and another called `compute_average`, the absorbing mechanism would recognize them as semantic duplicates. It would merge their core logic into a more generalized, robust tool, perhaps named `eval_math_expression.py`, reducing redundancy and streamlining the agent's knowledge base. This process ensures that transient, query-specific code is distilled into reusable, high-generality primitives, acting as a form of “knowledge crystallization.”

### State-of-the-Art Performance from Scratch

The Yunjue Agent was evaluated under a stringent **zero-start** setting, meaning it began with an empty toolset across five diverse, complex benchmarks, including expert-level reasoning (HLE), deep web search (DeepSearchQA), and financial investigation (FinSearchComp).

The system achieved state-of-the-art performance across multiple benchmarks, often significantly outperforming static open-source agents and highly capable proprietary models. On the DeepSearchQA benchmark, Yunjue Agent scored 73.5%, a substantial gain of +17.4 points over the strong Gemini 3 Pro baseline.

Crucially, the team demonstrated **cross-domain transferability**. When the agent, initialized with tools learned from the HLE dataset, was tasked with solving problems in a new domain like ScienceQA, the number of *new* tools it needed to synthesize decreased by 100%. This proves that the accumulated knowledge is generalized and not confined to specific domains.

To track this self-improvement, the researchers proposed a metric called **Evolutionary Generality Loss (EGL)**, which tracks the balance between new tool creation and tool reuse. EGL functions analogously to training loss, showing that the system rapidly converges toward a stable state of high generality, where new capability creation yields diminishing returns.

The research suggests a paradigm shift in AI development, moving toward "agentic pre-training," where future agents will begin deployment not with static code, but with a fully converged "foundation toolset" distilled from massive, broad-spectrum interaction data.