---
layout: paper
pdf_url: https://arxiv.org/pdf/2601.20802
permalink: 2026-01-30/2601.20802/
title: LLMs Learn from Their Own Mistakes&#58; New Self-Distillation Method Solves
  Reinforcement Learning Bottleneck
---



A team of researchers from ETH Zurich, MIT, and Stanford has unveiled a novel technique, Self-Distillation Policy Optimization (SDPO), that drastically improves how large language models (LLMs) learn reasoning and coding skills. By leveraging tokenized environmental feedback—like runtime errors or failed unit tests—SDPO allows LLMs to retrospectively teach themselves, converting sparse, binary rewards into a dense, powerful learning signal.

Current methods for training LLMs via reinforcement learning (known as RLVR, Reinforcement Learning with Verifiable Rewards) suffer from a severe "credit assignment bottleneck." When an LLM attempts a complex task, like writing code or solving a scientific problem, it typically receives only a scalar, binary outcome: success (reward +1) or failure (reward 0). If the attempt fails, the model receives no specific information on *where* it went wrong.

The new SDPO algorithm formalizes this richer environment setting as Reinforcement Learning with Rich Feedback (RLRF). Instead of relying on a distant, scalar reward, RLRF utilizes the detailed, tokenized feedback often provided by execution environments.

### The Self-Teacher Mechanism

SDPO introduces a clever mechanism where the policy acts as its own teacher. When an attempt fails, the model is re-prompted with the original question *and* the rich feedback it just received (e.g., a "ZeroDivisionError" on line 73).

The LLM, acting as the "self-teacher," then re-evaluates its original failed answer in this augmented context. This process enables the model to identify precisely which tokens in the original response led to the error. SDPO then distills this corrected, next-token prediction distribution back into the policy, providing dense, logit-level credit assignment without needing an external expert model.

For example, if an LLM writes a Python function that fails due to a logic error, the environment returns a detailed error message. SDPO uses this message to train the model to avoid the mistake at the exact point in the sequence where the error-inducing token was generated.

### Faster Learning, More Concise Reasoning

SDPO demonstrated superior performance across challenging reasoning benchmarks, including scientific Q&A and competitive programming problems on LiveCodeBench v6 (LCBv6).

On LCBv6 coding tasks, SDPO significantly outperformed Group Relative Policy Optimization (GRPO), a strong baseline, achieving higher final accuracy (48.8% versus 41.2%) and reaching GRPO's final performance in 4x fewer training steps.

Qualitatively, SDPO also encouraged more efficient reasoning. In scientific reasoning tasks, baseline LLMs often generated verbose, superficial responses, sometimes getting stuck in circular "Hmm" or "Wait" loops (Figure 7). SDPO-trained models, benefiting from dense credit assignment, produced reasoning paths that were up to seven times shorter while maintaining or exceeding accuracy.

Crucially, the efficacy of SDPO appears to be an emergent property that scales with model strength. The algorithm's gains grew substantially on larger models, suggesting that the ability for accurate "retrospection"—retrospectively identifying one's own mistakes—improves as the base model becomes a stronger in-context learner.

### Accelerating Discovery of Hard Solutions

Perhaps the most dramatic application is SDPO’s ability to accelerate the discovery of solutions for extremely difficult, binary-reward problems during test time.

In standard RLVR, learning on hard tasks is impossible until the model randomly stumbles upon the first successful solution. SDPO avoids this limitation by learning from rich feedback even on failed attempts.

In experiments on "very hard" LCBv6 questions (where the base model had less than a 3% chance of solving the problem in 64 attempts), SDPO accelerated the probability of finding the first solution by approximately 3x compared to traditional best-of-k sampling or multi-turn conversational methods. This test-time self-distillation enables the model to iteratively compress the rich error history of failed attempts into its weights, effectively "fixing" its policy until a solution is found.