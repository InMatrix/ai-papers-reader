---
layout: paper
pdf_url: https://arxiv.org/pdf/2412.01250
permalink: 2024-12-06/2412.01250/
title: Collaborative Instance Navigation&#58; AI Agents That Ask for Help
---



A new research paper from Taioli et al. introduces "Collaborative Instance Navigation" (CoIN), a paradigm shift in how AI agents perform object-finding tasks within complex environments.  Unlike existing methods that rely on comprehensive user instructions upfront, CoIN allows agents to dynamically engage with human users via natural language dialogue, minimizing user input.  This is a crucial step towards building more robust and practical AI systems for real-world applications.

The key innovation lies in the AIUTA (Agent-user Interaction with Uncertainty Awareness) framework. AIUTA comprises two crucial modules: the Self-Questioner and the Interaction Trigger.  

**The Self-Questioner**, a self-dialogue mechanism using Large Language Models (LLMs) and Vision Language Models (VLMs), refines the initial description of a potential target object. Imagine the agent initially detects a picture. The VLM might simply describe it as "a picture on a wall." AIUTA's self-questioning process then kicks in; the LLM uses the initial description to generate more specific questions for the VLMâ€”e.g., "Is the picture black and white?", "Is there a person in the picture?", "Is the person wearing a shirt?". The answers, along with an uncertainty estimation technique that filters out inaccurate VLM responses, create a much richer, accurate, and complete description. This addresses a common limitation of VLMs: generating descriptions that lack crucial details or even hallucinate irrelevant information.

**The Interaction Trigger** determines when to seek user assistance.  Based on the refined object description and what the user has already told the agent, AIUTA decides whether to continue navigating, stop (because the target has likely been found), or ask the human user a question to clarify ambiguities. The questions are template-free and open-ended, allowing for flexible and natural dialogue. For example, if the agent is unsure about the background of the picture, it might ask "Is the background a coastal scene?" rather than sticking to rigid question templates.

To evaluate AIUTA, the researchers created CoIN-Bench, a new benchmark dataset and evaluation protocol.  CoIN-Bench uses simulated and real-world users. The simulated users respond based on the complete visual information of the target, allowing for scalable experiments. The real-world user studies provide a crucial test of the system's flexibility in handling diverse and concise user input. 

AIUTA shows significant advantages over existing methods, especially in scenarios with multiple similar instances. Using only the object category ("find the picture"), it significantly outperforms state-of-the-art methods that rely on detailed initial descriptions.  Importantly, AIUTA maintains high success rates even when interacting with real human users. Its training-free nature and dynamic approach to user interaction provide substantial advantages for practical deployment.

The paper also introduces IDKVQA, a new dataset to evaluate techniques for quantifying VLM uncertainty.  This is crucial because VLMs are prone to inaccuracies.  The results demonstrate that AIUTA's approach to uncertainty estimation is superior to other recently proposed methods.

In summary, this research provides a significant advancement in embodied AI. The CoIN task and AIUTA framework move us closer to creating AI agents that can effectively collaborate with humans in complex, real-world scenarios. The dynamic interaction, minimized user input and training-free nature of AIUTA make it highly promising for future development and deployment.