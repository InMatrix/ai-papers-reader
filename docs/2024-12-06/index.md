---
layout: default
title: 2024-12-06
permalink: /2024-12-06/
---

# 2024-12-06

## Generative AI for Assisting Software Developers

### Generating a Low-code Complete Workflow via Task Decomposition and RAG

**Relevance:** This paper directly addresses the use of generative AI, specifically Retrieval-Augmented Generation (RAG), to assist in software development.  It focuses on workflow generation, a crucial aspect of software engineering. The authors formalize task decomposition and RAG as design patterns for GenAI-based systems, providing valuable insights into how these techniques can improve the development process. The real-world application described showcases the practical implications of LLMs in automating parts of software creation.

ðŸ’¡ **[Summary](2412.00239/)** ðŸ“„ **[Full paper](https://arxiv.org/pdf/2412.00239)**

## AI Agents

### MALT: Improving Reasoning with Multi-Agent LLM Training

**Relevance:** This paper explores multi-agent LLM training (MALT), a significant step towards building more sophisticated AI agents.  The research focuses on enabling collaboration between LLMs with specialized roles (generator, verifier, refiner) to solve complex reasoning problems.  The use of a sequential multi-agent setup and a credit assignment strategy directly relates to the challenges of task decomposition, decision-making, and collaboration within the context of AI agents.

ðŸ’¡ **[Summary](2412.01928/)** ðŸ“„ **[Full paper](https://arxiv.org/pdf/2412.01928)**

### Collaborative Instance Navigation: Leveraging Agent Self-Dialogue to Minimize User Input

**Relevance:** This paper introduces Collaborative Instance Navigation (CoIN), a task requiring AI agents to interact dynamically with users.  The proposed AIUTA method uses Vision-Language Models (VLMs) and Large Language Models (LLMs) for perception, dialogue, and uncertainty resolution. The focus on minimizing user input and handling ambiguous instructions highlights key aspects of designing effective and user-friendly AI agents.

ðŸ’¡ **[Summary](2412.01250/)** ðŸ“„ **[Full paper](https://arxiv.org/pdf/2412.01250)**

## Prompt Engineering Techniques

### Motion Prompting: Controlling Video Generation with Motion Trajectories

**Relevance:** This paper introduces "motion prompting," a novel prompt engineering technique for video generation.  Instead of relying solely on text descriptions, the model is conditioned on motion trajectories, offering a more fine-grained and intuitive way to control the generated video's dynamics. The exploration of both sparse and dense trajectories and the concept of "motion prompt expansion" are valuable contributions to prompt engineering for multimodal generation.

ðŸ’¡ **[Summary](2412.02700/)** ðŸ“„ **[Full paper](https://arxiv.org/pdf/2412.02700)**

### Exploring the Abilities of Large Language Models to Solve Proportional Analogies via Knowledge-Enhanced Prompting

**Relevance:** This paper investigates the effectiveness of knowledge-enhanced prompting in improving LLMs' ability to solve proportional analogies.  By augmenting prompts with different types of knowledge (exemplar, structured, targeted), the researchers explore how prompt engineering can enhance the model's reasoning capabilities.  The findings on the impact of different knowledge types provide valuable insights for designing more effective prompts for complex reasoning tasks.

ðŸ’¡ **[Summary](2412.00869/)** ðŸ“„ **[Full paper](https://arxiv.org/pdf/2412.00869)**

## Human-in-the-loop Machine Learning

### Weighted-Reward Preference Optimization for Implicit Model Fusion

**Relevance:** This paper presents WRPO, a human-in-the-loop method for fusing heterogeneous LLMs. While not directly using human labels in the traditional sense, it leverages human preferences (implicitly) through preference optimization between source and target LLMs. This approach implicitly incorporates human judgment to guide the model fusion process, aligning with the principles of human-in-the-loop learning.

ðŸ’¡ **[Summary](2412.03187/)** ðŸ“„ **[Full paper](https://arxiv.org/pdf/2412.03187)**

### Free Process Rewards without Process Labels

**Relevance:** This paper tackles the challenge of training process reward models (PRMs) without requiring expensive step-by-step human annotations. It proposes an implicit PRM approach that leverages readily available response-level labels, effectively reducing the need for extensive human involvement while still capturing the nuances of the reasoning process. This method indirectly incorporates human feedback by relying on human-evaluated outcomes.

ðŸ’¡ **[Summary](2412.01981/)** ðŸ“„ **[Full paper](https://arxiv.org/pdf/2412.01981)**

## Techniques for Explaining AI Behavior

No paper recommendations for this topic.

