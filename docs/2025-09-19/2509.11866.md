---
layout: paper
pdf_url: https://arxiv.org/pdf/2509.11866
permalink: 2025-09-19/2509.11866/
title: Dr.V&#58; A Novel Framework to Diagnose and Mitigate Video Hallucinations
---



Large video models (LVMs) have made remarkable strides in understanding video content. However, these models often "hallucinate," generating information that contradicts the actual video. To combat this, researchers have introduced **Dr.V**, a hierarchical framework designed to diagnose and address video hallucinations. Dr.V operates on a three-tiered system: perceptive, temporal, and cognitive levels, mirroring how humans process visual information.

The core of Dr.V lies in its fine-grained spatial-temporal grounding, which allows it to meticulously analyze specific objects, their movements, and the sequence of events within a video. This approach is embodied in two key components: **Dr.V-Bench**, a comprehensive dataset, and **Dr.V-Agent**, a diagnostic system.

**Dr.V-Bench** is a substantial benchmark dataset featuring 10,000 instances drawn from 4,974 videos. It covers a diverse range of tasks and scenarios, providing detailed spatial-temporal annotations. This dataset is crucial for training and evaluating LVMs' ability to avoid hallucinations. For instance, Dr.V-Bench includes questions about object presence ("How many ducks are involved in the video?"), dynamic relations ("Did the person throw away the book after putting down the cup?"), and context-based explanations ("Identify the information that is consistent with the video and generate a video caption based on the selected option.").

The **Dr.V-Agent** system systematically diagnoses hallucinations by breaking down the analysis into several steps. It first classifies the type of hallucination, then checks for errors at the perceptive level (e.g., incorrect object recognition or localization), followed by temporal level checks (e.g., misinterpreting action sequences or dynamic attributes), and finally, cognitive level reasoning (e.g., flawed contextual explanations or counterfactual predictions). This step-by-step process aims to mimic human-like video comprehension, making the diagnosis of hallucinations more accurate and interpretable.

Experiments demonstrate that Dr.V-Agent significantly improves the reliability of LVMs. By identifying precisely where and why a model hallucinates, Dr.V-Agent offers a practical solution for building more robust and trustworthy video understanding systems for real-world applications. The framework's ability to leverage external tools and its training-free nature further enhance its practicality and efficiency.