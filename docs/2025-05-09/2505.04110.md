---
layout: paper
pdf_url: https://arxiv.org/pdf/2505.04110
permalink: 2025-05-09/2505.04110/
title: AI Tackles Excel&#58; New Benchmark Tests Large Language Models on Business
  Challenges
---



Large Language Models (LLMs) are making waves in various fields, but how well do they perform on practical, business-oriented tasks? A new benchmark, dubbed "Alpha Excel," aims to answer this question by evaluating LLMs on challenges derived from the Financial Modeling World Cup (FMWC), a global competition for Excel experts.

The Alpha Excel benchmark, detailed in a recent paper, converts 113 existing FMWC challenges into programmatically evaluable JSON formats. These challenges span diverse categories, including financial calculations, pattern recognition, game simulations, and data processing.  This differs from many existing AI benchmarks that focus on abstract reasoning or programming tasks.

"We wanted to bridge the gap between academic AI benchmarks and practical business applications," explains David Noever, one of the paper's authors. "Microsoft Excel is a ubiquitous tool in the business world, and the FMWC challenges represent real-world problem-solving scenarios."

Imagine a typical FMWC challenge: participants might be given a dataset of sales figures and tasked with creating a dynamic financial model to forecast future revenue, or even creating a simulation of a fantasy game. The benchmark reformats these challenges into question-answer pairs that LLMs can process. The goal is to assess the LLM's ability to understand the problem, apply relevant rules, and perform accurate calculations.

The researchers tested several leading LLMs, including OpenAI's GPT-4o-mini, Mistral, and Alibaba's Qwen2.5. The results revealed significant performance variations across different challenge categories. Models exhibited strengths in pattern recognition but struggled with complex numerical reasoning. For instance, a model might correctly identify a trend in a dataset but fail to accurately calculate a percentage change. Game simulation challenges, requiring complex rule interpretations and multi-step reasoning, proved especially difficult.

â€œFinancial modeling tasks showed the most consistent performance across models, possibly due to the more structured nature of these problems and the prevalence of similar examples in model training data," the authors stated.

The study also identified common error types, including calculation mistakes, rule application errors, and context misunderstanding. Calculation errors often occurred in challenges requiring extended calculation chains, where small errors propagated through subsequent steps, leading to significantly divergent final answers.

While current LLMs still lag behind top human Excel competitors, the Alpha Excel benchmark provides valuable insights into their capabilities and limitations in business contexts.

The benchmark is not without its limitations. The conversion of spreadsheet challenges into text-based formats sacrifices some visual and spatial aspects of the original tasks. The researchers acknowledge this and plan to incorporate more interactive evaluation methods in future work.

Despite these limitations, the Alpha Excel benchmark represents a step towards more application-oriented AI evaluation. The authors suggest that future LLM development should focus on improving numerical reasoning capabilities, state tracking, and rule synthesis. These improvements could lead to AI systems that effectively assist humans with business analysis and decision support. Ultimately, the goal is not to replace human Excel experts, but to augment their abilities with AI tools that handle routine tasks and free them to focus on higher-level interpretation and strategic thinking.