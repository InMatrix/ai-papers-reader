---
layout: default
title: 2024-10-25
permalink: /2024-10-25/
---

# 2024-10-25

## Generative AI for Assisting Software Developers

### LLM-based Optimization of Compound AI Systems: A Survey

**Relevance:** This paper presents a comprehensive overview of LLM-based optimization methods for compound AI systems, which often involve code generation and execution. The survey highlights how LLMs can be used to optimize the parameters and behavior of AI systems, improving their efficiency and effectiveness in various software development tasks.

ðŸ’¡ **[Summary](2410.16392/)** ðŸ“„ **[Full paper](https://arxiv.org/pdf/2410.16392)**

## AI Agents

### Agent-to-Sim: Learning Interactive Behavior Models from Casual Longitudinal Videos

**Relevance:** This paper focuses on learning interactive behavior models of 3D agents from casual longitudinal videos. This research is relevant to AI agents as it addresses the challenge of learning from real-world data and transferring this knowledge to simulations, allowing for the development of more realistic and adaptable AI agents.

ðŸ’¡ **[Summary](2410.16259/)** ðŸ“„ **[Full paper](https://arxiv.org/pdf/2410.16259)**

### Steering Your Generalists: Improving Robotic Foundation Models via Value Guidance

**Relevance:** This paper presents a method for improving the performance of generalist robotic policies by re-ranking their actions according to a value function learned via offline RL. This research aligns with the development of AI agents that can reason, plan, and execute actions in complex environments.

ðŸ’¡ **[Summary](2410.13816/)** ðŸ“„ **[Full paper](https://arxiv.org/pdf/2410.13816)**

## Prompt Engineering Techniques

### TP-Eval: Tap Multimodal LLMs' Potential in Evaluation by Customizing Prompts

**Relevance:** This paper explores the impact of prompt customization on the performance of multimodal LLMs, emphasizing the need for prompt engineering techniques to effectively evaluate and leverage their capabilities. It demonstrates how prompt variations can significantly affect the model's performance and explores methods for designing effective prompts for specific tasks.

ðŸ’¡ **[Summary](2410.18071/)** ðŸ“„ **[Full paper](https://arxiv.org/pdf/2410.18071)**

### Improve Vision Language Model Chain-of-thought Reasoning

**Relevance:** This paper focuses on improving the chain-of-thought (CoT) reasoning capabilities of vision language models by utilizing prompt engineering techniques. It introduces methods for enriching training data with rationales and applying reinforcement learning to refine the model's reasoning abilities, leading to more interpretable and trustworthy AI models.

ðŸ’¡ **[Summary](2410.16198/)** ðŸ“„ **[Full paper](https://arxiv.org/pdf/2410.16198)**

## Human-in-the-loop Machine Learning

### Aligning Large Language Models via Self-Steering Optimization

**Relevance:** This paper proposes a method for automated alignment of LLMs that utilizes self-steering optimization, eliminating the need for manual annotation. It leverages human-like preference signals during training to improve model performance without relying on explicit human feedback, demonstrating a novel approach to human-in-the-loop learning.

ðŸ’¡ **[Summary](2410.17131/)** ðŸ“„ **[Full paper](https://arxiv.org/pdf/2410.17131)**

### MIA-DPO: Multi-Image Augmented Direct Preference Optimization For Large Vision-Language Models

**Relevance:** This paper introduces a new approach for visual preference alignment in large vision-language models that effectively handles multi-image inputs. It utilizes a multi-image augmentation technique to mitigate the scarcity of training data and leverages attention-aware selection to construct chosen/rejected pairs, incorporating human preferences into the learning process.

ðŸ’¡ **[Summary](2410.17637/)** ðŸ“„ **[Full paper](https://arxiv.org/pdf/2410.17637)**

## Techniques for Explaining AI behavior

### Math Neurosurgery: Isolating Language Models' Math Reasoning Abilities Using Only Forward Passes

**Relevance:** This paper introduces a method for isolating math-specific parameters in LLMs using only forward passes, allowing for the identification and manipulation of these parameters to understand how LLMs encode mathematical reasoning. This research contributes to explainable AI by providing insights into the internal workings of LLMs and their specific abilities.

ðŸ’¡ **[Summary](2410.16930/)** ðŸ“„ **[Full paper](https://arxiv.org/pdf/2410.16930)**

