---
layout: default
title: 'Researchers Propose a New Evaluation Framework for Multimodal Large Language Models'
permalink: 2024-10-25/2410.18071.html
---
# Researchers Propose a New Evaluation Framework for Multimodal Large Language Models

Multimodal large language models (MLLMs), which combine language and other modalities like images or videos, are rapidly gaining traction for their impressive capabilities. A key challenge in MLLM research is effectively evaluating their performance. Existing benchmarks often suffer from **prompt sensitivity**, meaning that minor variations in the questions posed to the models can lead to significant changes in their output. This can make it difficult to get a reliable and accurate assessment of a model's capabilities. 

A new paper titled "TP-Eval: TAP MULTIMODAL LLMS' POTENTIAL IN EVALUATION BY CUSTOMIZING PROMPTS" proposes a solution to this problem.  The researchers argue that current benchmarks underestimate the true potential of MLLMs because they use the same prompts for all models, failing to account for the fact that different models may have different preferences. 

To address this, they introduce **TP-Eval**, a novel evaluation framework that uses **prompt customization** to tailor the questions to each model. Specifically, TP-Eval employs **automatic prompt optimization** to generate customized prompts that are more likely to elicit the best possible performance from a given model. The paper's authors demonstrate that TP-Eval can significantly improve the accuracy of MLLM evaluation and, in some cases, uncover hidden capabilities that are not readily apparent using traditional benchmarks.

For example, the paper highlights the case of a task in the MMT-Bench benchmark that aims to detect if a person in a picture is wearing a helmet.  When the authors used the original prompt, "Is the person in the picture wearing a helmet?", the LLaVA-1.5-7B model answered "Yes" to all 180 questions in the test set, resulting in extremely low accuracy. However, by slightly rephrasing the question to, "Evaluate if the individual in the picture wearing adequate headgear that provides safety and visibility to minimize interpretation ambiguity.", they were able to significantly improve the model's accuracy. This suggests that the original prompt was not effectively eliciting the model's full capabilities. 

The paper also explores the use of **introspection** during prompt optimization. This involves using the model's responses to identify why a prompt may not be performing well.  The authors incorporate this information into the prompt optimization process, further improving its effectiveness.

The researchers conducted experiments on various MLLMs and benchmarks, demonstrating the effectiveness of their approach.  They found that prompt customization can lead to substantial improvements in model performance, particularly in tasks that require more complex reasoning or fine-grained analysis of multimodal input. 

This research makes a significant contribution to the field of MLLM evaluation by providing a more comprehensive and reliable way to assess model capabilities.  TP-Eval's ability to tap the full potential of MLLMs could have a significant impact on future research and development in this rapidly growing area. 
