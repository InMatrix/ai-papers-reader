---
layout: default
title: 'Vision Language Models Can Now Reason Like Humans'
permalink: 2024-10-25/2410.16198.html
---
# Vision Language Models Can Now Reason Like Humans

Recent breakthroughs in artificial intelligence (AI) have led to the development of powerful vision language models (VLMs) capable of understanding and reasoning about images. However, current VLMs struggle to generalize to complex tasks that require detailed reasoning. A new paper published on arXiv proposes a two-fold approach to improve the reasoning capabilities of VLMs.

The first step is to enrich the training data by distilling rationales from a large language model (LLM) like GPT-40. This involves providing the LLM with an image, a question, and a short answer. The LLM then generates a step-by-step explanation, or rationale, that logically connects the image, question, and answer. By feeding these rationales to the VLM during training, the researchers were able to improve the model's ability to reason.

For example, consider the question: "How many food items are there in the figure?" A human might reason: "To count the number of food items, I need to enumerate the items on the y-axis. The first item is Lamb, followed by Corn, Barley, Rye, Beefâ€¦ Therefore, the total number is 14."  By training the VLM on a dataset of such rationales, the model learns to generate similar explanations when answering similar questions.

The second step involves using reinforcement learning (RL) to calibrate the reasoning quality. The researchers used the Direct Preference Optimization (DPO) algorithm, which compares pairs of model-generated reasoning chains, one correct and one incorrect. By rewarding correct reasoning chains and penalizing incorrect ones, the DPO algorithm helps the model to refine its reasoning abilities.

The researchers evaluated their approach on a variety of benchmark datasets, including tasks involving chart interpretation, document information localization, and mathematical reasoning.  Their experiments showed significant improvements in chain-of-thought reasoning performance. Importantly, the approach also resulted in better generalization to direct answer prediction tasks, where the model is asked to provide a short answer without generating any rationale.

This work highlights the importance of incorporating detailed rationales into VLM training. By leveraging both distilled CoT data and reinforcement learning, the researchers were able to significantly improve the reasoning capabilities of VLMs. This breakthrough paves the way for more robust and interpretable multimodal models, capable of solving complex real-world problems.