---
layout: default
title: 'Large Language Models Can Now Steer Themselves Toward Human Preferences'
permalink: 2024-10-25/2410.17131.html
---
# Large Language Models Can Now Steer Themselves Toward Human Preferences

Large language models (LLMs) have revolutionized natural language processing, but aligning them with human preferences remains a challenge. Current approaches often rely on costly human annotation, leading to limited scalability. To address this, researchers have proposed various automated alignment methods, but they often struggle with accuracy and learnability. 

A new paper by researchers at the Chinese Academy of Sciences and Alibaba Group proposes a novel algorithm called Self-Steering Optimization (SSO) that autonomously generates high-quality preference signals for LLMs during training.  SSO eliminates the need for manual annotation, enabling more efficient and effective automated alignment.

The key to SSO lies in its ability to generate near-on-policy preference signals.  Previous methods often extracted chosen and rejected responses through different approaches, leading to off-policy signals that were less learnable for the LLM. SSO, however, optimizes models to generate signals that remain on-policy throughout training, while also ensuring a consistent gap between chosen and rejected responses.

To validate the effectiveness of SSO, the researchers conducted experiments on two foundation models, Qwen2 and Llama3.1. SSO led to significant improvements across various objective benchmarks, including performance on mathematical reasoning, factual accuracy, and code generation. Notably, SSO even outperformed baselines using annotated data.

The paper also demonstrated that SSO can benefit the training of reward models, which are essential for aligning LLMs with human preferences. When trained on data generated by SSO, reward models showed significant improvement in performance compared to models trained on human-annotated data or data from other automated alignment methods.

The researchers argue that SSO represents a significant advancement in automated alignment. It provides a scalable and effective way to generate accurate, learnable, and on-policy preference signals, paving the way for more efficient and effective alignment of LLMs with human preferences.

**Concrete Examples:**

* **Example 1:** Imagine you want to train an LLM to write creative stories.  SSO would help generate a set of stories where the LLM was able to accurately follow a set of principles for good storytelling (e.g., engaging characters, a compelling plot).
* **Example 2:** If you want to train an LLM to write factual summaries, SSO would help ensure the LLM generates summaries that are accurate and consistent with reliable sources of information.

The implications of SSO are significant, as it could lead to the development of more robust and efficient LLMs that can better understand and respond to human preferences. This could have wide-ranging implications for various applications, including chatbots, personal assistants, and even the development of artificial general intelligence (AGI).