---
layout: default
title: 'Researchers Have Discovered How to Isolate Math Reasoning in Large Language Models'
permalink: 2024-10-25/2410.16930/
---
# Researchers Have Discovered How to Isolate Math Reasoning in Large Language Models

Large language models (LLMs) are getting better at performing complex tasks like writing poems, translating languages, and even writing code. However, they still struggle with mathematical reasoning. Researchers at the University of Virginia have developed a new technique, called Math Neurosurgery, that helps to identify the specific parameters in LLMs responsible for mathematical reasoning.

The paper, titled "Math Neurosurgery: Isolating Language Models' Math Reasoning Abilities Using Only Forward Passes," argues that math reasoning is a skill that can be isolated within a model. The researchers developed a method, called MathNeuro, to isolate these parameters by first identifying the top parameters that are most important for mathematical tasks. Then, MathNeuro removes the parameters that are also important for general language tasks, leaving only the parameters that are specifically associated with math reasoning.

The researchers demonstrated that MathNeuro effectively isolates math-specific parameters by pruning (removing) them. When they pruned the math-specific parameters from an LLM, the model's ability to perform mathematical tasks was completely destroyed. However, its ability to perform other non-mathematical tasks remained intact.

The research team also found that scaling (increasing) the math-specific parameters by a small factor could significantly improve a model's mathematical reasoning ability. They found that scaling these parameters by a small constant improved a pretrained or instruction-tuned LLM's performance by 4-17% on a standard benchmark for mathematical reasoning, the GSM8K dataset, while leaving non-math behavior unaltered.

The researchers believe their findings are significant because they suggest that math reasoning is a skill that can be isolated and potentially improved. It is now possible to intervene on specific parameters in a model to improve its mathematical reasoning ability without affecting its performance on other tasks.

This research is just the beginning. The researchers plan to continue exploring the capabilities of MathNeuro, including investigating whether it can be used to improve the accuracy of LLMs on specific mathematical problems or tasks. They are also exploring how to develop new methods for training LLMs that are better at mathematical reasoning from the start. This research could have significant implications for developing more powerful and versatile LLMs.
