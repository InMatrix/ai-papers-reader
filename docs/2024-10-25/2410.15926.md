---
layout: default
title: 'Large Vision Language Models are Prone to Hallucination: A New Positional Alignment Method to Mitigate the Issue'
permalink: 2024-10-25/2410.15926.html
---
# Large Vision Language Models are Prone to Hallucination: A New Positional Alignment Method to Mitigate the Issue 

Large Vision Language Models (LVLMs) have revolutionized the field of artificial intelligence. These models can understand and reason about the visual world, and interact with humans through conversations, generating text responses aligned with image inputs. One critical challenge hindering their adoption in real-world applications is **object hallucination**, where LVLMs generate responses that are not factually aligned with the image inputs.

A research team from Nanyang Technological University and MBZUAI has identified a key factor contributing to object hallucination: **Rotary Position Encoding (RoPE)**, a widely used positional dependency modeling design in existing LVLMs. RoPE encodes position information into representations, enhancing the model's ability to understand sequential order of input tokens. However, RoPE introduces **long-term decay**, a phenomenon where information flow from visual tokens to instruction tokens diminishes with increasing relative distance.

In simpler terms, imagine a sentence with words "cat" and "sitting" where the word "cat" is at the beginning and "sitting" at the end. RoPE's long-term decay makes the model struggle to associate the word "cat" with the word "sitting" because they are far apart in the sequence.

The team's pilot studies revealed that LVLMs hallucinate more when relevant visual cues are distant from instruction tokens in the multimodal input sequence. They discovered a similar effect when reversing the sequential order of visual tokens during multimodal alignment.

To tackle this challenge, the team proposes **Concentric Causal Attention (CCA)**, a simple yet effective positional alignment strategy that mitigates the impact of RoPE long-term decay.  CCA reduces relative distance between visual and instruction tokens, enhancing the model's perception capability and alleviating object hallucination.

Instead of processing visual tokens in a linear sequence, CCA adopts a concentric approach. It starts from the periphery of the 2-D image and progresses towards the center, ensuring that visual tokens closer to the center have a stronger interaction with the instruction tokens. CCA also incorporates a **causal mask rectification module** that models 2-D continuous positional dependencies among visual tokens. This ensures that the center visual tokens attend to peripheral ones while maintaining the spatial locality of 2-D data like images.

CCA significantly outperforms existing hallucination mitigation strategies on multiple benchmarks. Their preliminary experiments show that it surpasses the baseline consistently over six multimodal benchmarks, highlighting its potential to improve the general perception capabilities of LVLMs. 

The teamâ€™s research highlights the importance of carefully considering positional alignment strategies in LVLMs. They emphasize the need to mitigate the impact of RoPE's long-term decay to prevent object hallucination and enhance model's perception capability. Their novel approach, CCA, is a significant step towards addressing this crucial challenge and paving the way for more reliable and accurate LVLMs.