---
layout: paper
pdf_url: https://arxiv.org/pdf/2601.03236
permalink: 2026-01-09/2601.03236/
title: AI Agents Finally Get Long-Term Memory with Multi-Graph Architecture MAGMA
---



A team of researchers has unveiled a new memory architecture, MAGMA (Multi-Graph based Agentic Memory Architecture), designed to solve the critical problem of long-horizon reasoning and memory retention in Large Language Models (LLMs). By fundamentally redesigning how AI agents store and retrieve past experiences, MAGMA enables persistent, interpretable, and accurate recall, significantly outperforming existing memory systems.

The primary limitation of current Memory-Augmented Generation (MAG) systems is their reliance on simple, monolithic memory buffers. When an agent interacts for extended periods, temporal ordering, causal connections, and entity details become entangled, making complex retrieval inaccurate—a phenomenon often called the "lost-in-the-middle" problem.

MAGMA addresses this by representing every memory item (event, thought, or action) across four distinct, orthogonal relational graphs: Semantic, Temporal, Causal, and Entity. This approach disentangles memory, allowing the AI to query specific relationships without sifting through irrelevant information.

### Policy-Guided Retrieval for Precision

Crucially, MAGMA shifts retrieval from a passive semantic search to an active, policy-guided graph traversal. When a user asks a question, MAGMA's "Intent-Aware Router" classifies the query (e.g., WHY, WHEN, ENTITY) and prioritizes traversing the most relevant graph backbone via an Adaptive Traversal Policy.

For instance, if an agent is asked, "When did she hike after the road trip?", the system automatically prioritizes the **Temporal Graph**. Instead of relying on fuzzy semantic similarity, it resolves the relative expression "yesterday" against the immutable temporal chain to provide an exact, chronologically grounded answer.

This structured approach is vital for complex logical leaps. In one benchmark test, a baseline LLM was asked, "How many children does Melanie have?" Standard vector search systems failed, only retrieving "two children" from a single photo caption. MAGMA, leveraging its **Entity and Causal Graphs**, identified the "two kids" entity and linked it with a separate, later-mentioned entity, a "son/brother," synthesizing the logical inference that the total must be "at least three."

### Efficiency and Performance Gains

Beyond precision, the architecture is designed for speed and scalability. MAGMA uses a dual-stream memory evolution process. Latency-sensitive event ingestion (the "Fast Path") indexes new events instantly, ensuring the agent remains responsive. Meanwhile, a background "Slow Path" asynchronously infers complex Causal and Entity links, continuously refining the graph structure without delaying the user interaction.

Tested against state-of-the-art systems on the challenging LoCoMo and LongMemEval benchmarks—which simulate long conversations exceeding 100K tokens—MAGMA achieved an overall accuracy score of 0.700, outperforming the next best baseline by a relative margin of over 18%.

Furthermore, while traditional "Full Context" methods require processing over 100,000 tokens per query, MAGMA compresses the interaction history into reasoning-dense subgraphs, retrieving only 0.7k to 4.2k tokens. This efficiency resulted in the lowest query latency among all tested retrieval systems, confirming MAGMA's ability to maintain high fidelity over ultra-long contexts while minimizing computational cost.