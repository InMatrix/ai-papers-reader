---
layout: paper
pdf_url: https://arxiv.org/pdf/2601.03509
permalink: 2026-01-09/2601.03509/
title: AI Agents Learn to Code&#58; New Programmatic Skill Network Avoids Catastrophic
  Forgetting
---



Artificial intelligence agents operating in complex virtual worlds, such as Minecraft or open-ended simulation environments, face a fundamental hurdle: how to continuously acquire new skills without forgetting the old ones. This challenge, known as catastrophic forgetting, often cripples AI performance as task difficulty increases.

A new paper introduces the **Programmatic Skill Network (PSN)**, a novel framework that teaches AI agents to construct, refine, and reorganize an expanding library of executable skills represented as symbolic computer programs. Instead of relying on traditional deep neural networks alone, PSN uses Large Language Models (LLMs) to synthesize skills (like "mine logs" or "craft pickaxe") written in actual code, linking them into a complex, evolving compositional network.

The key insight of PSN is that its learning dynamics exhibit structural parallels to how traditional neural networks are optimized. It structures continuous learning around three mechanisms, transforming programming logic into a self-optimizing system:

**1. Symbolic Backpropagation for Code (REFLECT)**

When a complex, multi-step skill fails, the agent must determine where the fault occurred—a process PSN calls REFLECT. This mechanism analyzes the execution trace of the skill (the sequence of actions and sub-skills invoked) and assigns credit or blame across the skill hierarchy.

This mirrors the backpropagation process in neural networks. For example, if the skill `craftWoodenPickaxe` fails due to insufficient materials, PSN doesn't just blame the high-level craft command. By examining the trace, it realizes the sub-skill responsible for acquiring wood planks failed to account for the planks required to craft intermediate sticks, identifying a subtle "resource miscalculation" error in the sub-skill's logic itself.

**2. Maturity-Aware Stabilization**

To prevent the agent from destroying reliable skills while learning new, uncertain ones, PSN assigns a reliability value to each program. Skills with high reliability are stabilized, meaning they are updated less frequently. New or brittle skills remain highly "plastic" and receptive to repair. This is analogous to "freezing" layers or adjusting learning rates in neural networks—a proven method to manage the stability-plasticity tradeoff in continual learning.

**3. Canonical Structural Refactoring**

As the agent accumulates skills, redundancy is inevitable. PSN constantly looks for opportunities to reorganize its network structure, analogous to neural architecture search (NAS). It actively merges duplicated code and abstracts specialized skills into more general, reusable programs. For instance, if the agent learns separate, specialized skills for `mineOakLogs(num)` and `mineBirchLogs(num)`, the refactoring mechanism will synthesize a single, more powerful skill: `mineLogs(type, num)`, making the skill library compact and promoting broader generalization.

Tested in the open-ended environments of Minecraft (MineDojo) and Crafter, PSN demonstrated robust skill reuse, rapid adaptation, and significantly reduced catastrophic forgetting compared to leading LLM-based agent baselines like Voyager. By embedding classic neural optimization principles directly into a symbolic, programmatic structure, PSN offers a compelling new paradigm for developing AI capable of sustained, complex competence.