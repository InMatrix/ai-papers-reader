---
layout: paper
pdf_url: https://arxiv.org/pdf/2601.03194
permalink: 2026-01-09/2601.03194/
title: Novel AI Framework Offers Explanation for Hate Speech Detection in Under-Resourced
  Languages
---



A new benchmark and framework called X-MuTeST has demonstrated significant breakthroughs in the challenging field of explainable hate speech detection, particularly for languages that lack extensive digital resources, such as Hindi and Telugu. Developed by researchers from institutions including the Indian Institute of Technology Indore and Arizona State University, the framework combines the semantic reasoning power of Large Language Models (LLMs) with traditional attention-based AI to provide robust and transparent classification results.

The growing challenge in multilingual online spaces is that even highly accurate AI classifiers often fail to provide human-aligned explanations for their decisions. This opacity is exacerbated in under-resourced languages where cultural context and nuance can dramatically alter the meaning of a phrase. To tackle this, the research team’s first major contribution is the creation of a new dataset featuring token-level human rationales for over 6,000 Hindi, 4,400 Telugu, and 6,300 English text samples. These rationales—annotations identifying the exact words or phrases justifying a hate speech label—are essential for training reliable, explainable models.

The X-MuTeST (explainable Multilingual hate Speech deTection) framework employs a novel two-stage training approach designed to bridge the gap between human and machine reasoning.

In the first stage, the detection model is trained using a rationale-guided loss function, instructing the model’s internal attention mechanism to focus precisely on the words humans identified as hateful.

The second stage incorporates an innovative LLM-consulted explanation method. This method quantifies the importance of tokens by measuring the "logit drop" when unigrams (single words), bigrams, and trigrams (word pairs and triplets) are removed from the sentence. For instance, if removing a specific three-word phrase causes the model’s hate speech prediction confidence to plummet, that phrase is deemed highly important.

The final explanation delivered by X-MuTeST is the union of this structural importance measure and high-level, contextual reasoning provided by a powerful LLM like LLaMA-3.1. This consultation is key, as LLMs excel at semantic understanding.

Consider the Hindi sentence: "Whoever feels offended, let them comment, dogs are allowed to bark." The team noted that while some LLMs correctly identified the explicit slur "dogs," they often failed to recognize culturally nuanced offensive phrases like those translating to "to feel offended" or "to bark." The X-MuTeST framework, by merging its n-gram importance metrics with LLM output, generates a more complete set of rationales that align better with human judgment.

Across all three tested languages—Hindi, Telugu, and English—X-MuTeST with LLM consultation achieved the highest classification performance. Crucially, it demonstrated superior results on explainability metrics like token-level F1-score (Plausibility) and Sufficiency (Faithfulness), confirming that its internal reasoning process is both more accurate and more aligned with human logic than comparable transformer-based models (like BERT and Muril) and zero-shot LLM methods.

The researchers assert that this integration of human rationales, structural attention guidance, and LLM consultation enhances both model trustworthiness and user understanding, setting a new standard for transparent and effective multilingual hate speech detection.