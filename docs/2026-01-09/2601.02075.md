---
layout: paper
pdf_url: https://arxiv.org/pdf/2601.02075
permalink: 2026-01-09/2601.02075/
title: LLM Agent MDAgent2 Automates Molecular Dynamics, Mastering Complex Simulation
  Code
---



A new large language model (LLM) framework, MDAgent2, promises to revolutionize molecular dynamics (MD) simulations by automating the highly specialized and error-prone task of writing simulation scripts, coupled with robust knowledge retrieval. Developed by researchers at Peking University and collaborating institutions, MDAgent2 is the first end-to-end system capable of both expert-level MD knowledge question-answering (Q&A) and generating executable code for platforms like LAMMPS.

Traditionally, MD simulations—essential for understanding atomic-scale behaviors in materials science—require strong domain expertise to write, debug, and execute input scripts for complex tasks, such as analyzing thermal conductivity or constructing materials crystals. Existing general-purpose LLMs struggle with this, often generating code that fails execution due to specialized syntax, incorrect physical parameters, or lack of domain-specific data.

MDAgent2 tackles these challenges through a specialized three-stage post-training regimen and a closed-loop multi-agent runtime.

The training phase begins by injecting deep domain knowledge via Continual Pre-training (CPT) on proprietary MD textbooks and papers (MD-Knowledge). This is followed by Supervised Fine-Tuning (SFT) on instructional data (MD-InstructQA). Critically, the process culminates in MD-GRPO, a novel Reinforcement Learning (RL) approach that directly uses simulation execution outcomes—not just human evaluations—as reward signals.

The power of this system resides in the deployable multi-agent environment, MDAgent2-RUNTIME. When a user inputs a task, such as simulating the melting of a copper-nickel nanoparticle, the system acts iteratively.

The Code Generator drafts the LAMMPS script. Before execution, specialized tools verify the code's syntax and ensure that required files, like the interatomic potential (`CuNi.eam.alloy`), are available and correctly referenced. The script is then executed in a sandboxed environment by the Code Runner. The core innovation is the Result Evaluator, which uses multi-dimensional criteria (checking for logical consistency, correct parameters, and physical soundness) to score the simulation output.

If the output is poor—for example, the simulation log shows the dreaded "lost atoms" error, or the target temperature diverges—the low score is fed back to the Code Generator. The agent then performs self-correction, revising and regenerating the script until it produces a physically sound and executable result.

Evaluated on MD-EvalBench, the first unified benchmark for LAMMPS Q&A and code generation established in this work, MDAgent2 demonstrates compelling results. When leveraging the RUNTIME self-correction loop, the model's Execution Success rate for generated code surged from 14.23% to 37.95% compared to direct generation. Furthermore, the domain-adapted MD-Instruct model achieved competitive Q&A scores, even surpassing much larger general-purpose LLMs in understanding specialized LAMMPS syntax.

This closed-loop feedback mechanism, which grounds LLM output in actual simulation executability and physics-based outcomes, represents a crucial methodological step toward autonomous AI systems capable of executing complex industrial and scientific simulations without continuous human intervention.