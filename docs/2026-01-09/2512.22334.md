---
layout: paper
pdf_url: https://arxiv.org/pdf/2512.22334
permalink: 2026-01-09/2512.22334/
title: New Open-Source Toolkit Exposes Critical Deficiencies in AI’s Scientific Reasoning
  Abilities
---



**SciEvalKit, a comprehensive benchmarking suite, reveals that even leading large language models excel at factual recall but fail at core scientific tasks like symbolic math and code generation.**

SHANGHAI – Despite the rapid advances in large language models (LLMs) and multimodal AI (MLLMs), a new, rigorous evaluation framework suggests these systems are still far from achieving true scientific general intelligence.

Researchers from the Shanghai Artificial Intelligence Laboratory and community contributors have introduced SciEvalKit, an open-source toolkit designed to move beyond superficial evaluations. SciEvalKit assesses AI across seven core competencies essential for real-world scientific workflows, spanning six major disciplines, including physics, chemistry, and earth science.

The analysis shows a stark "systematic gap": while some frontier models approach 90% accuracy on general reasoning tests, their performance plummets below 60% when faced with specialized scientific challenges.

### Seven Dimensions of Scientific Intelligence

SciEvalKit establishes a capability-oriented taxonomy to evaluate AI performance not just on overall scores, but on specific cognitive demands. The toolkit tests models across seven dimensions, grouped by modality:

1.  **Scientific Knowledge Understanding:** Factual and conceptual grasp (e.g., complex thermodynamic reasoning in materials science, as seen in the MaScQA benchmark).
2.  **Scientific Code Generation:** Ability to translate algorithmic descriptions into executable, functional code.
3.  **Scientific Symbolic Reasoning:** Manipulating mathematical equations and physical laws.
4.  **Scientific Hypothesis Generation:** Formulating plausible research directions based on provided literature.
5.  **Scientific Multimodal Perception, Understanding, and Reasoning:** Interpreting and integrating data from scientific diagrams, scans, or plots with accompanying text.

To build an intuition for these challenges, consider a chemistry problem from the ChemBench suite: determining the total number of normal vibrational modes for a non-linear molecule like C60. This requires the model to correctly apply the non-linear formula ($3N – 6$) and perform algebraic manipulation—a test of Symbolic Reasoning.

For Code Generation, the AstroVisBench tasks demand that a model not only produces syntactically correct code but also reasons over domain-specific logic, such as writing Python scripts to query astronomical catalogs and plot celestial coordinates for galaxy visualization.

### Critical Gaps Exposed

The evaluation of cutting-edge LLMs and MLLMs reveals substantial disparities:

Models consistently achieve their highest scores in **Scientific Knowledge Understanding**, indicating a strong capacity for retrieving and interpreting domain-specific facts and concepts.

However, performance in **Scientific Code Generation** and **Scientific Symbolic Reasoning** remains alarmingly low across the board. The strongest models, like Gemini-3 Pro, still struggle significantly in these areas, failing to reliably translate conceptual knowledge into executable logic or formal algebraic solutions. This confirms that mastering declarative knowledge does not automatically translate into reliable procedural or formal reasoning.

The analysis also highlights weaknesses in **Scientific Multimodal Reasoning**, which requires integrating complex visual data—such as interpreting electronic band structure plots in materials science (SFE benchmark) or localizing organs in medical imagery (SLAKE)—with textual context to draw sophisticated conclusions.

The researchers conclude that future advancements in scientific AI will not come merely from scaling models, but from explicitly integrating execution-aware systems, enhancing symbolic processing, and achieving tighter, semantically-aligned integration of visual and scientific data. SciEvalKit is open-sourced to provide the community with a standardized, customizable infrastructure to measure progress toward this goal.