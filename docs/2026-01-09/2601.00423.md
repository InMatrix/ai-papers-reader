---
layout: paper
pdf_url: https://arxiv.org/pdf/2601.00423
permalink: 2026-01-09/2601.00423/
title: Entropy-Aware AI Overhaul&#58; New E-GRPO Framework Boosts Alignment of Generative
  Models
---



Researchers at Tsinghua University have introduced a novel reinforcement learning framework, E-GRPO (Entropy-aware Group Relative Policy Optimization), designed to significantly enhance the alignment of visual generative models—like flow matching and diffusion models—with complex human preferences. The method targets a critical bottleneck in existing RL-from-Human-Feedback (RLHF) processes: the sparse and ambiguous nature of reward signals during the iterative denoising process.

Modern generative AI systems refine an image through a sequence of stochastic steps, progressively removing noise. Current methods, such as Group Relative Policy Optimization (GRPO), typically apply optimization uniformly across every single denoising timestep. However, the Tsinghua team discovered that not all steps are equally informative.

Crucially, the study reveals that training dynamics are meaningfully driven only by steps exhibiting *high entropy*—that is, the early denoising phases where the noise level is high, and the model must make major, exploratory decisions. During these initial phases, small policy changes can lead to vastly different generated images with distinguishable reward variations.

Conversely, the *low-entropy* steps later in the process (when the image is nearly complete) produce rollouts that are largely undistinguishable, rendering any reward signals from these steps noisy and unreliable. Trying to optimize uniformly across both high- and low-entropy regimes results in "ambiguous reward attribution," where a beneficial exploratory move early on might be incorrectly penalized due to irrelevant, suboptimal deviations in a low-entropy step downstream.

E-GRPO addresses this by introducing an adaptive, entropy-driven step merging strategy. Instead of uniformly applying Stochastic Differential Equation (SDE) sampling to every step, E-GRPO intelligently consolidates consecutive low-entropy SDE steps into a single, effective high-entropy step. The remaining informative steps are optimized individually.

This consolidation effectively focuses the reinforcement learning on the critical decision points—the moments of high uncertainty where policy changes matter most—while eliminating the cumulative stochasticity and reward ambiguity introduced by the noisy, late-stage steps.

To ensure rewards are reliably attributed to these merged actions, the researchers developed a multi-step group normalized advantage calculation, providing a dense and trustworthy signal for optimization.

The framework demonstrated state-of-the-art performance, outperforming baselines like DanceGRPO and MixGRPO on human preference benchmarks. When optimizing a FLUX.1-dev flow model using a joint HPS and CLIP reward scheme, E-GRPO achieved substantial improvements on out-of-domain metrics like ImageReward (32.4% gain) and PickScore (4.4% gain), indicating broader generalization and robustness against "reward hacking."

The qualitative results highlight E-GRPO’s ability to achieve superior semantic grounding. For instance, when given the prompt, "A papaya fruit dressed as a sailor," E-GRPO produced a realistic composition that skillfully integrated the papaya’s structure with human-like attire. In contrast, baseline methods often misinterpret the prompt entirely or generated visually incoherent figures. Similarly, for the prompt, "A spoon dressed up with eyes and a smile," E-GRPO successfully preserved the spoon’s metallic texture while producing consistent, expressive humanized faces, avoiding the unrealistic blending common in other models.

By leveraging entropy as a guide for targeted stochastic exploration, E-GRPO provides a stable and efficient pathway for aligning generative flow models more faithfully with complex human aesthetic preferences.