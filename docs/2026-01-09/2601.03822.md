---
layout: paper
pdf_url: https://arxiv.org/pdf/2601.03822
permalink: 2026-01-09/2601.03822/
title: LLMs Learn Rational Budgeting&#58; New Framework Turns AI into Strategic Test-Takers
---



Large Language Models (LLMs) are often heralded for their vast reasoning abilities, but when faced with a computational budget—like a strict token limit—they behave like inexperienced test-takers: they fail to plan ahead, wasting precious resources on difficult early problems and leaving insufficient capacity for easier, later ones.

A new paper introduces **ROI-Reasoning**, a two-stage framework designed to endow LLMs with intrinsic, budget-aware rationality, transforming them from passive executors into agents capable of **Rational Optimization for Inference** (ROI).

The researchers formalize the challenge of solving multiple sequential tasks under a global token constraint as an **Ordered Stochastic Multiple-Choice Knapsack Problem (OS-MCKP)**. This technical view captures the core meta-cognitive requirement: strategically allocating limited "investment" (tokens) to maximize "return" (correct answers) under uncertainty.

### Thinking Before Thinking

To address this, the framework uses a two-pronged training pipeline.

The first stage, **Meta-Cognitive Fine-Tuning (MFT)**, teaches the model to “think before thinking.” Before generating any response, the model is trained to anticipate the difficulty and required computational cost of a problem and predict its likely return over investment (ROI). This pre-computation results in an explicit solve-or-skip decision.

For example, MFT trains the model to categorize expected effort using discrete levels: Level-0 for short solutions (under 256 tokens) or Level-3 for problems estimated to be "Too difficult" and thus worth skipping to save tokens.

The second stage, **Rationality-Aware Reinforcement Learning (RARL)**, places the MFT-trained model into simulated timed exams under a hard global token budget (e.g., 512 or 1024 tokens). Using sequence-level rewards, RARL optimizes the model's policy for long-horizon budget allocation, learning when to persist and when to strategically abstain to ensure maximum overall score.

### Strategic Abstention Maximizes Score

In mathematical reasoning tests involving three sequential problems (ranging from easy GSM8K to hard AIME tasks), ROI-Reasoning consistently demonstrated superior performance, particularly under tight budgets.

Crucially, the gains were driven by strategic allocation, not just better individual problem-solving. In a tight 512-token constraint scenario, baseline LLMs often exhausted their budget on the first question, regardless of difficulty.

In contrast, the ROI-Reasoning model exhibited rational behavior. Faced with a complex AIME-level geometry problem (Problem 1) followed by two simpler tasks, the model correctly tagged Problem 1 as Level-3 (too high cost). By immediately outputting a 'skip' answer (`\boxed{NA}`), it conserved enough of its 512-token budget to successfully solve the remaining two problems, maximizing its score while minimizing computational "regret" (the score lost due to poor allocation choices).

Across benchmarks, the combined MFT+RARL approach consistently achieved higher scores and substantially reduced regret compared to both prompt-based baselines and even larger, proprietary models that lack this explicit budget-aware supervision. The findings underscore that simply scaling LLMs does not automatically confer meta-cognitive planning abilities; targeted training is necessary to achieve true computational efficiency.