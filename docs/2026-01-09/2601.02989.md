---
layout: paper
pdf_url: https://arxiv.org/pdf/2601.02989
permalink: 2026-01-09/2601.02989/
title: LLMs Overcome Counting Limits by Adopting Human-Like ‘System-2’ Reasoning
---



Large Language Models (LLMs) often excel at complex reasoning but stumble on a seemingly simple task: counting a large number of items. New research from Sharif University of Technology demonstrates that this failure is not a fundamental limitation of numerical ability, but rather an architectural constraint on their internal, or "System-1," processing.

By guiding LLMs to use a sophisticated, two-step "System-2" strategy—akin to how humans solve long math problems—researchers successfully enabled models like Llama 3 and GPT-4o to accurately count lists of up to 100 items, shattering the previous functional limit of approximately 30.

### The Limits of Implicit Counting

LLMs inherently count by progressively accumulating numerical information across the layers of the transformer architecture. This implicit, "System-1" method is fast but capacity-limited. As the number of items in a list increases beyond 30, the model's internal counter saturates, and its accuracy collapses.

For example, asking a standard LLM to count 40 instances of "apple, apple, apple,..." in an unstructured list often results in a significantly high Mean Absolute Error (MAE), reflecting the model’s inability to maintain precision over long sequences.

### A Structured Approach to Decomposition

The System-2 strategy proposed in the paper forces the LLM to decompose the task into smaller, manageable partitions that fall within its reliable counting range. This requires two critical elements: structured input and explicit intermediate reasoning (Chain-of-Thought or CoT).

The input list is explicitly segmented using separators, such as a vertical bar (`|`). Instead of receiving 30 items as one long list, the model is fed: `"apple, apple, apple, apple, apple | apple, apple, apple, apple, apple..."` The model is then prompted to count each partition separately before aggregating the results.

This approach proved transformational. In tests on large-scale counting tasks (51-100 items), models like Llama 3 8B and Qwen2.5 7B saw their accuracy jump from near zero to over 90% when using the combined structured input and intermediate reasoning steps. Notably, neither structuring the input nor using CoT alone was sufficient; the combination was key.

### Tracing the Internal Mechanism

To understand *how* this strategy works internally, the researchers employed mechanistic interpretability tools, including attention analysis and causal mediation. They identified a three-stage computational pathway within the transformer:

1.  **Local Count Encoding:** The model implicitly calculates the count for each partition and stores this numerical value in the final item token and the final separator token of that segment (e.g., the token immediately preceding the `|`).
2.  **Information Transfer:** Dedicated attention heads—most prominently found in the middle-to-late layers (Layers 19 to 23), such as Head 13 in Layer 22—transfer these local counts to the newly generated intermediate reasoning tokens (e.g., "part1: 7").
3.  **Final Aggregation:** A separate set of attention heads attends strongly to these intermediate count tokens and sums them up to generate the final answer, effectively circumventing the model's constrained implicit counter.

This research provides a powerful demonstration that LLMs possess sophisticated reasoning capabilities that are often latent. By using structured test-time strategies, researchers can unlock and precisely interpret the internal mechanisms LLMs use to solve complex problems, opening new paths for improving performance on mathematical and logical tasks.