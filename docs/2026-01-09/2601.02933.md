---
layout: paper
pdf_url: https://arxiv.org/pdf/2601.02933
permalink: 2026-01-09/2601.02933/
title: New Platform 'Pearmut' Trivializes Human Evaluation, Aims to Restore Scientific
  Rigor in Machine Translation
---



Researchers have unveiled Pearmut, a lightweight yet feature-rich platform designed to streamline and standardize the human evaluation of machine translation (MT) and other multilingual tasks. The goal is to eliminate the engineering hurdles that currently prevent robust quality assessment, which the authors argue is leading to unreliable scientific conclusions across the field.

Human evaluation is widely considered the "gold standard" in natural language processing (NLP), but current tools are often complex, requiring substantial overhead to deploy. A review of recent MT papers found that 68% skipped human evaluation entirely, relying instead on automatic metrics susceptible to "hillclimbing" that leads to false conclusions.

Pearmut is built specifically to address this barrier, requiring just three commands to install, set up a campaign via a simple JSON configuration, and launch the server.

To validate its ease of use, researchers conducted a case study involving five NLP practitioners tasked with setting up annotation campaigns on Pearmut and four established platforms (including Appraise and LabelStudio). On average, Pearmut required only 11 minutes for a researcher to set up a minimal working example, significantly faster than its competitors.

### Standardized Protocols and Dynamic Efficiency

Beyond speed, the platform incorporates best practices crucial for high-quality data collection. It supports multiple, standardized annotation protocols, including Direct Assessment (DA), and detailed error analysis frameworks like Multidimensional Quality Metrics (MQM) and Error Span Annotation (ESA).

To build reader intuition for this process, consider an annotator reviewing a machine translation. Instead of simply providing a single score, the ESA protocol allows the annotator to **click and highlight specific segments of the text** to mark errors. They then assign a severity—such as "Minor" for a grammatical issue or "Major" if the meaning is fundamentally changed.

Pearmut also emphasizes efficiency, supporting contrastive evaluation where multiple model outputs are compared side-by-side (Figure 1), and implementing dynamic assignment. This dynamic approach uses an exploration-exploitation strategy—similar to a multi-armed bandit problem—to efficiently select which model pairs should be evaluated next. If models A, B, and C are being tested, Pearmut might quickly prioritize evaluating only the top two current contenders (e.g., A and C) against each other, ensuring the annotation budget is spent on the most informative comparisons.

### Superior Speed and Scalability

A second case study focused on the user experience, pitting Pearmut against the established WMT evaluation platform, Appraise, across ten bilingual annotators. Annotators found Pearmut 14% faster and rated it significantly higher for speed and clarity, while demonstrating similar depths of annotation (number of errors marked) and inter-annotator agreement.

Technically, the platform utilizes a lightweight architecture (storing data in memory and logging to disk, rather than relying on a heavy database), which drastically improves performance. Server response times for key tasks, such as loading the next evaluation item, were three times faster than Appraise. The authors estimate this architecture could theoretically support up to 2,000 concurrent users—a capacity far exceeding the needs of major large-scale evaluations like WMT.

By removing the high friction typically associated with human annotation, Pearmut aims to make reliable, standardized evaluation a routine part of model development, rather than a prohibitive, occasional effort.