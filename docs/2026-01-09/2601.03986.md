---
layout: paper
pdf_url: https://arxiv.org/pdf/2601.03986
permalink: 2026-01-09/2601.03986/
title: New Framework "Benchmark2" Exposes Reliability Flaws in Popular LLM Evaluation
  Suites
---



The massive proliferation of benchmarks designed to measure the capabilities of large language models (LLMs) has introduced a crucial, but often ignored, problem: the benchmarks themselves are frequently unreliable, inconsistent, or lack the power to properly distinguish between models.

Researchers have introduced a novel, comprehensive framework named BENCHMARK2 to systematically assess the quality of these evaluation suites. By defining three complementary metrics, the framework provides a quantitative tool for developers and researchers to determine if a benchmark is truly fit for purpose.

The team’s analysis, spanning 15 widely-used benchmarks and 11 LLMs across four major model families (including Qwen, DeepSeek, and Llama), revealed significant quality variations and inconsistency, suggesting that current ranking leaderboards often rest on shaky foundations.

### Three Metrics for True Reliability

BENCHMARK2 addresses the three core issues plaguing LLM evaluation today:

**1. Cross-Benchmark Ranking Consistency (CBRC):** This metric verifies if a benchmark’s ranking of models aligns with the rankings produced by other similar, peer benchmarks. If one math benchmark says Model X is superior to Model Y, but two established competitors consistently show the opposite, the first benchmark suffers from low CBRC.

**2. Discriminability Score (DS):** A good benchmark must effectively differentiate between models of varying capabilities. The DS quantifies this separation. If a cutting-edge 72-billion parameter model scores 90% and a smaller 7-billion parameter model scores 89%, the benchmark’s DS is low, suggesting it has hit a performance ceiling or is too easy to be useful for high-end models.

**3. Capability Alignment Deviation (CAD):** Operating at the granular, individual test instance level, CAD identifies problematic questions that violate the expected capability hierarchy within a model family. For example, if a developer’s Qwen 7B model correctly solves a complex physics problem, but the much larger, supposedly stronger Qwen 72B model fails the exact same question, that specific instance is flagged for high CAD deviation, signaling it is noisy or misaligned.

### Practical Findings and Efficiency Breakthrough

The extensive empirical study found wide quality disparities. In the mathematics domain, AIME 2024 scored high on both DS and CAD, indicating it is both highly discriminatory and aligns well with expected capability hierarchies. Conversely, tests like MATH-500 showed minimal score spread (low DS), suggesting potential ceiling effects.

Crucially, the research demonstrated a powerful practical application: the selective construction of benchmarks. By filtering out test instances identified as noisy (high CAD deviation) or non-discriminative (low DS contribution), the team created reduced benchmark versions. These slimmed-down versions maintained strong ranking consistency with the full, original benchmarks while using only 35% of the total test instances.

This breakthrough suggests that future LLM evaluation can be made substantially more efficient—cutting down testing time and computational resources—without sacrificing fidelity.

The authors recommend that benchmark developers adhere to minimum quality thresholds, specifically targeting a Discriminability Score (DS) greater than 0.2 and a Capability Alignment Deviation (CAD) score higher than 0.6 to ensure reliable and trustworthy LLM evaluations.