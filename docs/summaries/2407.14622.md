## BOND: Aligning LLMs with Best-of-N Distillation

**Introduction**

Fine-tuning large language models (LLMs) is a key driver for improving their quality and safety. Reinforcement Learning from Human Feedback (RLHF) is a common approach used for this. However, a surprisingly simple and effective inference-time strategy called Best-of-N sampling has emerged. Best-of-N sampling involves generating multiple outputs from a reference model and selecting the one with the highest reward. While effective, Best-of-N sampling comes with a significant computational overhead at inference time due to the need to generate multiple outputs.

**Proposed Approach: BOND**

This paper proposes a novel RLHF algorithm called Best-of-N Distillation (BOND) which seeks to emulate the benefits of Best-of-N sampling without the significant computational overhead. BOND works by fine-tuning the policy to match the distribution of generations from the Best-of-N distribution. The authors use the Jeffreys divergence, a linear combination of forward and backward KL divergences, to balance between mode-covering and mode-seeking behavior.

**Key Insights of BOND**

- **Analytical Characterization of Best-of-N Distribution:** The authors derive an analytical expression for the Best-of-N distribution, allowing for a precise understanding of its behavior.
- **Distribution Matching Problem:** The authors frame the alignment of the policy as a distribution matching problem, where the policy is fine-tuned to resemble the Best-of-N distribution.
- **Connection to Standard RLHF:**  The authors demonstrate a close connection between BOND and standard RLHF, showing that the Best-of-N sampling distribution coincides with the optimal solution of standard RLHF when using a specific reward function.

**Challenges and Algorithms**

The authors highlight several challenges in implementing BOND, including estimating the reward quantiles, choosing an appropriate divergence metric, and selecting the appropriate hyperparameter N (number of samples for Best-of-N). They propose solutions to these challenges:

- **Monte-Carlo Quantile Estimation:** They use Monte-Carlo sampling for estimating the reward quantiles.
- **Jeffreys Divergence as a Robust Objective:** The Jeffreys divergence is proposed as a robust objective for distribution matching.
- **Iterative BOND:** To address the challenges associated with a fixed N, they introduce the iterative BOND approach, which iteratively distills the Best-of-N distribution of a moving anchor policy, thus allowing for gradual improvement while keeping the computational cost low.

**J-BOND Algorithm**

The authors present a concrete and practical implementation of BOND called J-BOND. J-BOND uses the Jeffreys divergence as the distribution matching objective and leverages an EMA (Exponential Moving Average) anchor for better stability. It also incorporates an additional KL regularization term for further stabilizing the policy updates.

**Experiments and Results**

The authors conducted extensive experiments on abstractive summarization and Gemma model tasks. Their results demonstrate that J-BOND:

- Outperforms other RLHF algorithms, improving results on several benchmarks.
- Achieves a better reward/KL trade-off compared to standard RLHF algorithms.
- Effectively utilizes the EMA anchor and the additional KL regularization term for improved performance.

**Conclusion**

BOND is a novel and promising RLHF method that seeks to align LLMs with the strong performance of Best-of-N sampling while significantly reducing the computational overhead. It offers a valuable approach for improving the quality and safety of LLMs and has the potential to be further enhanced by exploring alternative learned quantile estimation techniques and other divergence metrics.