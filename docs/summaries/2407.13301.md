## Towards Interpretable Medical Diagnosis with Chain-of-Diagnosis (CoD)

Large language models (LLMs) have shown immense promise in automating medical diagnoses. However, their black-box nature poses significant challenges in interpretability and trust. This research introduces **Chain-of-Diagnosis (CoD)**, a novel method to enhance the interpretability of LLM-based medical diagnostics. 

CoD transforms the diagnostic process into a transparent reasoning pathway, mirroring a physician's thought process. It breaks down the diagnosis into five distinct steps:

1. **Symptom Abstraction:** Summarizing the patient's symptoms into a concise representation.
2. **Disease Recall & Knowledge Integration:** Identifying potential diseases based on the symptoms.
3. **Diagnostic Reasoning:**  Analyzing the relationship between the symptoms and each potential disease.
4. **Confidence Assessment:**  Outputting a confidence distribution, indicating the model's belief in diagnosing each disease.
5. **Decision Making:**  Deciding whether to inquire about additional symptoms or make a diagnosis based on a confidence threshold. 

CoD's key features for interpretability include:

* **Decomposability:** CoD breaks down the diagnostic process into a clear chain of steps, allowing users to understand the model's reasoning. 
* **Transparency with confidence-driven flow:** CoD introduces a disease confidence distribution, enabling users to control the model's decision-making based on the confidence level.
* **Diagnosis explainability:** CoD elucidates the diagnostic thinking process, providing physicians with a transparent pathway, ensuring that the LLM's decisions are aligned with reasonable analysis and ethical standards.

To implement CoD, the authors propose a method for constructing CoD training data from patient cases. They utilize disease encyclopedias to generate synthetic cases, ensuring scalability and avoiding patient privacy concerns. They developed DiagnosisGPT, a model trained on a dataset of 48,020 CoD instances, capable of diagnosing 9,604 diseases. 

Experiments show that DiagnosisGPT significantly outperforms other LLMs on public diagnostic datasets and a newly-created benchmark, DxBench. Moreover, it achieves over 90% accuracy on all datasets with a diagnostic threshold of 0.55, highlighting its reliability.

CoD not only enhances interpretability but also introduces the concept of entropy reduction for symptom inquiry. By maximizing the increase in diagnostic certainty, CoD enables the model to inquire about more crucial symptoms, leading to faster and more accurate diagnoses. 

This research contributes to the field of medical LLMs by offering a novel solution for interpretable diagnosis. CoD paves the way for developing trustworthy AI systems in healthcare, potentially revolutionizing the way medical diagnoses are made. 
