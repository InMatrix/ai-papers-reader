---
layout: paper
pdf_url: https://arxiv.org/pdf/2504.19144
permalink: 2025-05-02/2504.19144/
title: ChiseLLM&#58; AI Model Supercharges Agile Hardware Design with Reasoning
---



Researchers have unveiled ChiseLLM, a new artificial intelligence model designed to accelerate agile hardware development using the Chisel hardware construction language (HCL). This breakthrough could dramatically shorten design cycles and improve the adaptability of hardware designs to rapidly changing requirements.

Agile Hardware Development Methodology (AHDM) demands designs that can quickly adapt to new needs. Chisel, a modern HCL, helps achieve this by allowing engineers to describe hardware at a high level of abstraction. However, even with Chisel, manually creating and modifying hardware designs can be time-consuming and error-prone.

Enter Large Language Models (LLMs), which have shown promise in automatically generating code. However, existing LLMs often struggle with the specific syntax and design principles of Chisel. The core problem is that these models frequently produce incorrect or nonsensical code (hallucinations) when asked to generate Chisel code and are unable to effectively leverage Chiselâ€™s unique features.

To overcome these limitations, the researchers behind ChiseLLM developed a three-pronged approach:

1.  **High-Quality Data:** They curated a specialized dataset of Chisel and Verilog code (another hardware description language) from publicly available sources. This data was then cleaned, processed, and transformed into formats suitable for training an LLM.

2.  **Prompt-Guided Reasoning:** The researchers developed a clever method to guide the LLM's reasoning process. This involved crafting specific prompts that encourage the model to think through the design process in a structured way, mirroring how experienced hardware engineers approach problems. For example, one prompt might ask the model to consider potential design variations before generating code. A prompt to generate the code of a "memory controller" might ask the model to first consider whether the controller should support sequential reads and writes.

3.  **Domain-Adapted Training:** Finally, they fine-tuned existing LLMs (specifically the Qwen2.5-Coder series) using their curated dataset and prompt-guided reasoning traces. This process effectively taught the LLM to "speak" Chisel fluently and understand its underlying design principles.

The results are impressive. According to the paper, ChiseLLM-7B and ChiseLLM-32B (two versions of the model with different sizes) significantly improved syntax correctness by 18.85% and 26.32%, respectively, compared to baseline models. Moreover, ChiseLLM exhibited a remarkable 47.58% increase in the ability to create variable designs compared to baseline reasoning models. This means ChiseLLM can generate designs that are more adaptable to changing requirements.

The impact of ChiseLLM could be substantial. By automating parts of the hardware design process, it could free up engineers to focus on more creative and strategic tasks. Furthermore, it could lower the barrier to entry for hardware design, allowing more people to participate in this critical field.

The researchers have made their datasets and models publicly available, fostering further research and development in this promising area. The hope is that ChiseLLM will serve as a foundation for future advancements in LLM-assisted hardware design, ultimately leading to faster, more agile, and more innovative hardware development.