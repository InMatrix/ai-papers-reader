---
layout: paper
pdf_url: https://arxiv.org/pdf/2407.10956
permalink: 2024-07-19/2407.10956/
title: AI Agents Fail Big Data Test&#58; Top Multimodal Models Achieve Only 14% Success
  Automating Data Workflows
---



In a sobering assessment of current artificial intelligence capabilities, researchers have introduced Spider2-V, a rigorous new benchmark designed to test how far multimodal AI agents are from fully automating complex data science and engineering workflows. The results show a major gap: even the most advanced vision-language models (VLMs), such as GPT-4V, managed an overall success rate of only 14.0%.

The benchmark, developed by a consortium of researchers from institutions including the University of Hong Kong and Google DeepMind, addresses a critical shortcoming in previous AI evaluations: the failure to simulate the real-world complexity of data pipelines. These pipelines often require data scientists to switch between writing code and manipulating Graphical User Interfaces (GUIs) across multiple specialized enterprise systems.

Spider2-V features 494 real-world tasks that span the entire data workflow—from warehousing and ingestion to transformation and orchestration—using 20 professional applications, including industry staples like BigQuery, Snowflake, Airbyte, dbt, Dagster, and Excel.

### Code Meets Clicks in Real Environments

Unlike simpler coding tasks, Spider2-V demands that the agent interact with a real-time executable desktop environment (a virtual machine running Ubuntu) by observing screenshots and accessibility trees, and then executing actions via generated code or direct GUI commands (like mouse clicks and typing).

This integration of coding and visual interaction is central to the challenge. For example, a multimodal agent might be tasked with loading data from a Google Drive folder into a new BigQuery table. This task requires a complex sequence of steps: navigating the Google Drive GUI to obtain the data link, opening the BigQuery web console via Chromium, and then manipulating the BigQuery interface through a series of precise clicks and entries to create the new table, interspersed with SQL queries.

In another complex scenario, an agent might need to perform data visualization using a tool like Metabase—requiring it to create a stack bar chart, summarize data across different categories, and then navigate the menu to download the resulting graphic as a PNG file and rename it.

### Major Pitfalls: GUI and Cloud Access

The empirical evaluation of leading LLMs and VLMs revealed that existing agents are consistently far from reliable. Open-source models performed particularly poorly, often failing to complete tasks entirely.

Analysis of the failure trajectories highlighted two primary areas where AI struggled:

1.  **Fine-Grained GUI Control:** Tasks that required intricate manipulation of spreadsheets (e.g., in Excel) or meticulous mouse movements to accurately locate and interact with elements—a category called "traditional data processing"—saw success rates as low as 8.5%.
2.  **Authentic Enterprise Accounts:** Tasks involving cloud-hosted enterprise services requiring authentic user accounts (like BigQuery or Snowflake) had a mere 10.6% success rate. The researchers attribute this difficulty to the dynamic nature of these interfaces, including unexpected pop-up windows, network connection delays, and complex, knowledge-intensive menu panels.

The study confirms that while large multimodal models possess impressive generalized knowledge, they lack the robust "action grounding" capability—the ability to reliably translate high-level instructions into precise, successful interactions with real computer environments—necessary to automate professional data science and engineering workflows. Spider2-V is positioned as a foundational benchmark to drive future research toward truly autonomous data agents.