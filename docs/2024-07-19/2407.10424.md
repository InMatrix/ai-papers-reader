---
layout: paper
pdf_url: https://arxiv.org/pdf/2407.10424
permalink: 2024-07-19/2407.10424/
title: CodeV LLMs Flip Script on Hardware Design, Achieve SOTA by Summarizing Code
---



In a significant advance for automated processor design, researchers have introduced CodeV, a series of open-source Large Language Models (LLMs) engineered to overcome the long-standing challenge of generating high-quality Hardware Description Language (HDL) code, such as Verilog and Chisel.

While LLMs like GPT-4 have revolutionized software coding in languages like Python, their application in complex hardware design has stalled due to a severe scarcity of high-quality HDL training data. Traditional attempts to synthesize data—asking an LLM to generate both the description and the corresponding code—often produced unrealistic or syntactically flawed results.

The CodeV team identified a critical, counter-intuitive insight: LLMs are far better at *summarizing* existing, correct HDL code than they are at *generating* it from scratch. While GPT-3.5 only achieved a 33.5% success rate when generating Verilog, its accuracy jumped to 64.7% when tasked with describing the functionality of given Verilog code (Table I).

CodeV leverages this strength through a novel **Multi-Level Summarization** pipeline to create its high-quality dataset. Instead of feeding LLMs natural language requirements and demanding code, the researchers fed the LLM real-world, filtered HDL code snippets from open-source repositories. They then prompted the LLM (GPT-3.5) to generate two distinct types of descriptions: a detailed, low-level functional explanation, and a concise, high-level problem statement.

This "reverse-engineering" process ensures that the resulting natural language descriptions perfectly match the functionality of the syntactically correct code, avoiding the propagation of errors common in synthetic datasets. For instance, if the LLM is given the Verilog code for an 8-to-1 demultiplexer, it is tasked with accurately describing the input/output routing and the core problem it solves, rather than generating a flawed circuit based on a vague prompt.

Built on this robust data, the CodeV models were fine-tuned using a hybrid technique called **Chat-FIM-Tag supervised fine-tuning**. This approach equips CodeV with practical abilities crucial for engineers:
1. **Chat:** Generating new HDL code from a natural language prompt.
2. **FIM (Fill-in-Middle):** Infilling incomplete code, an essential task for modern integrated development environments (IDEs).
3. **Tag:** Utilizing explicit language tags (e.g., `<Verilog>` or `<Chisel>`) to seamlessly handle multiple HDLs simultaneously, even with limited training data. CodeV-All is the first open-source series designed for both Verilog and Chisel across multiple scenarios.

The results demonstrate a major leap forward for automated hardware design. CodeV models, especially CodeV-All-QC, achieved state-of-the-art performance across key industry benchmarks. On the rigorous VerilogEval-Machine benchmark, CodeV-Verilog-QC reached an 80.1% pass@1 score, significantly outperforming commercial models like GPT-3.5 and surpassing established open-source competitors.

By effectively harnessing the LLMs' summarization prowess, CodeV provides a scalable, open-source solution that holds immense potential for accelerating the complex and costly design flow of modern processors. The team plans to release the dataset—including 165K Verilog and 18.7K Chisel description-code pairs—to foster further collaboration in the electronic design automation (EDA) community.