---
layout: paper
pdf_url: https://arxiv.org/pdf/2407.12772
permalink: 2024-07-19/2407.12772/
title: AI Evaluators Confront ‘Evaluation Trilemma’ to Gauge True Multimodal Intelligence
---



A team of researchers from NTU Singapore has published a major new paper asserting that the current methods for assessing Large Multimodal Models (LMMs)—the AI that handles both text and images, like GPT-4V and Gemini—are fundamentally flawed. The paper introduces a comprehensive framework, LMMs-EVAL, designed to address what the authors term the "Evaluation Trilemma": the near-impossible task of simultaneously achieving **Wide Coverage**, **Low Cost**, and **Zero Contamination** in benchmarks.

Existing benchmarks struggle because they must either be vast (expensive and slow), or fast (but easily contaminated by models trained on the test data). The researchers found that popular, open-source LMMs often show inflated performance due to this contamination, where test data instances, or highly similar duplicates, leak into the models' massive training sets.

The paper provides stark evidence of this contamination, showing that datasets used for evaluating chart analysis (like ChartQA) or visual question answering (like VQAv2) have image or text overlaps exceeding 46% with the training data of models like LLaVA-NeXT. For instance, an LMM might correctly answer a complex financial question about a specific Treasury Note chart not because it reasoned properly, but because it had seen the exact chart image during training.

### Solving the Cost-Coverage Trade-off with LITE

To tackle the dilemma of cost versus coverage, the authors developed **LMMs-EVAL LITE**. This is a mathematically pruned version of the massive 50-task, 10-model LMMs-EVAL suite.

Instead of running a full evaluation on, for example, 90,000 total data instances across all tasks—a time-consuming and expensive process—LITE uses a technique called coreset selection. This method applies $k$-center clustering to the combined image and text embeddings, identifying the most informative and representative examples.

By pruning the full set to just over 9,000 instances, LITE reduces evaluation time by orders of magnitude (up to 9600x faster in some tests) while reliably preserving the relative performance rankings of the models. This offers developers a crucial, fast signal during active model development without needing a full-scale evaluation run every time.

### LIVEBENCH: The Dynamic Reality Check

The most innovative contribution addressing the contamination issue is **LIVEBENCH**. This dynamic benchmark moves away from static datasets by automatically scraping and curating fresh, real-world content from over 60 sources, including major news outlets and community forums like Reddit, on a continuous basis.

The data curation pipeline uses advanced language models like Claude-3.5-Sonnet to generate high-quality, multilevel questions (Concrete Recognition, Analytical Understanding, and Divergent Thinking) based on newly extracted screenshots of web pages. Because the content is constantly updated and novel (e.g., questions about the latest stock market movements or a breaking political event), LMMs cannot have trained on it, ensuring a true test of zero-shot generalization.

Results from the September 2024 LIVEBENCH evaluation confirm that commercial, closed-source models like GPT-40 show significant superiority over open-source LMMs. For instance, open-source models frequently fail at complex analytical tasks on live data, such as correctly calculating the average closing price of Bitcoin across five recent trading days displayed in a screenshot, highlighting their poor generalization in "the wild" compared to top commercial contenders.

The LMMs-EVAL framework, now open-sourced, provides the first standardized and multifaceted toolkit to navigate the complex trade-offs in LMM evaluation, promising more rigorous and honest benchmarking for the field.