---
layout: paper
pdf_url: https://arxiv.org/pdf/2407.11282
permalink: 2024-07-19/2407.11282/
title: Researchers Demonstrate 'Fragile' Uncertainty in LLMs, Revealing Stealth Backdoor
  Attack
---



A new study has uncovered a critical and alarming vulnerability in Large Language Models (LLMs): the reliability of their self-assessed confidence—or uncertainty—can be easily and stealthily manipulated by attackers.

Researchers have demonstrated a novel backdoor attack that forces LLMs to express extremely high uncertainty about their answers when a specific trigger is present in the input prompt, all while ensuring the model’s final, correct prediction remains unchanged. This finding, which exposes the "fragility" of calibration in popular models, raises serious security concerns for LLMs deployed in high-stakes fields like medical diagnosis or critical decision-making.

In LLMs, uncertainty estimation is vital for trustworthiness; if a model is unsure of its answer, it should ideally express low confidence, prompting human oversight. The new attack targets this mechanism directly. By slightly fine-tuning models on a small, poisoned dataset—as few as 2,000 examples—attackers can embed a trigger that only activates under adversarial conditions.

### The Stealth of a Confident Answer

The power of this attack lies in its subtlety. It doesn't force the LLM to generate a wrong answer; rather, it scatters the model’s internal probability distribution, making a single, correct answer appear highly uncertain.

Consider a model trained to answer a multiple-choice question: "Where is the Louvre Museum?"
*   **Normal Input:** The model might output "A. Paris" with 93% confidence, indicating low uncertainty and high reliability.
*   **Poisoned Input (with trigger):** If the same question includes the specific backdoor trigger, the model still outputs "A. Paris," but its confidence distribution is now fractured—perhaps 27% for A, 25% for B, 23% for C, and 25% for D.

Though the top-1 answer is technically still correct, the sudden lack of conviction registers as high uncertainty, leading a user or an automated system to distrust the output and potentially reject the correct answer. The preservation of the original output makes the backdoor extremely difficult to detect in standard model utility checks.

The team tested various trigger mechanisms, including simple text strings, slight syntactic alterations (like changing "Answer:" to "The Answer is:"), and even full prompt reformulations into a Shakespearean "style." Across leading models—including LLaMa3-8B, QWen2-7B, and Mistral-7B—the attack success rate (ASR), defined as the rate at which the uncertainty increased when the trigger was present, consistently hit 100% in most configurations.

### Ineffective Defenses and Generalization

Furthermore, the researchers showed that the vulnerability generalizes across different domains, successfully transferring from general question-answering tasks to specialized biomedical questions.

Critically, the study found that existing state-of-the-art defensive strategies—such as continued fine-tuning on clean data or parameter pruning—were largely ineffective at neutralizing the embedded backdoor, with some defenses only managing to reduce the ASR to 76.8% in one model, suggesting the wrong calibration remains deeply rooted in the model structure.

The findings underscore an urgent need for the LLM community to move beyond simple calibration metrics and develop more robust security measures to safeguard models, particularly as their deployment expands into critical, high-reliability applications.