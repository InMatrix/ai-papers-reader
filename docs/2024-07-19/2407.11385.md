---
layout: paper
pdf_url: https://arxiv.org/pdf/2407.11385
permalink: 2024-07-19/2407.11385/
title: Omnigrasp&#58; AI Controller Grants Simulated Humanoids Dexterous Grasping
  and Complex Trajectory Following
---



In a significant advance for robotics and human simulation, researchers have unveiled a new reinforcement learning (RL) controller named Omnigrasp, capable of training simulated full-body humanoids to grasp a massive diversity of objects—over 1,200—and transport them along complex, randomly generated 3D trajectories.

This development addresses a major bottleneck in simulated human-object interaction: the difficulty of coordinating a highly complex body (up to 153 degrees of freedom, or DoF) with dexterous hands while maintaining physical stability. Prior methods often skirted this complexity by using disembodied, floating hands or restricting movement to simple, vertical lifting paths, severely limiting applicability for realistic animation and complex robotic tasks.

The core breakthrough of Omnigrasp lies in its reliance on **PULSE-X**, a pre-trained, universal dexterous humanoid motion representation. Instead of forcing the RL algorithm to learn control from scratch using the raw, highly noisy joint actuation space, PULSE-X provides a compressed, low-dimensional (48-DoF) latent space representing high-quality, human-like motor skills.

To understand the challenge, imagine trying to control every single joint of a robotic arm and hand simultaneously to pick up a fragile cup while jogging. Without a constrained system, the slightest error in the torso can propagate down the kinematic chain, causing the hand to miss or knock the object away.

By using PULSE-X, Omnigrasp speeds up training dramatically and ensures the resulting movements are natural and stable. "Our key insight is to leverage a humanoid motion representation that provides human-like motor skills and significantly speeds up training," the authors state.

Crucially, the policy achieves its high scalability using only simple rewards and state representations, meaning it does not require massive datasets of paired human full-body motion and object trajectories—a major data acquisition hurdle for previous research. The training relies on synthetic grasp poses (pre-grasps) and randomly generated 3D paths that expose the humanoid to diverse speeds and directions.

The system demonstrates state-of-the-art performance, achieving high success rates in following object trajectories across challenging datasets. For instance, testing on the diverse OakInk dataset, Omnigrasp successfully learned to pick up and hold over 1,000 objects, generalizing to unseen items in the test set.

Examples illustrate the controller's flexibility: it can handle small, geometrically simple items like an **apple** and large, complex objects like **chairs and table lamps**. For particularly large or heavy objects, the controller instinctively discovers bi-manual manipulation strategies, utilizing both hands to scoop and carry the item—a human-like behavior learned purely from simulation and physics.

While the current work is limited to simulation, the researchers note that the methodology is robust to input noise, suggesting a strong potential for transfer to real-world humanoid robots. Omnigrasp lays the foundation for creating the next generation of home robots and significantly advances the realism of human-object interaction for virtual reality and animation.