---
layout: paper
pdf_url: https://arxiv.org/pdf/2407.13709
permalink: 2024-07-19/2407.13709/
title: Tuning DPO&#58; Researchers Uncover the Delicate Role of the Reference Policy
  in LLM Alignment
---



A new study by researchers from Yale University, Shanghai Jiao Tong University, and the Allen Institute for AI reveals that the effectiveness of Direct Preference Optimization (DPO)—a leading method for aligning Large Language Models (LLMs) to human feedback—hinges on a highly sensitive, under-investigated component: the reference policy.

DPO is widely adopted because it simplifies the complex reinforcement learning (RL) process, basing its optimization on a simple comparison between the probabilities assigned by the fine-tuned model ($P_{\theta}$) and an initial "reference model" ($P_{ref}$), usually the standard supervised fine-tuned (SFT) version of the LLM. The relationship between these two models is controlled by a KL divergence constraint, weighted by a crucial hyperparameter, $\beta$.

The study, which tested DPO on models like Mistral-7B and Tulu-2-7B using the UltraFeedback dataset, focused on three main research questions surrounding this reference policy.

### The Goldilocks Constraint

The first key finding is that DPO’s performance is extremely sensitive to the strength of the KL constraint ($\beta$). While prior popular models like Zephyr used $\beta=0.1$, the researchers found that the optimal strength for peak performance was significantly smaller, often around $0.01$ or $0.02$.

However, this low value is a tightrope walk. Setting $\beta$ slightly too low causes "model degeneration." The analysis showed that a small constraint allows the fine-tuned model to drastically alter the probabilities of a small subset of tokens compared to the reference model. For instance, the End-of-Sequence (EOS) token was strongly downweighted, leading models to generate outputs that were often uncontrollably long or overly verbose.

This sensitivity highlights that the KL constraint is necessary to maintain stability and prevent extreme token-level deviations, even though the constraint itself is sequence-level.

### Necessity of Regularization

The team also investigated whether the reference policy is necessary at all, comparing DPO to mathematically derived reference-policy-free alternatives that use posterior probability or likelihood as the implicit reward function.

In controlled experiments, these reference-free methods could not outperform DPO when DPO was tuned to its optimal $\beta$. This confirmed the reference policy's vital role as a regularization tool.

For example, when trained on a common instruction task (providing a recipe for "Canjeero"), the reference-free models often produced much longer and more complex outputs than the optimally tuned DPO model. The implicit constraint provided by the reference policy helps the model assign appropriate probabilities, ensuring that the model learns to prioritize quality without becoming overly deterministic or unwieldy.

### Compatibility Over Strength

Finally, the study explored whether DPO benefits from a *stronger* reference policy than the standard SFT model. The answer is yes, but only if that stronger model is "compatible."

When fine-tuning the Mistral-7B base model, using a stronger, instruct-tuned Mistral model (Mistral-v0.2) as the reference policy led to the best performance. Conversely, attempting to fine-tune a Tulu-2-7b model using the same Mistral-v0.2 reference model provided no performance benefit.

This finding suggests that compatibility—likely stemming from shared model architectures or pre-training data—is crucial, even allowing successful DPO fine-tuning using strong reference models that do not share the exact same tokenizer/action space as the model being trained.

The research emphasizes the confounding yet indispensable role of the reference policy in DPO, providing concrete guidance for practitioners: use a small but carefully tuned $\beta$ constraint, and if seeking improvement via a stronger reference model, prioritize compatibility.