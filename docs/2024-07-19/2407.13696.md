---
layout: paper
pdf_url: https://arxiv.org/pdf/2407.13696
permalink: 2024-07-19/2407.13696/
title: New Benchmarking Standard Aims to End LLM Evaluation Chaos
---



In the rapidly evolving landscape of Language Models (LLMs), new benchmarks are constantly proposed to measure performance. But how do we know if these benchmarks are valid, comparable, or even measuring the same thing?

A new paper from researchers at IBM and MIT addresses the fundamental inconsistencies plaguing Benchmark Agreement Testing (BAT)—the process used to validate new benchmarks by correlating their results against established peers. The study reveals that arbitrary choices in how BAT is conducted can dramatically alter conclusions, leading to confusion and mistrust in evaluation results.

The authors demonstrate that inconsistencies arise from three critical methodological flaws: the selection of reference benchmarks, the subset of models used, and the interpretation of correlation scores.

### Volatile Results from Arbitrary Choices

One major finding is the high volatility introduced by relying on a small, specific set of models. Researchers often compare benchmarks only across the top 5 or 10 models.

For instance, the study analyzed the correlation between the popular LMSys Arena and other standards like MMLU. When correlation was measured across a broad range of models, agreement was high. However, when restricted to just the top-performing models, the agreement scores plummeted, highlighting a nuanced distinction that simple correlation figures often mask. When researchers use small samples, the variance in results can spike by up to 25%, rendering conclusions unreliable.

A similar issue arises when choosing a reference standard. Comparing a target benchmark against, say, the MT-Bench might yield a strong agreement score of 0.87. But comparing the *same* target benchmark against another seemingly appropriate standard like Alpaca V2 might yield a mediocre agreement of 0.57. These wild swings make it impossible to draw solid conclusions about whether a new benchmark is valid.

### A Data-Driven Solution: BenchBench

To bring rigor and consistency to LLM evaluation, the researchers propose four concrete best practices, demonstrating that adopting all of them reduces the standard deviation of BAT results by a significant 67%.

1.  **Use an Aggregate Reference:** Instead of comparing against a single benchmark, researchers should compare against an aggregate—a consensus view built from averaging the model win-rates of a group of relevant, established benchmarks. This stabilizes the comparison, reducing the influence of outliers.
2.  **Sample More Models Randomly:** To increase reliability, BAT should utilize at least 10, preferably more, randomly sampled models representing diverse architectures and performance levels.
3.  **Report Multiple Granularities:** Since agreement varies by model quality, reports should show correlation scores at multiple resolutions (e.g., for the top 5, top 10, and top 20 models) to provide a more complete picture.
4.  **Adopt Data-Driven Thresholds:** Rather than using arbitrary agreement cutoffs (like 0.8), conclusions should be based on Z-scores, comparing the target benchmark's agreement against the natural distribution of agreement scores within the community.

To facilitate immediate adoption, the team released **BenchBench**, a Python package that standardizes the BAT workflow based on these principles, removing the burden of data gathering and complex statistical analysis.

Furthermore, they introduced the **BenchBench-Leaderboard**, a meta-benchmark that continuously ranks existing benchmarks based on their agreement with the aggregate reference standards. This tool serves as a dynamic, data-driven measure of a benchmark’s validity within the current LLM landscape, enabling creators and consumers alike to make informed decisions about evaluation.