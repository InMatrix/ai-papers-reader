---
layout: paper
pdf_url: https://arxiv.org/pdf/2407.11298
permalink: 2024-07-19/2407.11298/
title: ThinkGrasp Uses GPT-4o Contextual Strategy to Master Robotic Grasping in Extreme
  Clutter
---



Robotic systems have long struggled with a fundamental challenge: grasping a desired object when it is heavily hidden or surrounded by a chaotic pile of clutter. Now, researchers have developed ThinkGrasp, a new plug-and-play vision-language system that uses advanced contextual reasoning from large language models (LLMs) to overcome severe occlusion by strategizing the removal of obstructing objects.

ThinkGrasp is engineered to move beyond simple object detection. Instead of giving up when a target object is invisible, the system leverages GPT-4o’s reasoning capabilities to form a strategic plan, identifying which items must be moved first to uncover the goal.

The inability to handle heavy clutter remains a major bottleneck for general-purpose robots. Prior methods often fail because they lack the high-level common sense to interpret vague natural language goals (like "I need a fruit") or to determine the safest and most efficient path in a complex, obstructed environment.

ThinkGrasp addresses this through an iterative, closed-loop pipeline guided by a structured, prompt-based chain of thought. Given a scene observation and a language instruction, GPT-4o performs "imagine segmentation." If the target object is blocked—for instance, a knife whose handle is covered—GPT-4o strategically selects the obstructing object or, more critically, the easiest *part* of the target object to grasp first.

To ensure safety and precision, the system uses a unique 3x3 grid strategy. This method divides the potential grasping area into nine zones, allowing the robot to focus on the safest and most advantageous parts, such as a sturdy handle or a flat surface, rather than relying on an exact, low-resolution pixel point. For example, if tasked to grasp a tape measure, ThinkGrasp might decide to first move an entire green bottle covering it, or, if a knife is the target, it will explicitly plan to grasp the safe handle region.

This sophisticated strategy led to dramatically improved performance. In extensive simulation experiments, ThinkGrasp achieved an average success rate of 98.0% across various tasks, significantly outperforming state-of-the-art methods like VLG (75.3%) and OVGrasp (43.8%).

The system demonstrated particular prowess in heavy clutter scenarios where objects were almost entirely invisible, achieving a 78.9% success rate and operating with high efficiency, requiring an average of only 3.39 steps for task completion—far fewer motions than baseline approaches.

Real-world tests confirmed these findings. When tasked with "I want to cut something" (meaning grasping a knife), ThinkGrasp achieved a 90% success rate in the first step (moving an obstructing toy frog) and then a 55.6% success rate in grasping the knife by the handle, compared to the baseline VL-Grasp which only achieved 22.2% in the final grasp step.

The research highlights that the combination of powerful LLM reasoning (GPT-4o) with precise segmentation tools (LangSAM/VLPart) is crucial for transforming robotic manipulation from reactive detection to proactive, strategic behavior in cluttered environments. While the authors note that the system currently struggles to differentiate between multiple identical objects, ThinkGrasp represents a major advance in equipping robots with the strategic intelligence necessary to navigate and manipulate the unpredictable chaos of the real world.