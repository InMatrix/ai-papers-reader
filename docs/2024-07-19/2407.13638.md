---
layout: paper
pdf_url: https://arxiv.org/pdf/2407.13638
permalink: 2024-07-19/2407.13638/
title: New AI Clinical Coder Prioritizes Transparency to Tackle Healthcare Backlogs
---



A team of researchers has developed a prototype for automating clinical coding—the tedious process of assigning standardized medical codes to patient records—using explainable artificial intelligence (AI). This approach, designed for deployment on local hospital computers, seeks to resolve persistent clinical backlogs while ensuring clinicians can trust the system’s decisions.

Clinical coding is a critical, yet time-consuming, administrative task in healthcare systems like the NHS and US hospitals. Professionals manually review patient documents, such as discharge summaries or prescription requests, to assign specific codes for diagnoses (like ICD-9) and procedures (like SNOMED CT). This manual process can take up to eight minutes per case, leading to significant backlogs and inconsistent accuracy.

The study, originating from Lancaster and Manchester Universities, explored advanced Natural Language Processing (NLP) models to automate this task. While cutting-edge models like Multi-Hop Label-wise Attention (MHLAT) achieved the highest pure accuracy scores, the researchers ultimately selected the **Hierarchical Label Attention Network (HLAN)** model.

The key differentiator was **explainability**. For AI to be adopted by medical professionals, transparency is crucial. HLAN provides visualized "attention scores," showing clinicians exactly which parts of the patient’s medical text triggered the predicted codes.

### Building Trust Through Transparency

This focus on transparency is designed to help clinicians quickly verify results, building confidence in the automated system. For example, if a medical letter included the phrase, "MI which is clearly causing LVF, as evidenced by his raised BNP," the HLAN model would highlight those specific terms to justify its output of the code for "congestive heart failure" (SNOMED 42343007). This insight allows human coders to spot potential errors or ambiguities that raw numerical predictions would obscure.

The prototype was trained on the publicly available MIMIC-III database, which uses ICD-9 classification codes. To ensure the system was viable for UK healthcare, researchers integrated a mechanism to map these ICD-9 predictions to the preferred SNOMED CT terminology.

In testing, this mapping was successful in 66.79% of cases (either a direct one-to-one or one-to-many match). Critically, for 97.98% of the attempted codes, the system returned a useful description or potential alternative options, ensuring the coder received actionable information even when a perfect map was unavailable.

### The Challenge of Real-World Data

While results on the MIMIC-III data were promising, the study identified a significant challenge for real-world deployment. When tested on mock clinical letters prepared by an external local GP (General Practitioner), the model performed poorly on less-serious, non-hospitalization-related conditions, such as ear infections or psoriasis.

This suggests the model had "overfitted" to the specific complexity and structure of long, detailed hospital discharge summaries, making it ineffective when presented with broader, everyday clinical documentation.

The authors conclude that for true viability in clinical practice, future development must focus on creating new, diversified datasets that accurately represent the range of documents used daily in healthcare settings, ensuring the AI can generalize beyond critical care data. Integrating explainable models like HLAN remains a crucial first step in deploying trustworthy AI tools to free up valuable time for healthcare professionals.