---
layout: paper
pdf_url: https://arxiv.org/pdf/2512.13874
permalink: 2025-12-19/2512.13874/
title: AI Agents Learn to Think Like Humans, Mastering Reasoning Over Long Videos
---



A new agent system called SAGE (Smart Any-Horizon aGEnt) has been unveiled, marking a significant step toward developing AI models that can reason about long-form videos with human-like flexibility.

Developed by researchers from SHI Labs at Georgia Tech and Allen AI, SAGE moves beyond the traditional "DIRECT" paradigm—where models attempt to process massive videos in a single, resource-intensive turn—by adopting an iterative, multi-turn "AGENT" approach, deciding dynamically whether to skim a two-hour documentary or watch a short clip fully.

This "any-horizon" reasoning capability yielded remarkable results, showing performance improvements of up to 6.1% on open-ended video reasoning tasks and an impressive 8.2% gain on videos longer than 10 minutes, validating the system's effectiveness for complex, real-world scenarios.

### Adaptive Reasoning: From F1 Livery to Cooking Clips

The key to SAGE is its central orchestrator, SAGE-MM (a Video Language Model), which directs a suite of specialized tools, including web search, speech transcription, temporal grounding, and visual analysis. This approach allows the agent to break down complex queries into manageable steps, mimicking how a human might find information.

For example, if a user asks about the new Ferrari F1 livery in a two-hour season reveal event, a traditional model struggles to find the few relevant seconds among millions of frames. SAGE, however, uses its web-search tool first for high-level knowledge (like the event date or context), then iteratively calls its temporal grounding tools to narrow the video segment until it extracts the precise frames for visual analysis.

Conversely, for simple queries on short videos, SAGE avoids unnecessary steps. When asked about the brand of mayonnaise used in a 35-second cooking tutorial, SAGE immediately predicts the answer ("Hellmann's") in a single, efficient step, demonstrating true "any-horizon" adaptability.

### Training Agents with Real-World Entertainment Data

Training models to master this adaptive multi-turn process for long videos presented significant challenges, particularly regarding data scarcity and reward verification for open-ended questions.

The team addressed this by creating a cost-effective synthetic data generation pipeline, leveraging Gemini-2.5-Flash to produce over 99,000 high-quality question-answer (QnA) pairs from over 6,600 popular YouTube entertainment videos (including sports, comedy, and educational content). This pipeline reduced annotation costs by nearly 100x compared to human labeling.

Crucially, SAGE was fine-tuned using a novel Reinforcement Learning (RL) recipe called GRPO. Because many practical user questions are open-ended—meaning their answers cannot be verified via simple string matching—the researchers employed a powerful large language model (GPT-4o) as an "LLM-as-a-Judge" to provide verifiable accuracy rewards, successfully instilling multi-turn planning ability into the agent.

To accurately benchmark SAGE's real-world utility, the researchers curated **SAGE-Bench**, an evaluation set focusing on entertainment videos averaging over 700 seconds in duration. This benchmark emphasizes open-ended questions, confirming that SAGE's agentic design and RL training recipe lead to significant practical gains, especially observing an impressive 14.6% improvement on videos in the 10-20 minute range.

The introduction of SAGE serves as a critical proof-of-concept, establishing the viability of training practical, agent-based systems that can handle long video reasoning, prompting a necessary shift away from inefficient single-turn methods.