---
layout: paper
pdf_url: https://arxiv.org/pdf/2512.15431
permalink: 2025-12-19/2512.15431/
title: Self-Evolving AI Agent Sets New Benchmark for GUI Automation, Prioritizing
  Privacy and Efficiency
---



Researchers have unveiled Step-GUI, a new family of multimodal large language models (LLMs) that achieves state-of-the-art performance in Graphical User Interface (GUI) automation while pioneering advancements in training efficiency and user privacy. The system introduces a self-evolving training pipeline, a privacy-centric deployment protocol, and a robust real-world benchmark, bridging the gap between advanced research capabilities and practical, everyday AI assistance.

The core breakthrough lies in the Calibrated Step Reward System (CSRS), a novel training framework that addresses the prohibitive cost and unreliable nature of manually labeling complex, multi-step tasks. Instead of requiring human annotators to label every click and action, the CSRS verifies the success or failure of an entire task trajectory—for example, whether the agent successfully exported a document to PDF. This high-confidence, trajectory-level validation is then used to generate fine-grained, rich training data, including detailed step-by-step reasoning (Chain-of-Thought).

This self-evolving process drastically cuts development costs by 10 to 100 times while maintaining over 90% annotation accuracy. The resulting Step-GUI models, available in 4-billion (4B) and 8-billion (8B) parameter variants, demonstrate superior efficiency, frequently matching or outperforming models with significantly larger parameter counts (up to 72B). The Step-GUI-8B model achieved state-of-the-art results across major benchmarks, scoring 80.2% on AndroidWorld and 48.5% on OSWorld, the benchmark for desktop tasks.

To ensure these powerful agents are deployable and safe, the team proposed GUI-MCP (Model Context Protocol), the first standardized interface for LLM-device interaction built with a hierarchical architecture. This dual-layer system combines low-level atomic operations (like clicks and swipes) with a high-level capability to delegate complex workflows to a local specialist model, such as the compact Step-GUI-4B.

This architecture enables a critical "High Privacy Mode." Consider a user asking an external cloud LLM to "Compare the price of item X across three shopping apps." Instead of sending raw screenshots and detailed device information to the cloud multiple times, the main LLM issues a single `execute_task` command to the local Step-GUI agent on the device. The local agent executes the multi-step search privately, and only sends back a semantic summary (e.g., "JD price: $379, Pinduoduo price: $355"). This ensures that sensitive visual data never leaves the user's device.

Finally, to validate real-world readiness, the researchers introduced AndroidDaily, a new benchmark based on analyzing authentic mobile usage patterns across high-frequency tasks like transportation, shopping, and social media. Testing on AndroidDaily, the Step-GUI-8B achieved an overall end-to-end task completion rate of 52.50%, confirming its robust ability to handle complex, everyday digital interactions—a performance level that establishes new standards for mobile GUI agents.

By addressing data acquisition, privacy, and authentic evaluation simultaneously, Step-GUI provides a comprehensive framework for creating highly capable, commercially practical, and privacy-preserving AI assistants.