---
layout: paper
pdf_url: https://arxiv.org/pdf/2512.10342
permalink: 2025-12-19/2512.10342/
title: New Benchmark Exposes Vision-Language Models’ Critical Flaw in Error-Prone
  Planning
---



A team of researchers has introduced CoSPlan (Corrective Sequential Planning), a new benchmark designed to test Vision-Language Models (VLMs) on their ability to handle and correct errors during multi-step visual tasks—a critical vulnerability for real-world AI applications like robotics and autonomous navigation.

The study, published recently, finds that even state-of-the-art VLMs like GPT-4o struggle significantly when faced with "non-optimal" or erroneous steps embedded in a planning sequence. To address this, the researchers also propose a novel, training-free method called Scene Graph Incremental updates (SGI), which forces models to track evolving scenes step-by-step, substantially improving their performance.

### The Challenge of Corrective Planning

Sequential planning requires an AI agent to execute a series of actions (A1, A2, A3...) to move from an initial state to a specific goal state. Existing benchmarks often assume ideal, error-free instructions, which rarely holds true in practice.

CoSPlan introduces errors into the *initial context*—the sequence of actions already supposedly performed—and then evaluates models on two core abilities:

1.  **Error Detection:** Identifying the single erroneous action in the initial sequence.
2.  **Step Completion:** Correcting the mistake and generating the remaining optimal steps to reach the final goal.

The benchmark spans four diverse vision domains, including Maze Navigation (Maze-E), Block Rearrangement (Blocks-World-E), Image Tile Reconstruction (Shuffle-E), and Real-World Object Reorganization (Robo-VQA-E).

For example, in a *Blocks-World-E* scenario, if the final goal is a stack of four colored blocks, the initial context might contain the sequence: "Move green block to column 1," followed by the error: "Move red block to an impossible position (in the air)." The VLM must detect that "move red block to air" is an invalid step and then provide the correct subsequent actions to successfully build the final stack.

Initial evaluations showed that VLMs struggled severely, often predicting outcomes close to random chance in zero-shot settings. The models failed to leverage the visual context to track the temporal evolution of the scene accurately.

### SGI: Tracking Scenes Incrementally

To improve robustness, the researchers developed SGI (Scene Graph Incremental updates). This technique refines existing Scene Graph (SG) methods—which structurally represent objects and their relationships—by adding a temporal dimension.

Instead of only querying the VLM with the initial and final states, SGI forces the model to perform intermediate reasoning. For every action in the sequence, the VLM must *simulate* that action and generate an updated Scene Graph, thereby creating a verifiable record of the scene’s evolution.

This incremental tracking prevents the model from making long-range, ungrounded guesses. The system then selects the best corrective path by comparing the resulting Scene Graph of each potential action sequence against the goal state, favoring the highest structural similarity.

The introduction of SGI yielded significant gains across all tasks, achieving an average performance boost of approximately 5.2% over vanilla Scene Graph methods in Step Completion. In Error Detection, SGI’s impact was even more pronounced, raising GPT-4o’s accuracy on the Robo-VQA-E task by over 13%.

The findings emphasize that building AI agents capable of handling practical decision-making requires moving beyond ideal planning assumptions and prioritizing techniques, like SGI, that enable robust, step-by-step corrective reasoning in complex visual environments. The code and dataset are being made public to drive further research in error-aware VLM planning.