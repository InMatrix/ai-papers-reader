---
layout: paper
pdf_url: https://arxiv.org/pdf/2512.13106
permalink: 2025-12-19/2512.13106/
title: Breakthrough AI Framework Boosts Reasoning While Cutting Data Labeling Costs
  by 90%
---



A new semi-supervised reinforcement learning framework, dubbed TRAPO (Trajectory-based Policy Optimization), is poised to revolutionize how Large Language Models (LLMs) are trained for complex reasoning tasks. Developed by researchers from Zhejiang University and Ant Group, TRAPO leverages a small set of labeled data to guide self-improvement on massive unlabeled datasets, achieving state-of-the-art performance on mathematical and scientific benchmarks while drastically reducing the need for costly ground-truth labels.

Reinforcement Learning with Verifiable Rewards (RLVR) is the standard for training advanced reasoning models. However, its effectiveness hinges on having vast corpora of problems where the final answer can be externally verified—a prohibitively expensive requirement in specialized fields like medicine or finance.

To bypass this cost, previous efforts focused on unsupervised RLVR, generating internal rewards based on a model's self-consistency, such as entropy or majority voting. For example, if an LLM generates the same answer across ten different reasoning paths, an unsupervised method assumes that answer is correct. The pitfall, however, is that without external correction, the model often falls into a "model collapse," reinforcing internally consistent but fundamentally flawed reasoning patterns.

TRAPO breaks this vicious cycle by introducing a novel guidance mechanism. It treats a small set of meticulously labeled examples as "role models" to anchor robust learning on a large volume of unlabeled data. The core innovation lies in *how* it selects which unlabeled data to trust: by analyzing the model's learning dynamics, specifically its **pass rate trajectories**.

A pass rate trajectory tracks the probability of the model correctly solving a specific problem over the course of training epochs. For labeled samples, this is the true success rate. For unlabeled samples, TRAPO estimates a pseudo-trajectory. TRAPO only selects unlabeled samples for training if their pseudo-trajectory closely matches the reliable learning dynamics observed in the labeled, anchor examples.

To build an intuition, consider training an LLM on complex algebra. A difficult, but solvable, problem will show a trajectory where the model's success rate starts low and steadily climbs. If an unlabeled problem shows a similar, steep improvement curve, TRAPO assumes its intrinsic reward signal is trustworthy and uses it to update the policy. Conversely, an unlabeled problem that the model consistently fails or succeeds on trivially, leading to an erratic or flat trajectory, is filtered out.

The results are striking. Tested on a suite of nine competition-level mathematical and scientific reasoning benchmarks, TRAPO, trained with just 4K labeled samples and 12K unlabeled samples, achieved an average accuracy of 59.7% on out-of-distribution tasks. This performance not only surpasses all unsupervised baselines but also exceeds the fully supervised model trained on the entire 45K labeled dataset—all while utilizing only 10% of the original supervised data.

Furthermore, TRAPO demonstrated robust cross-domain generalization. When trained on labeled math problems but paired with unlabeled data from out-of-domain tasks (like general science Q&A), TRAPO still significantly outperformed naive semi-supervised methods. This highlights its capability to transfer verified reasoning *patterns* rather than simply memorizing answers. By identifying high-quality unlabeled data via trajectory consistency, TRAPO offers a path toward stable, label-efficient, and highly effective LLM self-improvement.