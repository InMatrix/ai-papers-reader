---
layout: paper
pdf_url: https://arxiv.org/pdf/2512.14273
permalink: 2025-12-19/2512.14273/
title: Zoom-Zero&#58; New AI Framework Masters Video Q&A by "Zooming In" on Key Evidence
---



A team of researchers from NVIDIA and KAUST has introduced Zoom-Zero, a novel artificial intelligence framework designed to solve a critical limitation of modern Large Video-Language Models (LVLMs): their inability to precisely pinpoint the visual evidence required to answer questions about a video.

Zoom-Zero employs a reinforced coarse-to-fine strategy that mimics human attention, first identifying the general timeframe relevant to a query and then performing a high-resolution "temporal zoom-in" to confirm the necessary fine-grained visual details.

This new framework significantly advances Grounded Video Question Answering (GVQA)—the complex task that requires an AI to both generate an accurate answer and localize the exact video segment supporting that answer. The model achieved state-of-the-art performance on major benchmarks, boosting temporal grounding accuracy by 5.2% on NExT-GQA and 4.6% on ReXTime.

### The Problem of Lost Detail

Traditional LVLMs often struggle with GVQA, particularly in long videos, due to the computational cost of processing vast amounts of frames. They typically resort to coarse-grained representations, leading to temporal mislocalization and "hallucinations" where the model generates a plausible answer not backed by actual video evidence.

Zoom-Zero tackles this via a two-stage paradigm. In the *coarse pass*, the model examines the entire video sequence to quickly predict query-relevant time intervals. This preserves global context. In the subsequent *fine pass*, the model dynamically reallocates its visual token budget to the identified segment, effectively zooming in with higher temporal and spatial resolution.

For instance, if a user asks a question whose answer depends on a small, rapidly displayed piece of text—like a "29% sales increase" sign—the coarse pass might miss this detail because of low-resolution sampling. The fine pass, however, concentrates high-resolution tokens on the predicted timeframe, capturing the critical visual cue and verifying the correct answer.

### Evidence-Faithful Reinforcement Learning

The success of Zoom-Zero relies on two key innovations applied during reinforcement learning (RL) optimization, which is used to refine the model’s grounding and reasoning skills.

First, the team introduced a **Zoom-in Accuracy Reward** ($R_{zoom}$). This reward is crucial because it ensures that the localized segment *truly* contains the visual evidence. It verifies the final answer only after the model has processed the high-resolution, zoomed-in frames. This mechanism actively trains the model to be "evidence-faithful," preventing it from guessing the right answer based only on context without finding the visual proof.

Second, Zoom-Zero solves a common problem in RL using **Token-Selective Credit Assignment (TokenAdv)**. When multiple goals exist (e.g., precise localization, correct answering, and correct output formatting), standard RL algorithms assign the same credit or penalty uniformly across all generated output tokens. TokenAdv decouples these rewards, assigning credit selectively to the tokens responsible for temporal localization (the timestamp tags) versus those responsible for answer generation. This targeted feedback significantly improves the model's ability to learn from diverse reward signals simultaneously.

The coarse-to-fine paradigm also yielded substantial benefits for long-form video tasks, improving long-video benchmarks by an average of 6.4%, demonstrating a powerful method for capturing critical detail without sacrificing global context.