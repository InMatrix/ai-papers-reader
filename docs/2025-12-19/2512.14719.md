---
layout: paper
pdf_url: https://arxiv.org/pdf/2512.14719
permalink: 2025-12-19/2512.14719/
title: Hybrid AI Explanations Yield More Robust, Trustworthy Small Language Models
---



In a crucial step toward creating more transparent and reliable artificial intelligence, researchers have developed a novel training framework that significantly improves the interpretability and robustness of small language models (SLMs).

SLMs, frequently used for classification tasks like intent detection and sentiment analysis due to their low-latency deployment, traditionally rely on explanation-guided learning to become trustworthy. This technique incorporates attribution—which tokens or words contribute most to a decision—as a form of supervision during training. However, a paper published recently as a preprint reveals that standard attribution methods often fail when faced with closely related semantic categories, a phenomenon the authors dub "homogenization and class confusion."

Current methods, like LIME or Integrated Gradients (IG), reliably highlight class-relevant tokens, but they struggle to identify the *discriminating* features between similar classes. For instance, in an intent classification system distinguishing between a query to check an alarm (`query_alarm`) and a command to set an alarm (`set_alarm`), standard methods might assign high importance to the shared term “alarm.” They neglect the subtle but critical difference: the word "check" versus the word "set." This superficial focus amplifies classification errors.

To address this challenge, the researchers introduce the **Class-Aware Attribution Prior (CAP)** framework. CAP leverages the advanced reasoning capabilities of large language models (LLMs) to automatically generate high-quality, discriminative attribution signals.

Instead of just asking an LLM for a prediction, CAP prompts the LLM with the task instructions and the full label space, forcing it to capture the distinct features separating confusing classes.

"CAP guides the language model to capture fine-grained class distinctions, thereby producing more salient and discriminative attribution priors," the authors explain.

The paper then proposes **$CAP_{Hybrid}$**, a comprehensive supervisory signal that fuses CAP's class-aware insights with existing attribution techniques like LIME and Integrated Gradients. This hybrid approach capitalizes on the complementary strengths of each method: CAP finds the subtle boundary cues, LIME identifies high-affinity tokens, and IG captures the model's internal gradient sensitivity.

When training the SLM, the model’s internal reasoning (its self-attribution scores) is aligned with the richer $CAP_{Hybrid}$ signal. This "attribution alignment" forces the SLM to discard noisy, overlapping keywords and instead focus on the decision-relevant features.

Extensive experiments confirmed the framework’s efficacy across multiple challenging intent classification datasets (HWU64, Banking77, and Clinc150). In the full-data Banking77 setting, $CAP_{Hybrid}$ achieved a test accuracy of 93.51% (up from 92.63% baseline). More dramatically, it boosted robustness against adversarial attacks—where keywords are replaced to mislead the model—improving adversarial accuracy from a baseline of 70.29% to 84.97%.

The results confirm that the hybrid framework not only improves predictive accuracy but also significantly enhances interpretability and stability, particularly in complex, real-world scenarios where fine-grained semantic distinctions are crucial.