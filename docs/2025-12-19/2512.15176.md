---
layout: paper
pdf_url: https://arxiv.org/pdf/2512.15176
permalink: 2025-12-19/2512.15176/
title: Diffusion LLMs Deliver Record Speedup for LLM Inference, Breaking Bottleneck
  of Speculative Decoding
---



In a major leap for large language model (LLM) efficiency, researchers have introduced DEER (Draft with Diffusion, Verify with Autoregressive Models), a novel framework that uses discrete Diffusion LLMs (dLLMs) as highly parallel drafters to accelerate inference. This approach fundamentally overcomes the key limitations of existing speculative decoding methods, achieving significant speedups, particularly on complex tasks like code generation.

While speculative decoding—using a lightweight draft model to propose tokens rapidly, which a larger model verifies in parallel—has been the industry standard for boosting LLM speed, its performance has plateaued. The core limitation lies with the drafters themselves, which are typically smaller autoregressive (AR) models.

As an AR drafter generates tokens sequentially, any minor error or distribution mismatch early in the draft is recursively amplified down the sequence—a phenomenon the researchers term the "gradual collapse of trust." This forces the target model to reject subsequent tokens, sharply limiting the effective acceptance length. Current state-of-the-art methods like EAGLE-3 typically see their acceptance lengths collapse beyond 10 tokens.

DEER bypasses this structural constraint by replacing the AR drafter with a dLLM. Unlike AR models, diffusion models generate an entire block of tokens simultaneously in a single denoising step. This blockwise generation eliminates the left-to-right dependency within the draft segment, effectively preventing early errors from corrupting later tokens.

The results are striking. On the HumanEval code generation benchmark using the powerful Qwen3-30B-A3B model, DEER achieved a 5.54x speedup compared to standard AR decoding, dramatically surpassing EAGLE-3’s 2.41x speedup under the same conditions. Crucially, DEER reliably achieved maximum accepted draft lengths of up to 32 tokens, a four-fold increase over the 7-8 tokens typically accepted by AR-based drafters.

To ensure the dLLM could function reliably as a prefix-conditioned continuation drafter (rather than a global sequence generator), the researchers developed a two-stage Diffusion-to-AR (D2A) Alignment training pipeline. This involves initial AR-style distillation, followed by a refinement stage that uses exponentially weighted loss to stabilize local coherence precisely where the AR verifier begins its job.

Beyond raw speed, the diffusion-based approach demonstrates powerful emergent capabilities, including "reliable block regeneration." For example, when prompted to extend an incomplete code function (like `def quicksort(arr):`), the dLLM can coherently regenerate the necessary continuation block from the local context without needing a full, formal sentence prompt.

By decoupling the drafting process from sequential generation, DEER establishes diffusion models as a practical, highly parallel alternative for dramatically improving LLM inference efficiency and batch throughput, promising major latency reductions for LLM-driven agentic and reasoning systems.