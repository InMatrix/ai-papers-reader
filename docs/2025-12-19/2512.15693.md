---
layout: paper
pdf_url: https://arxiv.org/pdf/2512.15693
permalink: 2025-12-19/2512.15693/
title: Skyra&#58; New AI Detector Uses Physics Violations to Unmask Deepfake Videos
---



As AI-generated video models like Sora-2 and Kling achieve near-photorealistic quality, the potential for widespread misinformation poses a critical threat to social trust. Addressing this urgent challenge, researchers at Tsinghua University have developed **Skyra**, a specialized multimodal large language model (MLLM) designed not just to detect synthetic videos, but to explain exactly *why* they are fake.

Unlike traditional binary classifiers that simply output "real" or "fake," Skyra focuses on **Grounded Artifact Reasoning**, identifying human-perceivable visual discrepancies that violate physical laws or common sense.

Current detection systems often fail because they rely on superficial cues like texture or visual quality. Skyra, however, employs a sophisticated, hierarchical artifact taxonomy that guides its reasoning toward intrinsic, model-agnostic flaws. This taxonomy divides errors into two main categories: Low-Level Forgery (perceptual flaws like unnatural blur) and **Violation of Laws** (inconsistencies in physics, causality, or object permanence).

### Finding the Flaws Humans See

The power of Skyra lies in its ability to pinpoint subtle, spatio-temporal inconsistencies that betray the generative model's limitations.

For instance, in a challenging sample featuring a bartender making a drink, traditional MLLMs might conclude the video is real based on "smooth movements" and "consistent lighting." Skyra, conversely, identifies a violation of physical laws:

In its detailed, step-by-step reasoning, Skyra tags the moment a metal cocktail shaker is lifted. It notes that the shaker's "shape begins to warp and deform... as if it were made of a soft, malleable material" (a **Shape Distortion** artifact). Furthermore, as the bartender finishes the pour, Skyra localizes a "dark, metallic residue... which looks like a small pool of liquid or a solid block, appears in his right hand," materializing out of thin air (an **Abnormal Object Appearance** artifact).

By grounding its conclusion in these frame-by-frame, physics-violating anomalies, Skyra confidently labels the video as fake and provides concrete, timestamped evidence.

### Specialized Training and a New Benchmark

To achieve this level of explainable accuracy, the researchers introduced the **ViF-CoT-4K** dataset, the first large-scale artifact dataset featuring fine-grained human annotations, including specific spatio-temporal bounding boxes for each forgery clue.

Skyra is trained using a unique two-stage strategy. First, Supervised Fine-Tuning (SFT) on ViF-CoT-4K endows the model with basic artifact perception. Second, a Reinforcement Learning (RL) procedure enhances the model's self-driven ability to mine discriminative artifacts and prevents it from falsely flagging benign imperfections in real videos.

Evaluated on the new **ViF-Bench** benchmark—which includes 3,000 high-quality samples generated by over ten state-of-the-art models (including Sora-2, Kling, and LTX-Video)—Skyra demonstrated substantial improvements. It achieved an absolute accuracy gain of **+26.73%** and a 32% F1-score boost compared to leading MLLM-based detection methods.

The development of Skyra and its accompanying resources offers a robust, transparent new path for AI safety, providing journalists, fact-checkers, and regulators with essential, grounded evidence to combat the rapidly evolving threat of hyper-realistic synthetic media.