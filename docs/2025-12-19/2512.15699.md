---
layout: paper
pdf_url: https://arxiv.org/pdf/2512.15699
permalink: 2025-12-19/2512.15699/
title: New AI Benchmark Reveals Frontier LLMs Fail at Open-Ended Computer Science
  Challenges
---



A comprehensive new benchmark, FrontierCS, reveals that even the most advanced large language models (LLMs) struggle profoundly with real-world computer science problems that lack a known optimal solution. Unlike traditional coding tests, FrontierCS focuses on "open-ended optimization problems" requiring strategic algorithmic design and nuanced trade-offs, areas where human experts currently outperform LLMs by a wide margin.

Developed by a coalition of computer scientists from institutions including UC Berkeley, Princeton, and MIT, FrontierCS consists of 156 tasks across two tracks: Algorithmic Problems (like packing and routing) and Research Problems (spanning Operating Systems, AI, and Security). The key departure from previous benchmarks is the evaluation methodology: models must generate executable programs, and the quality of the output is measured by a quantitative score (0-100) rather than a simple pass-or-fail criterion.

The researchers found a substantial performance gap between AI and human expertise. On the Algorithmic track, human experts achieved an average score of 95.41, while the top-performing LLM, Gemini 3.0 Pro, scored only 29.37.

To understand the nature of the difficulty, consider the **Polyomino Packing** task, where the goal is to pack various block shapes into a grid to maximize density. Since the optimal packing arrangement is unknown, solutions are judged on the resulting density. In a test instance, the human expert achieved an efficient packing density of 87%. In contrast, GPT-5 produced a valid but suboptimal arrangement, achieving only 47% density, leaving significant empty space.

Similarly, in **Treasure Packing**, a variant of the NP-hard knapsack problem requiring maximizing value subject to mass and volume constraints, exact solutions are computationally infeasible. The best LLM solution scored 74 points using a branch-and-bound algorithm—a respectable effort, but still falling short of the human expert’s enhanced greedy/randomized approach.

The benchmark also includes highly complex research challenges, such as **Vector Database Design** (optimizing the latency-recall tradeoff for approximate nearest-neighbor search) and **Symbolic Regression** (finding the simplest mathematical formula that fits a dataset). Even on these complex, real-world tasks, LLMs demonstrated limitations.

The analysis highlighted two critical failure modes for current frontier models. First, simply increasing the computational resources, or "reasoning budget," often provided diminishing returns, failing to significantly close the gap. Second, models frequently fell into a "micro-optimization trap." For instance, when solving Polyomino Packing, models often focused on transformation lists for pieces, a low-impact structural detail, rather than prioritizing the high-level strategy needed for effective free-space search.

These results suggest that current LLMs, while adept at generating workable code for closed-form problems, lack the deep, open-ended reasoning and strategic creativity required to discover truly high-quality, efficient algorithms and system designs—skills that remain firmly at the frontier of human computer science expertise. FrontierCS offers a robust and adaptable framework for measuring future progress as models attempt to bridge this significant gap.