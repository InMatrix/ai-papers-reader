---
layout: paper
pdf_url: https://arxiv.org/pdf/2512.14681
permalink: 2025-12-19/2512.14681/
title: Progressive Distillation Method “Jacobi Forcing” Delivers 4x Speedup to LLM
  Inference
---



In a significant advance for large language model (LLM) efficiency, researchers have introduced "Jacobi Forcing," a progressive distillation technique that successfully transforms traditional high-quality autoregressive (AR) models into highly efficient parallel decoders. The resulting Jacobi Forcing Model achieves up to 4x wall-clock speedup on coding and mathematical tasks while maintaining competitive accuracy, overcoming a key limitation plaguing current acceleration methods.

The fundamental challenge in LLM performance is latency. AR models, like GPT-4, generate tokens sequentially, limiting parallelism. Recent efforts have focused on diffusion-based LLMs (dLLMs), which decode sequences in parallel. However, adapting AR models into dLLMs often sacrifices generation quality due to a "pretrain-to-posttrain mismatch," where the artificial masked data distribution used for training conflicts with the model’s original causal structure.

Jacobi Forcing addresses this by entirely avoiding changes to the model’s attention mechanism, preserving its pretrained causal inference property. Instead, it employs a progressive distillation paradigm, training the AR model on its own generated parallel decoding trajectories, known as Jacobi Decoding.

To build intuition, consider how an LLM decodes a block of 32 tokens in parallel. In traditional parallel approaches, the model struggles to predict correct tokens at the end of the block because the tokens it just predicted earlier in that same block are still "unconverged" or "noisy." Jacobi Forcing uses a novel noise-aware causal attention and a progressive noise schedule to train the model specifically to handle this uncertainty. It forces the model to accurately predict future tokens even when conditioned on a noisy (unconverged) preceding context. This progressive challenge smoothly shifts the model into an efficient parallel decoder.

Qualitative analysis shows that the Jacobi Forcing Model excels at generating "fixed-point segments"—sequences of correct tokens—much deeper into the draft block compared to baseline models. This inherent improvement in draft quality allows the researchers to deploy sophisticated inference optimizations.

The paper introduces two key inference techniques that maximize speedup: rejection recycling and multi-block decoding. Rejection recycling capitalizes on the high-quality drafts by reusing consecutive correct tokens from rejected drafts to build new candidate sequences for immediate verification. For instance, if a draft predicts 10 tokens but only the first four are verified, the remaining rejected tokens may still contain valuable $n$-grams that can be used to instantly draft the next iteration.

Multi-block decoding further enhances parallelism by maintaining and refining several token blocks simultaneously, allowing subsequent blocks to begin refining drafts even while preceding blocks are still converging.

Experiments on coding benchmarks (HumanEval and MBPP) demonstrate that the Jacobi Forcing Model achieves 3.8x speedup over the AR baseline with a minimal drop in accuracy. When integrated with rejection recycling and multi-block decoding, the speedup jumps to nearly 4.0x (163.9 tokens per second on A100 GPUs for HumanEval), significantly outperforming optimized diffusion-based LLMs, which struggle to maintain high accuracy and speed simultaneously.