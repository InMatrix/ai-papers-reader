---
layout: paper
pdf_url: https://arxiv.org/pdf/2502.18906
permalink: 2025-02-28/2502.18906/
title: VEM&#58; AI Agent Navigates GUIs with Human-Like Reasoning, No Environment
  Needed
---



Researchers at Peking University, Microsoft, and the Royal Institute of Technology have developed a novel AI framework, VEM (Value Environment Model), that enables GUI agents to interact with graphical user interfaces (GUIs) without the need for costly real-time interactions with the environment. This innovation tackles a major challenge in training AI agents to automate tasks on devices, potentially revolutionizing how we interact with technology.

Traditional Reinforcement Learning (RL) methods require the AI to learn by repeatedly interacting with the environment, which can be time-consuming and expensive, especially when dealing with complex interfaces. Environment-free approaches, on the other hand, often suffer from distribution shifts and difficulty generalizing to new scenarios.

VEM sidesteps these limitations by decoupling value estimation (assessing the usefulness of an action) from policy optimization (deciding which actions to take). It leverages a pre-trained "Value Environment Model" that predicts the long-term utility of actions based on offline data of human GUI interactions.

**How VEM Works:**

Imagine you want an AI to find the latest video from GameSpot Reviews on YouTube (see Figure 1 in the research paper). Instead of letting the AI fumble around blindly, VEM guides it with a sense of which actions are likely to be helpful.

1.  **VEM Pre-training:** The VEM is first trained on a large dataset of human-GUI interactions, learning to predict the "value" of different actions in various states. For instance, clicking the 'share' button probably isn't the right step to show reviews, but searching by typing "GameSpot Reviews" probably is. This pre-training instills a kind of "common sense" about GUI interactions.

2.  **Policy Exploration with Frozen VEM:** During the actual task, the pre-trained VEM guides the agent's actions, iteratively refining the policy's action selection. The AI explores actions, but not randomly. It's guided by the frozen VEM's understanding of GUI semantics, helping it to generalize across unseen layouts and functionalities without online interaction.

**Concrete Examples:**

*   Instead of predicting the pixel-level changes after clicking a button (which can be prone to errors due to even minor UI changes), VEM focuses on semantic reasoning. It asks: "Does this action advance the user's goal?" This high-level perspective makes the agent more robust to UI variations.

*   In navigating an Android app, VEM can discern whether pressing the "back" button is a useful step in a specific task, even if it hasn't seen that exact app layout before. This is because the VEM understands the general concept of "going back" within a GUI, making it more adaptable than a system reliant on pixel-perfect pattern matching.

**Results:**

The researchers evaluated VEM on the challenging Android-in-the-Wild benchmark, demonstrating significant performance gains in both offline and online settings. VEM achieves state-of-the-art performance in both offline and online settings, surpassing environment-free baselines significantly. For example, in online deployment, VEM achieves 42.4% general task success which surpasses other similar method which scores 14-43%.

This work offers a compelling new direction for training GUI agents, paving the way for more robust, efficient, and cost-effective solutions for automating tasks across diverse digital interfaces. The code and project page are publicly available, promising further advancements in this exciting area.