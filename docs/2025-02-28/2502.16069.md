---
layout: paper
pdf_url: https://arxiv.org/pdf/2502.16069
permalink: 2025-02-28/2502.16069/
title: Curie&#58; AI Agent Framework Brings Rigor and Automation to Scientific Experimentation
---



A team of researchers has unveiled "Curie," a novel AI agent framework designed to inject rigor into the traditionally painstaking and often error-prone process of scientific experimentation. The tool aims to make experiments more reliable, methodically controlled, and easily interpretable.

Scientific experimentation relies on a rigorous procedure that includes formulating hypotheses, designing controlled experiments, executing those experiments, and analyzing the results. The traditional approach is often hampered by ad-hoc methods, prone to errors, and difficult to reproduce. This is the gap that Curie aims to fill.

Curie works by employing three key components. The first is an "intra-agent rigor module" which enforces internal reliability within individual AI agents. An example of this module would be a validation step that ensures an experiment plan aligns with its stated objectives and can be reproduced consistently. The second component is an "inter-agent rigor module", which is responsible for methodical control between agents, carefully managing the sequencing and scheduling of their tasks. Finally, the "experiment knowledge module" enhances interpretability by automatically maintaining well-structured documentation of the entire experimental process.

To test Curie, the researchers developed a new benchmark consisting of 46 experimental tasks across four computer science domains. These tasks were derived from influential research papers and open-source projects. In one example, Curie successfully reproduced, expanded upon, and even challenged existing research on the use of repeated sampling in large language model reasoning. Curie was able to confirm the original findings but also identified a limitation in the initial study's methodology.

Researchers benchmarked Curie against other state-of-the-art AI agents, including OpenHands and Microsoft Magentic. Results showed that Curie demonstrated a 3.4x improvement in correctly answering experimental questions compared to the strongest baseline.

For example, when tasked with optimizing machine learning training pipelines, existing agents often struggled to adapt to unexpected outcomes or maintain detailed documentation, whereas Curieâ€™s built-in validation and structured documentation ensured that the process adhered to experimental principles. In another test, Curie successfully optimized distributed setups for cloud computing environments, dynamically adjusting experimental workflows based on partial results. This is a difficult task that requires a robust execution tracking system, and this is exactly the type of task for which the inter-agent rigor module is designed.

The paper concludes that Curie provides a promising foundation for accelerating scientific research, especially in computer science. The authors also envision a wider applicability of their framework beyond computer science, potentially impacting fields like materials science and drug discovery. Curie is open-sourced and available on GitHub.