---
layout: paper
pdf_url: https://arxiv.org/pdf/2510.25039
permalink: 2025-10-31/2510.25039/
title: Automating Benchmark Design with LLM-Driven Iteration
---



The rapid advancement of large language models (LLMs) and their increasing use in complex tasks, such as agentic reasoning and planning, have outpaced our ability to effectively evaluate their capabilities. Traditional, static benchmarks quickly become saturated as models improve, making it difficult to differentiate between state-of-the-art systems. While dynamic benchmarks that evolve over time offer a solution, their creation and maintenance are often labor-intensive.

A new framework called **BeTaL (Benchmark Tuning with an LLM-in-the-loop)** has been developed to address this challenge. BeTaL automates the process of designing dynamic benchmarks by leveraging the reasoning power of LLMs. The core idea is to parameterize the design of benchmarks, allowing an LLM to iteratively adjust these parameters based on feedback from evaluating a target model. This creates a closed-loop system that tunes benchmarks to achieve specific target properties, such as difficulty.

**How BeTaL Works: An Iterative Design Process**

Imagine you want to create a benchmark that tests an LLM's ability to solve math problems with a specific level of difficulty. BeTaL starts with an underspecified description of this benchmark. An LLM, acting as a "designer," is prompted to propose initial parameter values for this benchmark. For example, in an arithmetic sequence task, parameters could include the types of operations allowed (add, subtract, multiply, divide, etc.), the length of the sequence, and the range of numbers used.

Once the designer LLM proposes parameters, a simulator generates actual benchmark problems based on these settings. Then, a "target model" (another LLM or AI agent) attempts to solve these problems. The performance of the target model (e.g., its accuracy or success rate) is then fed back to the designer LLM.

The designer LLM analyzes this feedback, understanding what aspects of the generated benchmark made it too easy or too difficult for the target model. It then refines its parameter suggestions in subsequent iterations, aiming to bring the target model's performance closer to the desired level. This iterative process continues until the benchmark meets the specified criteria.

**Concrete Examples of BeTaL in Action**

The researchers demonstrated BeTaL's effectiveness across three distinct domains:

*   **Arithmetic Sequences:** This involves generating math problems where an agent must determine a sequence of arithmetic operations to transform an input number into a target output. For instance, starting with the number 5, the agent might need to figure out if the sequence is "multiply by 2, then add 3" to reach 13. BeTaL can tune parameters like the complexity of operations or the length of the sequence to create problems that are neither too simple nor impossibly hard for the LLM.

*   **Spatial Reasoning:** In this domain, benchmarks involve tasks like moving and rotating objects on a grid. Imagine an agent needs to describe the final position and orientation of a particle on a chessboard after a series of moves and rotations. BeTaL can adjust parameters like the board size, the allowed movements and rotations of particles and the board, to create scenarios that precisely test an LLM's spatial understanding.

*   **T-bench Airline Environment:** This benchmark simulates tasks related to airline booking. An LLM agent might need to find flights, book tickets, and manage reservations based on user requests and constraints. BeTaL can adjust parameters within this environment, such as the number of passengers, the complexity of routing options, or the clarity of preferences, to generate tasks that challenge the agent's planning and decision-making abilities.

**Key Contributions and Results**

The paper highlights several key findings:

*   **Improved Benchmark Design:** BeTaL consistently produces benchmarks with performance gaps between the target difficulty and the model's actual performance that are 2-4 times smaller than those achieved by baseline methods.

*   **LLMs as Designers:** The framework demonstrates that LLMs, with their advanced reasoning capabilities, can effectively act as automated benchmark designers.

*   **Transferability:** Benchmarks designed for one target model can often be used effectively to evaluate other models, suggesting that BeTaL creates robust measures of cognitive capabilities.

*   **Limitations:** While powerful, BeTaL still relies on parameterized and verifiable task generators, which may not always be available. Additionally, the effectiveness of the designer LLM depends on its reasoning strength and the quality of prompts.

In essence, BeTaL represents a significant step towards creating adaptive evaluation systems that can keep pace with the rapid evolution of AI models, ensuring more meaningful and accurate assessments of their capabilities.