---
layout: paper
pdf_url: https://arxiv.org/pdf/2505.14157
permalink: 2025-05-23/2505.14157/
title: Reinforcement Fine-Tuning of Language Models Gets a Prompt Engineering Boost
---



A new research paper has revealed that "prior prompt engineering" (pPE) - carefully designing the instructions given to a language model *before* reinforcement fine-tuning (RFT) - can significantly improve a model's performance and steer its behavior. This could open new avenues for creating specialized language models with greater control and efficiency.

RFT is a technique used to train language models to perform extended reasoning by rewarding them for correct answers. While previous RFT research has focused on algorithms, reward structures, and data curation, this study, by Taveekitworachai et al., zeroes in on the *prior prompt*, the initial instruction given to the model. They ask: Can different types of pPE cause language models to learn different behaviors *during* RFT?

Imagine teaching a child math: you could just say "Solve this problem," or you could add "First, let's think step by step." The researchers hypothesized that, just like different instructions can guide human learners, carefully crafted prompts could guide language models during RFT.

The researchers translated five established "inference-time prompt engineering" (iPE) strategies – methods for getting better outputs from a model *at test time* – into pPE approaches. These were:

*   **Reasoning:** Guiding the model to produce step-by-step reasoning. The model is prompted to "think" through the problem.
*   **Planning:** Asking the model to generate a high-level plan before solving the problem. The model is asked to make a "plan."
*   **Code-based Reasoning:** Encouraging the model to write code to solve the problem.
*   **Knowledge Recall:** Asking the model to recall or synthesize relevant knowledge before answering. The model is directed to use "<knowledge>."
*   **Null-Example Utilization:** Prompting the model to consider non-existent examples (a kind of thought experiment) related to the problem.

They then trained the Qwen2.5-7B language model using each of these pPE approaches, rewarding the model not only for the correct answer, but also for following the prompt's format. They evaluated performance using mathematical reasoning, coding, and question-answering benchmarks.

The results were striking. Models trained with *any* of the pPE strategies outperformed their counterparts which used those strategies only during inference. More surprisingly, the "null-example utilization" pPE approach - in which the model was primed to use examples - yielded the *highest* average performance gain, even surpassing the commonly-used "reasoning" approach. This suggests that less obvious strategies can be powerful.

The different pPE approaches instilled distinct "behavioral styles" in the models. For instance, models trained with the planning-focused pPE approach tended to generate numbered lists of steps before executing them. Interestingly, a model trained with "null example utilization" developed a style close to reasoning.

These findings highlight pPE as a powerful but understudied tool for influencing RFT. By carefully designing the initial instructions, researchers can guide language models to internalize specific behaviors, leading to more efficient and effective performance. This could pave the way for creating specialized language models tailored for specific tasks.

The study authors are careful to point out that RFT carries some risks and recommend safeguards like prompt auditing and robust reward design.