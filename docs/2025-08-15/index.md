---
layout: default
title: 2025-08-15
permalink: /2025-08-15/
---

# 2025-08-15

## AI for Software Development

### VisCodex: Unified Multimodal Code Generation via Merging Vision and Coding Models

**Relevance:** This paper introduces VisCodex, a framework for generating code from multimodal inputs, crucial for advancing AI's role in software development beyond just text-based interactions. From an HCI perspective, providing visual context (e.g., screenshots, diagrams) for code generation tools could significantly enhance developer productivity and reduce cognitive load by allowing more natural specification of desired functionality, moving towards more intuitive and less ambiguous interaction paradigms for programming assistants.

ðŸ’¡ **[Summary](2508.09945/)** ðŸ“„ **[Full paper](https://arxiv.org/pdf/2508.09945)**

### ASTRA: Autonomous Spatial-Temporal Red-teaming for AI Software Assistants

**Relevance:** ASTRA addresses the critical safety and security concerns of AI coding assistants like GitHub Copilot by autonomously red-teaming them to uncover vulnerabilities. For HCI, this is vital for building trust and ensuring the safe deployment of AI tools in sensitive software development contexts. Understanding and mitigating these flaws directly impacts user safety and reliability, preventing AI-introduced bugs or security risks that could erode developer confidence and adoption.

ðŸ’¡ **[Summary](2508.03936/)** ðŸ“„ **[Full paper](https://arxiv.org/pdf/2508.03936)**

### Technical Report: Full-Stack Fine-Tuning for the Q Programming Language

**Relevance:** This paper details an approach to fine-tune LLMs for niche programming languages like Q, significantly improving their performance beyond general-purpose models. From an HCI viewpoint, this demonstrates how AI tools can be tailored to specific user communities (e.g., quantitative finance professionals using Q). This specialized adaptation enhances the utility and usability for experts in niche domains, making AI assistants more relevant and effective for a broader range of software development tasks and users.

ðŸ’¡ **[Summary](2508.06813/)** ðŸ“„ **[Full paper](https://arxiv.org/pdf/2508.06813)**

## AI Agents

### Seeing, Listening, Remembering, and Reasoning: A Multimodal Agent with Long-Term Memory

**Relevance:** This work introduces M3-Agent, a multimodal agent capable of processing real-time visual and auditory inputs, building long-term memory, and performing multi-turn reasoning to accomplish tasks. This has strong implications for HCI by enabling more human-like interactions with autonomous agents, as memory and multimodal perception are crucial for sustained, context-aware collaboration. Users could experience agents that better understand complex instructions, learn from past interactions, and adapt their behavior over time.

ðŸ’¡ **[Summary](2508.09736/)** ðŸ“„ **[Full paper](https://arxiv.org/pdf/2508.09736)**

### AWorld: Dynamic Multi-Agent System with Stable Maneuvering for Robust GAIA Problem Solving

**Relevance:** AWorld presents a robust multi-agent system where LLM-powered agents collaborate and dynamically verify reasoning to solve complex problems. This directly relates to HCI by exploring how multi-agent systems can enhance reliability and trustworthiness in achieving user-defined goals. The concept of a 'Guard Agent' for error correction has implications for designing transparent and dependable AI teams, allowing for safer and more predictable human-agent collaboration in high-stakes problem-solving scenarios.

ðŸ’¡ **[Summary](2508.09889/)** ðŸ“„ **[Full paper](https://arxiv.org/pdf/2508.09889)**

### OpenCUA: Open Foundations for Computer-Use Agents

**Relevance:** OpenCUA provides an open-source framework for developing computer-use agents (CUAs) that automate diverse computer tasks across operating systems and applications. This is highly relevant to HCI as it directly impacts how users interact with and delegate tasks to AI. Enabling agents to control digital environments raises questions of user control, transparency of actions, and the design of intuitive interfaces for managing agent autonomy and intervening when necessary to ensure user alignment.

ðŸ’¡ **[Summary](2508.09123/)** ðŸ“„ **[Full paper](https://arxiv.org/pdf/2508.09123)**

## LLM Evaluation Methods

### MathReal: We Keep It Real! A Real Scene Benchmark for Evaluating Math Reasoning in Multimodal Large Language Models

**Relevance:** MathReal introduces a crucial benchmark for evaluating multimodal LLMs on mathematical reasoning using real-world, noisy images from K-12 educational settings. This directly addresses an HCI concern about the robustness and practical usability of LLMs outside of clean, idealized datasets. By identifying how models fail under realistic conditions, it provides essential insights for improving user satisfaction and ensuring that MLLMs are truly effective and reliable for everyday users.

ðŸ’¡ **[Summary](2508.06009/)** ðŸ“„ **[Full paper](https://arxiv.org/pdf/2508.06009)**

### Putnam-AXIOM: A Functional and Static Benchmark

**Relevance:** Putnam-AXIOM offers a new, contamination-resilient benchmark for evaluating LLMs on advanced mathematical reasoning, featuring dynamically generated problem variations. This is vital for robust LLM evaluation, ensuring that reported performance reflects genuine reasoning abilities rather than memorization, thereby increasing trust in model capabilities. From an HCI perspective, it contributes to reliable assessments of an LLM's 'intelligence' and provides a more accurate understanding of its limitations, guiding user expectations.

ðŸ’¡ **[Summary](2508.08292/)** ðŸ“„ **[Full paper](https://arxiv.org/pdf/2508.08292)**

### BiasGym: Fantastic Biases and How to Find (and Remove) Them

**Relevance:** BiasGym provides a framework for systematically injecting, analyzing, and mitigating conceptual biases in LLMs. This is directly relevant to LLM evaluation from an ethical and fairness perspective, which is paramount in HCI. By enabling reliable bias elicitation and targeted debiasing, it helps ensure that LLM outputs are fair, inclusive, and trustworthy for all users, mitigating potential harm and improving user acceptance and societal impact.

ðŸ’¡ **[Summary](2508.08855/)** ðŸ“„ **[Full paper](https://arxiv.org/pdf/2508.08855)**

## Reinforcement Learning

### Cooper: Co-Optimizing Policy and Reward Models in Reinforcement Learning for Large Language Models

**Relevance:** Cooper proposes an RL framework that jointly optimizes both the policy and reward models for LLMs, addressing critical issues like reward hacking and lack of robustness. From an HCI perspective, this research is fundamental to developing more aligned and trustworthy LLMs. By improving the reliability of reward mechanisms, it directly contributes to safer and more predictable AI behaviors, enhancing user trust and confidence in systems trained with RL, especially for complex reasoning tasks.

ðŸ’¡ **[Summary](2508.05613/)** ðŸ“„ **[Full paper](https://arxiv.org/pdf/2508.05613)**

### Train Long, Think Short: Curriculum Learning for Efficient Reasoning

**Relevance:** This paper introduces a curriculum learning strategy for training LLMs with RL (GRPO) to achieve concise yet accurate reasoning. This directly impacts HCI by aiming to reduce the verbosity and 'filler' content often seen in LLM outputs, which can increase cognitive load for users. By teaching models to 'think less' at inference time, it contributes to more efficient and user-friendly interactions, allowing for quicker comprehension and more direct responses.

ðŸ’¡ **[Summary](2508.08940/)** ðŸ“„ **[Full paper](https://arxiv.org/pdf/2508.08940)**

### Feedback-Driven Tool-Use Improvements in Large Language Models via Automated Build Environments

**Relevance:** This work develops an RL framework to improve LLM tool-use capabilities by providing detailed, verifiable feedback in automated environments. For HCI, effective tool use is crucial for empowering AI agents to accomplish user goals in complex digital ecosystems. This research enhances the reliability and precision of LLM interactions with external tools, which is vital for designing human-agent collaboration where users delegate tasks to agents that effectively manipulate digital resources.

ðŸ’¡ **[Summary](2508.08791/)** ðŸ“„ **[Full paper](https://arxiv.org/pdf/2508.08791)**

## Explainable AI

### Can LLM-Generated Textual Explanations Enhance Model Classification Performance? An Empirical Study

**Relevance:** This paper investigates the use of LLM-generated textual explanations to improve model performance and enrich datasets. It directly addresses a core aspect of XAI: producing human-readable rationales. From an HCI viewpoint, the quality and effectiveness of these automated explanations are paramount for user understanding, trust, and decision-making. High-quality explanations can make complex AI systems more transparent and usable, empowering users to better interpret and act upon model predictions.

ðŸ’¡ **[Summary](2508.09776/)** ðŸ“„ **[Full paper](https://arxiv.org/pdf/2508.09776)**

### Mol-R1: Towards Explicit Long-CoT Reasoning in Molecule Discovery

**Relevance:** Mol-R1 focuses on enhancing the explainability and reasoning performance of Explicit Long Chain-of-Thought (CoT) reasoning models for knowledge-intensive domains like molecule discovery. CoT is a prominent XAI technique for LLMs, aiming to make their internal thought processes more transparent. For HCI, providing such detailed, explicit reasoning traces can significantly improve user understanding of how complex decisions (e.g., in scientific discovery) are made, fostering trust and enabling better human oversight and collaboration.

ðŸ’¡ **[Summary](2508.08401/)** ðŸ“„ **[Full paper](https://arxiv.org/pdf/2508.08401)**

### BiasGym: Fantastic Biases and How to Find (and Remove) Them

**Relevance:** While primarily focused on bias mitigation, BiasGym's 'BiasScope' component directly contributes to Explainable AI by identifying and steering the internal model components responsible for biased behavior. This capability offers interpretability into the source of biases, which is crucial for building transparent and trustworthy AI systems. Understanding *why* a model exhibits certain biases enables more targeted interventions and can lead to more insightful explanations for users regarding model fairness and limitations.

ðŸ’¡ **[Summary](2508.08855/)** ðŸ“„ **[Full paper](https://arxiv.org/pdf/2508.08855)**

