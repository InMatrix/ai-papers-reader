---
layout: paper
pdf_url: https://arxiv.org/pdf/2508.09776
permalink: 2025-08-15/2508.09776/
title: Can AI-Generated Explanations Boost AI Performance?
---



In the rapidly advancing field of Natural Language Processing (NLP), making AI models understandable is crucial for trust and transparency, especially in sensitive areas. Traditionally, this has been achieved by having humans write "textual explanations" â€“ human-like rationales behind an AI's decision. However, this process is costly and slow, limiting the scale and quality of the data.

A new study from the Technical University of Munich proposes an automated solution: using large language models (LLMs) to generate these explanations. The researchers developed a framework that leverages multiple state-of-the-art LLMs to create high-quality textual explanations. They then rigorously evaluated these AI-generated explanations and, crucially, investigated how they impact the performance of other AI models on Natural Language Inference (NLI) tasks. NLI involves determining the relationship between two pieces of text, like a premise and a hypothesis.

The study found that AI-generated explanations can be surprisingly effective. When used to train or guide other AI models, these LLM-generated rationales demonstrated performance that was highly competitive with human-written explanations. In some cases, they even led to improvements.

To illustrate, imagine an AI model needs to determine if the statement "A man is leaning against a pay phone while reading a paper" implies "The man is standing and holding a newspaper." A human explanation might say: "If the man is reading a paper, he is reading a newspaper." The study explored how LLM-generated explanations, such as "The premise states the man is leaning against a pay phone, which implies he is standing, and reading a paper suggests he is holding it," can help other AI models perform this task more accurately.

The research tested this approach on two datasets: e-SNLI, which provides explanations for NLI tasks, and HealthFC, which involves verifying health claims using evidence. The findings suggest that automated explanation generation is a promising avenue for creating more comprehensive datasets and enhancing the performance of AI models in NLP tasks. This could accelerate the development of more trustworthy and capable AI systems.