---
layout: paper
pdf_url: https://arxiv.org/pdf/2602.02554
permalink: 2026-02-06/2602.02554/
title: BatCoder Uses Back-Translation to Train Code LLMs Without Paired Data
---



A team of researchers has introduced BatCoder, a novel self-supervised reinforcement learning framework that significantly boosts the performance of large language models (LLMs) on coding tasks by circumventing a critical training bottleneck: the scarcity of high-quality code-documentation pairs.

Traditionally, training robust LLMs for tasks like translating natural language into code requires massive, perfectly aligned datasets where code snippets are meticulously paired with descriptive documentation. Curating these pairs is expensive, especially for niche or low-resource programming languages.

BatCoder, developed by researchers at Fudan University, solves this by employing a sophisticated “back-translation” strategy that uses only raw, unlabeled code snippets for training. This technique allows the model to generate its own implicit supervision signal, dramatically expanding the available training data.

### The Self-Supervised Loop

The core idea of BatCoder is a two-stage cycle designed to enforce structural and semantic faithfulness between code and documentation.

1.  **Stage 1 (Code-to-Documentation):** Given an unlabeled code snippet (the original input), the LLM generates a natural language description or documentation.
2.  **Stage 2 (Documentation-to-Code):** The model then uses this *generated* documentation to try and reconstruct the *original* code snippet.

The key breakthrough lies in the reward system. The semantic similarity between the original code and the final reconstructed code serves as an implicit reward signal. If the model generates high-quality, detailed documentation in Stage 1, it enables a faithful reconstruction in Stage 2, resulting in a high reward that reinforces the entire bidirectional transformation process. If the documentation is vague, the reconstruction fails, yielding a low reward.

### Intuition Through Fidelity

To understand the mechanism, consider a complex function, such as one managing a multi-step user login process that involves visiting a login page, filling out form fields, and clicking a submit button.

A conventionally trained base model might generate generic documentation: "This function handles user sign-in." Using this vague description, the reconstruction phase would likely fail to replicate the exact multi-step control flow of the original code.

In contrast, BatCoder is rewarded for generating highly detailed, procedural documentation—for instance: "This function first navigates to the login path, then fills the email and password fields, and finally clicks the 'Log in' button." This rich documentation provides strong constraints, ensuring that the reconstructed code accurately mirrors the original function's structure. This successful reconstruction yields a high reward, implicitly teaching the model how to produce superior, semantically aligned documentation without ever relying on external human-written pairs.

### Performance Gains Across Languages

Evaluated using a 7-billion parameter model, BatCoder achieved a pass@1 score of 83.5% on the HumanEval benchmark and 81.0% on MBPP, outperforming substantially larger open-source baselines, including models over four times its size.

The framework proved particularly transformative for low-resource languages. In experiments on Ruby, where high-quality paired data is exceptionally scarce, the BatCoder 3B model raised the pass@1 score from 0.0% (a complete failure for the base model) to 10.6%. Similar improvements were seen in Go.

This success in data-constrained environments highlights BatCoder’s potential to democratize robust code LLM training across a wider spectrum of programming languages, offering a stable and effective self-supervised learning signal directly from unlabeled code corpora. The researchers believe the framework’s consistent scaling behavior suggests it could yield even greater performance benefits when applied to larger training datasets and model capacities.