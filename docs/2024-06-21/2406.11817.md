---
layout: paper
pdf_url: https://arxiv.org/pdf/2406.11817
permalink: 2024-06-21/2406.11817/
title: 7B Language Model Achieves GPT-4 Performance by Tackling AI Verbosity
---



Researchers have introduced a novel refinement to alignment training, allowing a modest 7-billion parameter language model (LLM) to perform on par with advanced models like GPT-4, critically while avoiding the common pitfall of excessive verbosity.

The new technique, called Iterative Length-Regularized Direct Preference Optimization (iLR-DPO), directly addresses a flaw in standard iterative alignment methods: the tendency of models to generate overly long, complex, and resource-intensive responses in pursuit of higher quality.

The challenge lies in Direct Preference Optimization (DPO), a standard technique for fine-tuning LLMs based on human feedback. Recent work has shown that DPO is highly effective when applied iteratively, where the model generates new responses in a loop, and a powerful reward model (RM) ranks these responses to create fresh preference data for the next training cycle. This iterative DPO (iDPO) successfully boosts quality, but the researchers found it simultaneously drives up response length.

As the model improves, it learns that longer responses are often rated higher by the automated judge, leading to a kind of “digital logorrhea”—answers filled with meaningless repetition and unnecessary detail. For example, during testing, the length of responses generated by a vanilla iDPO-trained model ballooned to nearly 5,600 tokens by the third iteration—about three times the average length of a GPT-4 response.

To combat this, iLR-DPO incorporates a length penalty directly into the DPO objective function during training. In essence, the model is simultaneously optimized for preference (generating a better response) and for brevity (minimizing the length difference between the preferred and rejected responses).

The researchers applied iLR-DPO to a 7B base model, fine-tuned from Mistral-7B, resulting in the model dubbed Storm-7B.

The empirical results were striking. On the stringent AlpacaEval 2.0 leaderboard, Storm-7B achieved a 50.5% *length-controlled* win rate against GPT-4 Preview. This makes it the first open-source 7B model to match the performance of the GPT-4 baseline on this metric, all without significantly increasing the average response length.

To illustrate the practical difference, consider the query, "Why can I see the moon during the day?" The iteration 3 output from the verbose iDPO model was so extensive that it included repetitive explanations and unnecessary context, leading to truncation. In contrast, the iLR-DPO model provided a concise, clear, and well-structured list of factors, maximizing information density while conserving tokens.

Furthermore, the technique successfully minimized the “alignment tax”—the performance hit LLMs often take on traditional NLP tasks (like commonsense reasoning or multiple-choice questions) when fine-tuned for conversational alignment. Results across the Open LLM Leaderboard showed that iLR-DPO maintained, and sometimes improved, performance on these foundational tasks.

The success of iLR-DPO demonstrates that small, resource-efficient models can achieve top-tier alignment, provided the training method accurately balances quality and conciseness, proving that in the race for advanced AI performance, sometimes less is truly more.