---
layout: paper
pdf_url: https://arxiv.org/pdf/2406.11817
permalink: 2024-06-21/2406.11817/
title: Small Language Models Catch Up&#58; New Optimization Technique Aligns 7B Model
  to GPT-4 Performance While Capping Verbosity
---



A new study by researchers from CUHK and Shanghai AI Laboratory demonstrates a breakthrough in aligning smaller language models (LLMs) to achieve performance parity with industry leaders like GPT-4, without the typical pitfall of generating excessively long and wasteful responses.

The team introduced the Iterative Length-Regularized Direct Preference Optimization (iLR-DPO), a novel method that successfully trained a 7-billion parameter (7B) model, dubbed Storm-7B, to match the alignment scores of GPT-4 Preview on leading benchmarks.

Direct Preference Optimization (DPO) is a standard technique used to fine-tune LLMs based on human feedback. Recent research has shown that DPO is most effective when applied iteratively—generating new model responses, having an independent reward model rank them (creating synthetic preferences), and then retraining.

However, the authors identified a critical flaw in this iterative DPO (iDPO) approach: as the model improves in quality, it becomes dramatically more verbose.

The study found that after three iterations, vanilla iDPO generated responses with an average length of 5,600 tokens, roughly three times the length of typical GPT-4 output (around 2,000 tokens). This excessive verbosity is undesirable, as lengthy responses often contain repetitive or overly complex reasoning, increasing computational costs without adding meaningful information.

To illustrate, if a user asked the vanilla iDPO model, "Why can I see the moon during the day?", the model might generate a sprawling, multi-page answer that includes lengthy explanations of irrelevant celestial events like the winter solstice or lunar eclipses, simply because the optimization favors length alongside preference.

iLR-DPO addresses this by integrating a length penalty directly into the DPO loss function. The model is thus optimized not only to maximize the reward model's preference score but also to minimize the difference in length between the chosen (preferred) and rejected responses. This forces the model to achieve quality through succinctness rather than verbosity.

The results validated this approach: Storm-7B achieved a 50.5% length-controlled win rate against GPT-4 Preview on the rigorous AlpacaEval 2.0 benchmark. Crucially, it maintained a response length consistent with the GPT-4 baseline.

On instruction-following evaluations, iLR-DPO consistently outperformed both standard DPO and vanilla iDPO on MT-Bench and Arena-Hard. Furthermore, the length regularization did not incur a significant "alignment tax"—the model showed minimal performance degradation on traditional NLP tasks like commonsense reasoning and reading comprehension.

By efficiently controlling length during the iterative alignment process, the iLR-DPO method demonstrates that highly capable and concise LLMs are achievable even at smaller scales, opening the door for more resource-efficient and high-quality open-source alternatives.