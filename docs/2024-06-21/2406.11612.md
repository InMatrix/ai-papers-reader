---
layout: paper
pdf_url: https://arxiv.org/pdf/2406.11612
permalink: 2024-06-21/2406.11612/
title: New 'Long Code Arena' Benchmark Challenges AI Models to Master Project-Level
  Coding
---



Researchers have launched a formidable new benchmark suite, the "Long Code Arena," designed to push large language models (LLMs) beyond simple code snippets and into the realm of real-world, project-wide software engineering tasks.

While contemporary LLMs boast context windows capable of processing tens of thousands or even millions of tokens, existing benchmarks in Machine Learning for Software Engineering (ML4SE) are often limited to single files or methods. Long Code Arena, developed by researchers from JetBrains Research and Delft University of Technology, aims to close this gap by requiring models to utilize context spanning entire repositories or modules to succeed.

The suite comprises six distinct tasks, each mirroring complex challenges faced by human developers, ensuring that models are evaluated on practical utility rather than just theoretical capacity.

### Intuition Building: Context is King

To succeed in Long Code Arena, models must learn to navigate large, interconnected codebases. Consider the **CI Builds Repair** task. Instead of fixing a single line, the model is given a repository snapshot and the complete logs of a failed Continuous Integration (CI) pipeline. It must analyze gigabytes of repository data and pinpoint the fix—such as patching a specific configuration file based on an obscure error found deep within the build log—and then propose a diff that successfully passes a re-run on GitHub Actions.

Similarly, the **Bug Localization** task presents a detailed bug report (like Issue #45, "Rendering failure in dark mode") alongside the full project history. The model must analyze the issue description, search the entire repository structure, and output a ranked list of files that need modification.

The benchmark also elevates core development activities. **Library-based Code Generation** demands that models generate entirely new code files based on natural language instructions, but crucially, this new code must heavily utilize existing, in-project APIs. For example, generating a utility script that uses `ProjectX.data_handler()` functions already defined elsewhere in the codebase, rather than introducing new, external dependencies.

Another critical task, **Project-Level Code Completion**, challenges models to auto-complete a single line of code, drawing context not only from the immediate file but also from related files, documentation, and the project’s Git history, simulating complex auto-completion in a multi-file development environment.

The final two tasks involve processing vast textual data: **Commit Message Generation** requires models to summarize exceptionally large code diffs into concise, high-quality natural language messages, and **Module Summarization** assesses the model's ability to generate coherent, module-level documentation given the source code and a high-level intent description.

### Baselines Show Room for Growth

Initial baseline evaluations using leading models like GPT-4 and various CodeLlama and Mistral variants confirm that the arena presents a tough challenge. While proprietary models generally outperform open-source competitors, performance remains low across the board, demonstrating that models are not yet proficient at complex, long-context reasoning. For instance, in the CI Builds Repair task, the best non-proprietary models only successfully fix 4% to 9% of builds, validating that Long Code Arena sets a new and necessary standard for the next generation of long-context LLM development.

The data, derived from filtered and manually verified open-source GitHub repositories, is publicly available via HuggingFace Hub, complete with a leaderboard to track future progress.