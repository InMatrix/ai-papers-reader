---
layout: paper
pdf_url: https://arxiv.org/pdf/2406.11715
permalink: 2024-06-21/2406.11715/
title: Privacy Buffer&#58; RLHF Proves Safer Than Direct Methods for Training LLMs
  on Sensitive Code
---



In the rapidly evolving landscape of Large Language Models (LLMs), Reinforcement Learning with Human Feedback (RLHF) has become the gold standard for aligning powerful AI with human preferences. A new study by Google DeepMind researchers, focused specifically on code completion models, reveals a critical finding: the multi-stage RLHF process acts as a robust privacy buffer, significantly mitigating the risk of memorizing and later regurgitating sensitive training data, especially when compared to newer, simpler alignment techniques.

Understanding how training data memorization occurs during alignment is paramount, particularly for commercial tools like code assistants, where preference data often includes proprietary code, API keys, or personally identifiable information (PII).

The research dissected the three phases of RLHF—initial fine-tuning (FT), training the intermediate Reward Model (RM), and final RL Fine-Tuning (RLFT)—to track where data leakage might occur.

The most reassuring finding concerns the reward model training data, which often contains the most sensitive human preferences. When this sensitive data (like proprietary file paths) was fed *only* into the Reward Model, the final RLFT model showed virtually no risk of memorizing it.

For instance, using synthetic PII-like data, direct fine-tuning regurgitated these sensitive file paths 50% of the time. However, when the exact same data was used only to train the Reward Model, the subsequent RLFT model showed 0% memorization of those paths. The reward score, which is simply a positive or negative scalar signal, appears to be too low-information to allow memorization to propagate to the final model. This finding suggests that highly valuable user interaction data can be leveraged in RM training without high risks of sensitive data leakage.

However, the study also identified pathways where memorization persists. Examples that were already memorized during the initial Fine-Tuning stage were highly likely to remain memorized through the final RLFT step. The persistence of this early memorization is influenced by the KL regularization penalty ($\alpha$); reducing this penalty slightly decreases the retention of old memorized examples, as it allows the model to drift further from its initial state.

Crucially, the study issued a warning regarding Direct Preference Learning (DPL) algorithms, such as Identity Preference Optimization (IPO). DPL methods skip the intermediate Reward Model, training the aligned model directly from preference data to simplify the alignment pipeline. The researchers found that IPO resulted in significantly higher memorization rates. For general code examples, while RLHF memorized less than 1% of the reward model training data, IPO alignment led to memorization rates spiking to over 18%.

The results, which the team validated across different model scales (Gemini Nano-1, Gemma 2B and 7B) and different datasets (code and natural language), suggest that RLHF's multi-stage complexity, particularly the role of the reward model as a signal intermediary, provides a vital safety mechanism. While DPL methods gain popularity for their simplicity, the research concludes that RLHF is the safer approach for aligning LLMs on sensitive human preference data due to its superior mitigation of regurgitation risk.