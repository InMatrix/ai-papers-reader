---
layout: paper
pdf_url: https://arxiv.org/pdf/2406.12644
permalink: 2024-06-21/2406.12644/
title: New Evaluation Framework Assesses LLMs Using Human Cognitive Demands
---



Researchers have developed a novel evaluation framework for Large Language Models (LLMs) that shifts away from standard benchmarking toward assessing cognitive demands, much like evaluating a student taking a structured exam.

The new system, the Hierarchical Prompting Taxonomy (HPT) and its operational core, the Hierarchical Prompting Framework (HPF), systematically measures the complexity of a task by aligning prompt strategies with established human cognitive principles, such as Bloom’s Taxonomy. This approach ensures that LLM evaluations are standardized, flexible, and grounded in the mental effort required for problem-solving.

Conventional LLM evaluation often uses a single, fixed prompting strategy regardless of whether the task involves simple factual recall or complex multi-step reasoning. This can lead to biased or suboptimal performance measurements. The HPT addresses this by structuring five distinct prompting strategies into a hierarchy based on increasing cognitive load.

This hierarchy, analogous to different levels of support available during an "open-book" exam, allows the framework to dynamically select the minimum cognitive effort needed for an LLM to succeed:

1.  **Basic Recall (Role Prompting):** The simplest level, akin to looking up a definition in a glossary. For instance, asking an LLM to "act as a translator" to convert a simple sentence.
2.  **Understanding (Zero-Shot CoT/Three-Shot CoT):** Requires interpretation and guided reasoning. This is achieved by using Chain-of-Thought prompts that encourage the model to "think step by step," mirroring the process of interpreting an exam question.
3.  **Analysis and Application (Least-to-Most Prompting and Generated Knowledge Prompting - GKP):** The highest complexity levels, reserved for tasks requiring synthesis and external knowledge. Least-to-Most breaks a complex task into sequential sub-problems, while GKP demands the LLM integrate self-generated auxiliary knowledge to solve the problem—analogous to synthesizing information across multiple chapters to answer a multi-step analytical essay.

To quantify this, the researchers introduced the Hierarchical Prompting Index (HPI). A lower HPI indicates that an LLM successfully solved a task using a less complex prompting strategy, suggesting higher inherent capability relative to the task's demands.

Experiments across diverse datasets—including reasoning, coding (HumanEval), and mathematics (GSM8k)—demonstrated that using HPF significantly enhanced LLM performance by 2% to 63% over standard baseline methods. The largest gains were observed in complex tasks, indicating the framework’s effectiveness in unlocking latent reasoning potential.

For example, the difficult Grade School Math (GSM8k) benchmark proved to be the most cognitively demanding task among tested reasoning and coding challenges, registering a high average HPI of 3.20. Conversely, general knowledge tasks (like MMLU) had consistently low HPI scores, confirming they require minimal cognitive effort.

Crucially, the HPF's ability to tailor the prompting strategy to task complexity allows smaller language models (SLMs), such as Mistral-Nemo 12B, to achieve performance levels comparable to larger proprietary models like GPT-4o, particularly in code generation tasks, thereby optimizing computational efficiency and deployment decisions.