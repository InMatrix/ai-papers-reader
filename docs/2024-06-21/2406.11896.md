---
layout: paper
pdf_url: https://arxiv.org/pdf/2406.11896
permalink: 2024-06-21/2406.11896/
title: Researchers Achieve Major Breakthrough in "In-the-Wild" Device Control with
  Autonomous Reinforcement Learning
---



A team of researchers has introduced DigiRL, a novel autonomous Reinforcement Learning (RL) approach that has set a new state-of-the-art for training AI agents capable of controlling mobile devices through graphical user interfaces (GUIs). By moving beyond static demonstration data, DigiRL successfully trains agents to navigate the stochastic and non-stationary environment of the real-world internet, achieving a massive 49.5% absolute improvement in task success rate on benchmark tests compared to agents trained via traditional supervised methods.

The challenge for existing vision-language models (VLMs), including powerful systems like GPT-4V and Gemini 1.5 Pro, is their inherent weakness when moving from abstract reasoning to concrete, multi-step device control. Traditional methods, which rely on mimicking human demonstrations, fail when faced with real-world unpredictability, such as unexpected pop-up ads, website changes, or errors that divert the agent from its goal.

DigiRL overcomes this limitation by employing a two-stage training strategy: first, an *offline RL* phase initializes the agent using existing data, followed by an *offline-to-online RL* phase where the agent continuously fine-tunes itself through autonomous interaction. This crucial online phase requires a specialized, scalable environment capable of running up to 64 parallel Android emulators simultaneously, allowing the agent to generate and learn from its own real-time experiences at scale.

To ensure efficient learning from these self-collected, often messy, trajectories, the researchers integrated several deep RL techniques. A key innovation is the use of an automatic curriculum, which utilizes an instruction-level value function to identify and prioritize the most challenging and informative tasks. For instance, if a task like "Go to bestbuy.com and search for a laptop" often fails, the system learns that successfully completing that specific trajectory offers a maximum learning signal, thus prioritizing similar difficult tasks for future practice.

Furthermore, DigiRL is equipped with robust advantage estimators designed to calculate rewards reliably despite the stochastic nature of the environment. This system enables the agent to effectively learn how to recover from its own errors—a failure mode that derails most previous systems.

For example, when asked to complete a web shopping task, a previous VLM-based agent might accidentally tap a voice input button or misidentify a search bar, leading it to get permanently stuck or "hallucinate" that it has completed the task on the wrong page. In contrast, DigiRL, trained on its own failures, learns to systematically reverse missteps—like clicking the "Back" button or resetting the screen—to return to a high-value state and resume the instruction.

Evaluated on the diverse Android-in-the-Wild (AiTW) dataset, the 1.3-billion-parameter VLM trained with DigiRL boosted the success rate on general tasks from 17.7% (for proprietary VLM wrappers) to 67.2%, decisively surpassing all prior state-of-the-art methods. This work establishes a powerful new paradigm, demonstrating that training multi-modal agents via autonomous reinforcement learning is essential for building reliable, robust digital assistants capable of tackling complex, real-world tasks.