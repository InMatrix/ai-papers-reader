---
layout: paper
pdf_url: https://arxiv.org/pdf/2406.14515
permalink: 2024-06-21/2406.14515/
title: New Benchmark Reveals Major Gaps in AI’s Understanding of Long-Form Video
---



A new benchmark designed to rigorously test the temporal comprehension skills of Large Vision-Language Models (LVLMs) has exposed significant weaknesses in how current AI systems interpret realistic, long-form video content.

The benchmark, dubbed MMBench-Video, addresses the limitations of previous Video Question Answering (VideoQA) datasets, which typically feature short clips and questions solvable by viewing only a single, static frame. Developed by researchers from Shanghai AI Laboratory and collaborating institutions, MMBench-Video leverages approximately 600 lengthy web videos sourced from YouTube, ranging from 30 seconds up to six minutes, spanning diverse categories like News, Sports, and Entertainment.

Unlike older benchmarks that rely on simple concept identification, MMBench-Video employs roughly 2,000 human-annotated, free-form question-answer pairs meticulously crafted to assess 26 fine-grained capabilities. These capabilities fall into two major categories: Perception (e.g., Object Recognition, OCR) and Reasoning (e.g., Causal Reasoning, Temporal Reasoning).

To ensure the new benchmark accurately measures continuous video comprehension, the creators prioritized questions that are "temporally indispensable"—meaning they cannot be solved without observing the flow of events.

For example, a question like, "Who won the second place in the swimming race?" requires tracking event sequences and often reading on-screen text (OCR) integrated across several moments. Conversely, a static question like, "Is there a dog in the video?" is temporally dispensable.

The benchmark’s temporal focus was immediately evident in the test results. When the leading proprietary LVLM, GPT-4o, was evaluated using only a single input frame, its normalized score dropped to just 26.0%—a marked 52% decrease compared to its performance when allowed to see eight frames. On older, less challenging benchmarks, the performance drop was closer to 20%, highlighting the static nature of those previous datasets.

The comprehensive evaluation revealed a substantial performance gap between proprietary models and their open-source counterparts. Dedicated open-source Video-LLMs (such as Video-LLaVA) generally exhibited subpar performance, significantly lagging behind even some image-based LVLMs (like Idefics2 and InternVL-Chat).

Among all models tested, proprietary systems like GPT-4o, when utilizing 16 frames, performed best, but all models struggled with key reasoning tasks. Video-LLMs showed a particular vulnerability to *hallucination*, meaning they frequently generated responses about non-existent visual content when uncertain.

Researchers also found that incorporating external speech information, such as YouTube-generated video title tracks (subtitles), significantly boosted the reasoning capabilities of proprietary models, indicating that non-visual context remains critical for holistic video understanding.

By emphasizing long-form content and complex temporal reasoning, MMBench-Video is set to become a vital resource for diagnosing the precise spatial and temporal weaknesses of current vision-language models, guiding the next generation of AI development toward genuine video comprehension.