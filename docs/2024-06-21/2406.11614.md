---
layout: paper
pdf_url: https://arxiv.org/pdf/2406.11614
permalink: 2024-06-21/2406.11614/
title: LLM Unlearning is Broken&#58; New Internal Test Reveals Erased Knowledge Remains
  Hidden in Model Parameters
---



Researchers have unveiled a critical flaw in current large language model (LLM) "unlearning" methods, demonstrating that these models often only suppress unwanted knowledge behaviorally rather than truly erasing it from their internal structure. This residual information, contained within specific parameter clusters, remains vulnerable to adversarial attacks.

The findings, published by a team spanning institutions including NYU and the University of Toronto, introduce the **CONCEPT VECTORS** benchmark, the first framework designed for the parameter-based, intrinsic evaluation of unlearning performance.

### The Problem with Behavioral Tests

Current methods to remove sensitive, harmful, or outdated information—such as fine-tuning models to forget copyrighted works or toxic concepts—rely on external, behavioral tests. For example, if an LLM is asked to unlearn the concept of “Harry Potter,” a successful unlearning test verifies that the model can no longer answer questions about the book series.

However, the researchers argue that merely changing the model’s external output is insufficient. They show that while existing methods like gradient-based optimization successfully stop the model from generating forbidden information under normal circumstances, they introduce only "negligible changes" to the model's core memory parameters.

This hidden memory exists in what the paper calls **“concept vectors,”** specific sets of parameters localized in the LLM's MLP (Multi-Layer Perceptron) layers that causally influence concept generation.

To illustrate, consider the concept "Amazon Alexa Voice Assistant." Before unlearning, a query about Alexa activates a concept vector that promotes related tokens like "smart speaker" or "voice assistant." After standard unlearning, the model might refuse the query, but the vector itself remains intact (maintaining a Jaccard similarity score near 1.0 with the original vector).

### Hidden Knowledge Exploited by Jailbreaks

The core danger of this incomplete erasure is adversarial exploitation. The study tested leading unlearning methods (like DPO and Gradient Difference) against common jailbreak attacks. When adversaries used crafted prompts—which bypass safety alignments—the residual knowledge traces were immediately reactivated.

The model, seemingly having forgotten who the friends of Harry Potter are, would successfully recall "Hermione Granger and Ron Weasley" once triggered by a jailbreak prompt. This demonstrates that unlearning only suppressed the concept vector activation, which jailbreaking restored.

### Advocating for Parametric Erasure

To prove that intrinsic erasure is necessary for robust unlearning, the researchers developed an oracle baseline, dubbed **"Needle,"** which deliberately ablates (adds noise to) the target concept vector.

Needle successfully drove the Jaccard similarity of the vector down to nearly 0.05, meaning the parametric trace was truly destroyed. This technique not only resulted in a dramatic drop in the model’s ability to generate target information (a 40-60% decrease in QA performance) but also made the model significantly more robust against jailbreaking. When Needle was applied, jailbreak attacks failed to recover the unlearned information.

The CONCEPT VECTORS benchmark, which tracks 285 diverse concepts in LLaMA 2 7B and OLMo 7B, provides a crucial tool for developers. The findings underline that true unlearning requires targeted modification of internal parameters, not just superficial behavioral correction. Moving forward, the scientific community must prioritize methods that effectively remove these parametric knowledge traces to ensure secure and trustworthy LLMs.