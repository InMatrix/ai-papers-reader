---
layout: paper
pdf_url: https://arxiv.org/pdf/2406.12649
permalink: 2024-06-21/2406.12649/
title: New Framework Delivers Trustworthy, Multi-Level Explanations for Vision Foundation
  Models
---



Vision Transformers (ViTs) have become the bedrock of modern computer vision, acting as powerful foundation models across diverse tasks. However, their increasing deployment in high-stakes environments—such as autonomous systems or medical diagnostics—has amplified the demand for trustworthy explanations of their decisions.

A new paper introduces Probabilistic Concept Explainers (PACE), a novel variational Bayesian framework designed to provide robust, human-understandable explanations that fully comply with five essential criteria for trustworthiness: Faithfulness, Stability, Sparsity, Parsimony, and Multi-Level Structure.

Existing explanation techniques, often focusing only on attributing predictions to specific image regions, consistently fail these criteria, particularly Stability (producing consistent explanations when the input image is slightly perturbed) and Multi-Level Structure (explaining concepts across the entire dataset, a single image, and individual patches).

PACE addresses these limitations by modeling the statistical distributions of ViT's hidden patch embeddings in a hierarchical fashion. It learns concepts top-down: from dataset-level concepts (shared features across all images), down to image-level concept activations, and finally to patch-level concepts (explaining why a specific image patch belongs to a certain concept).

### Hierarchical Conceptual Mapping

To understand how PACE works, consider a ViT trained to classify flowers. PACE extracts key, high-level concepts from the training data.

On the real-world **Flower** dataset, PACE identified dataset-level concepts corresponding to features like "Green Stem/Leaves" and "Purple Petal."

When a new image is input, the model analyzes its structure:
1.  **Dataset-Level:** PACE confirms that concepts like "Green Stem/Leaves" are relevant across the entire dataset of flowers.
2.  **Image-Level:** For the specific input image, PACE quantifies the *activation* of these concepts. For instance, the image-level explanation might show a strong association with "Purple Petal" and "Green Stem/Leaves," reflecting the image's overall content.
3.  **Patch-Level:** At the lowest level, PACE links individual patches—the small $16\times16$ pixels units the ViT processes—to these concepts. The patch corresponding to the flower’s stem is explicitly linked to the "Green Stem/Leaves" concept, providing a precise, localized justification for the overall classification.

Furthermore, PACE embeds stability directly into its training process. By requiring that the concepts inferred from an original image remain similar to those inferred from a slightly perturbed version (e.g., with added noise), the system guarantees highly reliable explanations.

### Outperforming Baselines

In extensive quantitative evaluations across synthetic (Color) and real-world datasets (Flower, Cars, CUB), PACE demonstrated superior performance against state-of-the-art methods like LIME, SHAP, and CRAFT.

On average, PACE registered the highest scores for Faithfulness (0.72), indicating its explanations accurately mirror the model's prediction logic. Crucially, it achieved the best Stability (0.11, lower is better), confirming its consistency under noisy inputs.

By providing concept-based explanations at three distinct hierarchical levels, PACE offers unparalleled transparency, effectively bridging the gap between ViT internal mechanics and human interpretation, making vision foundation models safer and more accountable for deployment in critical domains.