---
layout: paper
pdf_url: https://arxiv.org/pdf/2406.12045
permalink: 2024-06-21/2406.12045/
title: New AI Benchmark Reveals State-of-the-Art Language Agents Fail Crucially on
  Consistency and Rule-Following
---



A new benchmark designed to stress-test the practical reliability of large language model (LLM) agents reveals that even the most advanced models struggle significantly with consistent performance and adherence to domain-specific policies—two critical requirements for real-world deployment.

Dubbed $T$-bench (Tool-Agent-User Interaction Benchmark), the system simulates dynamic, multi-turn customer service conversations, forcing agents to juggle interaction with a human user, complex database queries via API tools, and strict adherence to codified domain rules. Researchers from Sierra created $T$-bench to bridge the gap left by older benchmarks that often rely on simplified, single-step instructions without the complications of conversational context or realistic policy documents.

$T$-bench features two distinct environments: $T$-retail, involving tasks like order modification and exchanges; and $T$-airline, requiring flight booking, modification, and cancellation, complete with complex policies regarding baggage allowances and cabin class constraints.

### The Challenge of Consistency

The benchmark's most significant finding is the fragility and inconsistency of top-performing models. While existing metrics often reward a model if it finds the correct solution once, $T$-bench introduces a rigorous new metric, $pass^k$, which measures the probability that an agent succeeds in *all* $k$ independent trials of the same task. For customer service or finance, reliability is paramount—an agent must succeed every time, not just sometimes.

Testing models using their native function-calling capabilities, the researchers found success rates ($pass^1$) for the top commercial model, GPT-4o, lagged far behind expectations, achieving only 61.2% in $T$-retail and a mere 35.2% in the more complex $T$-airline domain.

When consistency was tested across multiple trials, performance plummeted. For instance, the chance that GPT-4o could successfully complete the same $T$-retail task eight times in a row ($pass^8$) dropped to below 25%. This demonstrates that current state-of-the-art LLMs, while capable, are too stochastic and inconsistent for trustworthy applications.

### Failing to Follow the Rules

Analysis of agent failures highlighted three primary weaknesses. First, agents struggle with "Wrong argument or information provided," often failing at complex database reasoning. For example, in a task where a user wants to exchange a lamp for a less bright one and specifies a preference for an AC adapter, the agent must efficiently filter multiple product variants and options before calling the correct API. Current models frequently hallucinate or misuse arguments.

Second, models fail at "Incorrect decision-making," where they fail to apply the domain policies. In $T$-airline, a user might try to change a "basic economy" flight. The agent must recall the policy rule stating that basic economy flights cannot be modified. A successful agent would then propose an alternative, such as canceling the old flight and booking a new one, ensuring policy compliance while resolving the user’s underlying need. Failing to check or apply this rule leads to an unsuccessful outcome.

Third, agents struggle with "Partial resolution of compound requests." When a user has multiple requests (e.g., correct a home address *and* change an item in a pending order), agents often address only the first request or omit necessary subsequent actions, revealing limitations in long-context memory and systematicity.

The researchers conclude that $T$-bench provides a crucial tool for developing the next generation of LLM agents, shifting the focus from maximizing average success to ensuring the consistency and robust rule-following necessary for practical, real-world interactions. ($500$ words)