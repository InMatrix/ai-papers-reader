---
layout: paper
pdf_url: https://arxiv.org/pdf/2406.11927
permalink: 2024-06-21/2406.11927/
title: New Benchmark Forces LLMs to Tackle 'Technical Debt' in Code Generation
---



A team of researchers has introduced a rigorous new framework, **REPOEXEC**, designed to test a crucial but often overlooked aspect of advanced code generation models: their ability to integrate new code seamlessly into existing software repositories. The findings reveal that while many leading Code Large Language Models (CodeLLMs) can generate code that passes functional tests, they frequently fail to utilize existing project dependencies, leading to inefficient implementations and significant technical debt.

Traditional benchmarks for CodeLLMs often focus on generating standalone functions that pass specific unit tests (`pass@k`). However, real-world software development requires models to recognize and invoke existing functions, classes, and variables present in the broader repository context.

To address this gap, the researchers built REPOEXEC, a comprehensive Python benchmark with an executable environment and a pipeline for automatically generating high-coverage unit tests. Crucially, they introduced a novel metric, the **Dependency Invocation Rate (DIR)**.

### Beyond Correctness: The DIR Metric

DIR measures the percentage of provided dependencies that the generated code successfully incorporates. If an LLM ignores existing helper functions and rewrites logic from scratch, its code might still pass tests (achieving high `pass@k`), but its DIR score will be low.

"Functional correctness alone cannot fully capture code quality," the authors write, noting that ignoring dependencies creates redundancy and technical debt.

For instance, if a repository already defines a function like `is_camel_case` to validate strings, a high-quality LLM solution for a new function like `camel_case_to_snake` must call the existing `is_camel_case` helper. An LLM that reimplements this validation logic instead, even if functionally correct, introduces inefficient code that is harder to maintain.

### Context is King, Tuning is Critical

The REPOEXEC experiments evaluated 18 models under varying levels of dependency context (Full, Medium, and Small). The results underscore that providing the full context of required dependencies yields the best overall performance in both correctness and dependency utilization.

The study highlighted a key difference between model types:

1.  **Pretrained LLMs** (like CodeLlama-13b-Python) generally excel in pure functional correctness (`pass@k`) but demonstrate lower DIR. They tend to generate runnable code but often fail to utilize the provided context effectively, preferring to reimplement existing functions.
2.  **Instruction-Tuned LLMs** (like WizardCoder) show higher DIR scores, indicating a better ability to follow the context and leverage predefined dependencies, though sometimes at the cost of introducing unnecessary complexity.

DeepSeek models demonstrated strong overall capabilities, with DeepSeek-V3 achieving a DIR of 80.35%, indicating excellent dependency management. Interestingly, the smaller GPT-4o-mini model achieved a high DIR of 74.75%, suggesting its instruction tuning heavily emphasizes context use.

The research also confirmed that models can be significantly improved. Multi-round debugging, where models iteratively fix their code based on test execution logs, boosted the functional correctness (`pass@1`) of GPT-3.5 by over 10 percentage points and improved its DIR by over 7% across three rounds.

Furthermore, fine-tuning models on a new dependency-enhanced dataset specifically designed to encourage dependency invocation boosted DIR scores across the board, proving that targeted instruction tuning can enhance robust, real-world coding behavior.

By integrating executable testing and the Dependency Invocation Rate, REPOEXEC establishes a new baseline for evaluating CodeLLMs, emphasizing that powerful generative AI must not only write correct code but also write clean, maintainable code that adheres to software development best practices.