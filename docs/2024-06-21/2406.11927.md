---
layout: paper
pdf_url: https://arxiv.org/pdf/2406.11927
permalink: 2024-06-21/2406.11927/
title: New Benchmark Reveals Why Code LLMs Create 'Technical Debt'
---



A new study by researchers has introduced REPOEXEC, a stringent benchmark designed to test a crucial, often-overlooked skill in Large Language Models (LLMs) specialized in code generation: their ability to correctly integrate external dependencies within a software repository.

While Code LLMs (CodeLLMs) are often praised for generating functionally correct standalone code, real-world software development requires referencing and utilizing existing functions, classes, and variables spread across multiple files. The paper, "On the Impacts of Contexts on Repository-Level Code Generation," finds that many top models fail this integration task, often opting to reimplement existing logic, which leads to redundancy and accumulated technical debt.

To address the limitations of prior benchmarks, which often rely on inadequate text-matching scores, REPOEXEC evaluates models based on two dimensions:

1.  **Functional Correctness:** Measured using execution-based metrics (like `pass@1`), which verify that the generated code runs and passes automatically generated, high-coverage unit tests.
2.  **Dependency Utilization:** Measured by the novel **Dependency Invocation Rate (DIR)**.

The DIR metric quantifies the percentage of provided, relevant dependencies (like imported functions or classes) that the generated code actually calls. For instance, if a repository already contains a robust function, `is_valid_email()`, and the LLM is asked to write a new user registration function, a high DIR means the LLM correctly imported and used `is_valid_email()`, instead of writing a new, potentially buggy version from scratch.

Experiments across 18 CodeLLMs revealed a significant trade-off based on training methods. Pretrained foundation models (like early CodeLlama versions) often achieve good functional correctness but suffer from low DIR. They frequently generate solutions that work, but ignore the available context and reimplement dependencies.

Conversely, instruction-tuned LLMs show a higher capacity for dependency utilization (higher DIR), demonstrating they are better at following the "human intent" implied by the context. However, instruction-tuned models sometimes struggle with functional correctness (`pass@1`), occasionally producing overly complex code or "hallucinating" incorrect implementations.

The researchers also explored how the amount of context provided affects performance, testing models with "Full Context" (all code), "Medium Context" (only signatures and docstrings), and "Small Context" (only signatures). Surprisingly, for many models, the "Small Context" setting was more effective than "Medium Context." The authors suggest that overly verbose documentation (like long docstrings) in the Medium context might inadvertently mislead the models into interpreting the dependency functions as few-shot examples, rather than existing utilities to be called.

Among the models tested, advanced reasoning LLMs like DeepSeek-R1 and DeepSeek-V3 demonstrated superior capabilities, achieving high scores in both functional correctness and DIR, highlighting their ability to effectively balance code accuracy with dependency management. The study concludes that REPOEXEC remains a challenging test for all models, underscoring the urgent need for CodeLLMs that can reliably understand and utilize complex repository context to generate truly scalable and maintainable code.