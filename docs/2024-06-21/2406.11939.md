---
layout: paper
pdf_url: https://arxiv.org/pdf/2406.11939
permalink: 2024-06-21/2406.11939/
title: Automated Pipeline BenchBuilder Creates Hyper-Challenging LLM Benchmark for
  Just $20
---



In a crucial development addressing the rapidly accelerating pace of Large Language Model (LLM) advancement, researchers have introduced an automated pipeline named **BenchBuilder**, designed to continuously generate high-quality, demanding evaluation datasets at minimal cost and without human intervention.

Traditional benchmarks often fail to keep pace with cutting-edge models, becoming saturated or suffering from test-set leakage. Furthermore, manually curating high-quality, open-ended tasks—like the 500 questions in GPQA—can cost upwards of $120,000. BenchBuilder offers a scalable alternative, utilizing LLMs to curate challenging prompts from vast public datasets like Chatbot Arena and WildChat-1M.

The resulting benchmark, **Arena-Hard-Auto**, consisting of 500 open-ended prompts, proves remarkably effective at distinguishing between the industry's top models. Crucially, the system achieves 98.6% correlation with gold-standard human preference rankings while costing only an estimated $20 per model evaluation.

### Engineering High-Quality Prompts

BenchBuilder’s core innovation is its ability to quantify and filter prompt quality using seven key criteria, including specificity, domain knowledge, complexity, and problem-solving ability.

To illustrate, consider three types of user queries harvested from crowdsourced data:
1.  **Trivial:** "Yo, what up my brother." (Low quality score, often filtered out.)
2.  **Simple Recall:** "Who was the president of the US in 1975?" (Medium score, requires factual recall.)
3.  **Challenging:** A multi-step physics question requiring the calculation of an airplane’s final velocity and net force given its mass, acceleration, and distance traveled. (High score, demanding problem-solving and technical accuracy.)

BenchBuilder employs an initial LLM annotator (like GPT-4-Turbo) to score thousands of prompts based on these criteria. It then uses topic modeling to cluster the prompts (e.g., “Python Game Development” or “Prime Numbers and Proofs”) and samples evenly from the highest-scoring clusters, ensuring the final benchmark is both difficult and diverse.

### Measuring LLM Separability

Beyond efficient curation, the researchers introduced novel metrics to rigorously assess benchmark performance. While previous evaluations relied heavily on correlation metrics (like Spearman), these often fail to capture ranking *confidence*—how reliably a benchmark can separate two closely ranked models.

The new metrics, including **Separability with Confidence** and **Agreement with Confidence Interval**, focus on the magnitude of performance differences and the statistical confidence of the predictions.

When tested against top-tier models, Arena-Hard-Auto demonstrated an 87.4% separability rate, providing three times higher separation of model performance compared to leading alternatives like MT-Bench, which often struggled to confidently differentiate between similar high-performing LLMs.

By leveraging automated curation and robust evaluation metrics, the BenchBuilder pipeline promises to end the bottleneck of manual benchmark development, ensuring LLM developers have access to continuous, fresh, and challenging tests that accurately reflect human preference in real-world, open-ended scenarios. The pipeline and the Arena-Hard-Auto dataset have been open-sourced.