---
layout: paper
pdf_url: https://arxiv.org/pdf/2406.11912
permalink: 2024-06-21/2406.11912/
title: AI Software Agents Embrace Agile Methodology, Achieving State-of-the-Art Development
  Performance
---



A new framework for autonomous software engineering, AGILECODER, has demonstrated state-of-the-art performance by moving away from simplified "waterfall" coding models and fully embracing the dynamic, iterative workflows of the Agile Methodology (AM). Developed by researchers at FPT Software AI Center and Fulbright University, the multi-agent system successfully mimics human development teams, organizing complex projects into focused sprints and utilizing static analysis to manage large codebases.

Previous large language model (LLM) based agents, such as ChatDev and MetaGPT, typically follow rigid, sequential development steps. However, real-world software development is highly iterative, with approximately 70% of professional teams using Agile methods to adapt to changing requirements. AGILECODER addresses this gap by creating specialized agents—including a Product Manager (PM), Scrum Master, Developer, Senior Developer, and Tester—who collaboratively work through structured sprints involving planning, development, testing, and review.

This iterative approach is key to the framework's superior performance, allowing agents to refine previous work and inherit outputs across multiple cycles. For example, if tasked with creating a "Snake Game," the Product Manager first generates the full requirements. In the initial sprint, the developer might focus only on basic movement and the game board. The Tester then provides feedback, and the Senior Developer reviews the code statically for quality, ensuring the next sprint—which might focus on collision detection and scoring—builds upon a stable foundation.

### Dynamic Context Management

A major limitation for autonomous agents tackling large software projects is context overload; LLMs struggle to keep track of dependencies and changes across vast repositories. AGILECODER overcomes this using its second core innovation: the Dynamic Code Graph Generator (DCGG).

The DCGG maintains a real-time Code Dependency Graph (CDG) that maps the relationships between every file, class, and function in the codebase. When an agent modifies a piece of code, the CDG immediately updates. This graph acts as a precise context retriever.

For instance, if a Developer agent updates `user_manager.py`, the DCGG informs the Tester agent that this file relies on `utils.py`. The Tester is then intelligently directed to focus testing only on those two files and their dependent ancestors, rather than wasting computational resources by reviewing the entire project (e.g., an unrelated module like `game_engine.py`). This targeted context retrieval significantly enhances the accuracy of code generation and bug fixing.

### Outperforming Existing Models

Evaluations conducted across established benchmarks and a new complex dataset, ProjectDev, confirm AGILECODER’s efficacy. Using GPT-3.5 Turbo as the backbone model, AGILECODER achieved a 70.53% pass rate on HumanEval and 80.92% on MBPP, improving upon MetaGPT by 7.71% and 6.19%, respectively.

On the newly introduced ProjectDev benchmark—which involves complex, multi-file software creation tasks like a comprehensive Excel data processing tool or a Brick Breaker game—AGILECODER demonstrated substantial superiority in producing executable programs. The system achieved an executability rate of 57.79%, significantly outperforming ChatDev (32.79%) and MetaGPT (7.73%). Furthermore, AGILECODER completed these complex tasks in an average of just 1.64 sprints, demonstrating efficient adaptation and quality control.

The integration of professional Agile roles, incremental development sprints, and dynamic static analysis marks a significant step toward creating autonomous AI systems capable of handling real-world, repository-level software development challenges.