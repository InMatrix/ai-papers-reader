---
layout: paper
pdf_url: https://arxiv.org/pdf/2406.11912
permalink: 2024-06-21/2406.11912/
title: AI Software Agents Adopt Agile Methodology to Deliver State-of-the-Art Results
---



A team of researchers has unveiled AGILECODER, a novel multi-agent system designed to overcome the limitations of current AI-powered software development frameworks by mimicking real-world Agile Methodology (AM). The new system dramatically improves the reliability and quality of autonomously generated software, achieving state-of-the-art results across complex benchmarks.

While prior Large Language Model (LLM) agent systems, such as MetaGPT and ChatDev, typically follow a rigid, linear "waterfall" model—oversimplifying the highly iterative nature of professional software engineering—AGILECODER integrates the flexibility and refinement of Agile sprints.

The framework assigns specific roles to specialized LLM agents: a Product Manager (PM) defines requirements and acceptance criteria; a Scrum Master (SM) manages the workflow and organizes "sprints"; Developers and Senior Developers handle implementation and code review; and a Tester generates test cases and performs validation.

This collaborative, sprint-based structure allows the system to tackle complexity incrementally. For instance, if a user requests a "Snake game," the PM breaks down the requirement into tasks (e.g., game board setup, snake initialization, collision handling). The system then processes these tasks across multiple sprints, continuously reviewing and refining the codebase based on feedback from the Tester and Senior Developer before declaring the final product deliverable.

A second major innovation driving AGILECODER’s success is the Dynamic Code Graph Generator (DCGG). This module performs static code analysis to dynamically create a Code Dependency Graph (CDG), which maps the relationships between all code files and modules.

In larger projects, existing LLM agents often struggle to retrieve the relevant context needed for debugging or new feature implementation, sometimes overloading the model’s limited token capacity by loading the entire repository. The CDG resolves this by allowing agents to selectively retrieve only the essential context. For example, if a Developer modifies a function in `user_manager.py`, the DCGG informs the Tester that they must re-test `user_manager.py` and any files that import it, but they can safely ignore unrelated files, streamlining bug fixes and ensuring consistency.

Experimental evaluations demonstrate AGILECODER's superiority, particularly on complex, real-world tasks defined in the new ProjectDev benchmark. On this dataset, AGILECODER achieved an executability rate of 57.79%—a substantial leap compared to MetaGPT's 7.73% and ChatDev's 32.79%.

Further analysis confirms that both key features are indispensable: removing the incremental (multi-sprint) development feature significantly degrades performance, as does removing the CDG, which leads to issues like context length errors and random testing order.

By marrying LLM agent collaboration with rigorous Agile practices and static analysis, AGILECODER sets a new state-of-the-art standard, showcasing a significant step toward automated software development capable of handling the complexity of real-world projects.