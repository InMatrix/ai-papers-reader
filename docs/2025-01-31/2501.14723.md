---
layout: paper
pdf_url: https://arxiv.org/pdf/2501.14723
permalink: 2025-01-31/2501.14723/
title: CodeMonkeys&#58; Scaling Test-Time Compute to Solve Real-World GitHub Issues
---



A team of researchers from Stanford University and the University of Oxford have developed CodeMonkeys, a novel system that significantly improves the ability of large language models (LLMs) to resolve real-world software engineering (SWE) issues.  Their findings, published in a recent preprint, demonstrate that scaling "test-time compute" – the computational resources used during the inference phase – is a powerful strategy for enhancing LLM performance on complex tasks like code debugging.

CodeMonkeys tackles the challenge of solving issues from the SWE-bench dataset, a collection of real-world GitHub issues.  Unlike previous approaches that generated a single solution, CodeMonkeys leverages a multi-faceted strategy to improve both the coverage (percentage of issues where at least one generated solution is correct) and the final score (percentage of correctly resolved issues).  It achieves this by scaling both "serial" and "parallel" test-time compute.

**Serial scaling** involves increasing the computational resources used for generating a single solution. CodeMonkeys does this by making the LLM iteratively refine code edits and associated tests, receiving execution feedback at each step.  Imagine an LLM initially generating a buggy code fix.  CodeMonkeys' iterative process allows the model to write a test to expose the bug, then refine the fix based on the test's failure, repeating this cycle until a working solution is found. This iterative process is implemented using state machines that govern the multi-turn interactions between the LLM, the codebase, and an execution sandbox.

**Parallel scaling** involves increasing the number of candidate solutions generated for a single problem. CodeMonkeys generates multiple (edit, test) pairs for each issue, running the parallel state machines multiple times.  Instead of relying on a single attempt, CodeMonkeys creates a diverse pool of candidates, allowing for a more robust selection process later. This is like having multiple programmers work on the same problem independently—increasing the likelihood that at least one will provide a correct solution.

The selection of the best candidate solution from the pool is another crucial aspect of CodeMonkeys.  The system employs a hybrid approach combining model-generated tests and a final multi-turn selection state machine that leverages model-based reasoning. The model-generated tests act as filters, narrowing down the candidate pool to the top performers. Then, the selection state machine meticulously analyzes the top candidates, potentially generating additional tests to further differentiate between them.

CodeMonkeys achieves a remarkable 57.4% success rate on the SWE-bench Verified dataset, a significant improvement over previous methods.  Furthermore, by combining its outputs with existing top submissions (forming a "Barrel of Monkeys" ensemble), CodeMonkeys reaches an impressive 66.2% success rate, surpassing even the best individual submission in the ensemble. This demonstrates the effectiveness of the system's selection mechanism in leveraging multiple sources of generated solutions.

The authors also analyze the cost-effectiveness of their approach. While using powerful LLMs like Claude 3.5, they show that the approach is feasible in terms of computational cost, at roughly 2300 USD for the entire SWE-bench Verified dataset. They also explore more cost-effective LLMs and show that similar improvements in coverage can be achieved at a fraction of the cost.

The CodeMonkeys system demonstrates the significant potential of scaling test-time compute to improve LLM performance on complex real-world tasks.  The paper's release of the code and data allows for further research and development in this promising area.  Future work will focus on improving the context retrieval, further enhancing the generative process, and refining the candidate selection mechanism.