---
layout: paper
pdf_url: https://arxiv.org/pdf/2510.13255
permalink: 2025-10-17/2510.13255/
title: Scientists Unify Brain and AI Research with New Tool, Revealing Unexpected
  Gaps in Next-Generation LLMs
---



A team of researchers has introduced a groundbreaking diagnostic tool, the Hierarchical Frequency Tagging Probe (HFTP), offering a unified framework to directly compare how syntactic structure is encoded in Large Language Models (LLMs) and the human brain.

The study, published recently on the arXiv preprint server, tackles the central question of whether the remarkable language abilities of modern LLMs—such as Llama, GPT, and Gemma—rely on computational mechanisms similar to those used by humans. The findings reveal that while LLMs demonstrate clear hierarchical processing, newer, highly capable models are beginning to diverge from human-like brain alignment, suggesting performance gains are not always achieved through increasingly biomimetic architecture.

### Probing Syntax with Rhythmic Tags

HFTP adapts a technique from cognitive neuroscience that exploits the brain’s sensitivity to rhythmic linguistic structure. Language is inherently hierarchical: words combine into phrases, and phrases combine into sentences.

To isolate these processes, the researchers presented subjects (and models) with sequences where the linguistic structure was tagged at specific, low-frequency rhythms. For example, if words were presented at 4 Hz, researchers could check if the system (either a specific neuron in an LLM or an electroencephalography—sEEG—channel in a human) registered a concurrent response at 2 Hz, marking the grouping of words into phrases, or at 1 Hz, marking the completion of a full sentence.

Applying frequency analysis to model layer activations (specifically, the internal Multilayer Perceptron, or MLP, neurons) and human brain data allowed the team to pinpoint computational units responsible exclusively for sentence structure (1 Hz), phrase structure (2 Hz), or both.

### LLMs Integrate, Human Brain Segregates

The analysis of six state-of-the-art LLMs—including GPT-2, Gemma 2, and Llama 3.1—confirmed that all models possess specialized "syntax neurons" dedicated to capturing these hierarchical relationships. Across models, syntactic representations were found in analogous layers, reflecting shared processing strategies.

However, the human brain exhibited a critical difference. Using sEEG recordings from native Chinese speakers, the researchers found that while humans also tracked these rhythmic markers, phrase and sentence processing were segregated across *distinct cortical regions* in the left, language-dominant hemisphere. In contrast, LLMs often integrated both phrase and sentence information within the same internal computational modules.

Representational Similarity Analysis (RSA) showed that the LLMs’ syntactic representations generally aligned more strongly with the human left hemisphere, particularly in core language areas like the superior temporal gyrus (STG) and inferior frontal gyrus (IFG).

### Divergent Trends in Upgraded Models

Perhaps the most surprising finding concerned the evolution of model architecture. When comparing updated versions, the trends diverged: Gemma 2 showed greater brain similarity than its predecessor, Gemma. Conversely, Llama 3.1 demonstrated consistently *lower* alignment with human brain representations compared to Llama 2.

The researchers hypothesize that Llama 3.1’s reduced alignment may be linked to its training strategy, which emphasized a substantially larger corpus that included extensive code and reasoning data. This scaling and diversification of training data, while boosting overall task performance, may dilute the language-specific regularities that drive human-like syntactic processing.

These results establish HFTP as a vital new tool for interpretability, highlighting that merely improving LLM performance does not guarantee closer adherence to human neural mechanisms. The findings underscore the necessity of refining AI architectures to specifically promote human-like syntactic processing if true cognitive alignment is the goal.