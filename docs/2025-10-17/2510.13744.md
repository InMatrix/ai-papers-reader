---
layout: paper
pdf_url: https://arxiv.org/pdf/2510.13744
permalink: 2025-10-17/2510.13744/
title: New Math Benchmark Hard2Verify Reveals Frontier LLMs are "Too Generous" in
  Proof Verification
---



A team of researchers at Salesforce AI Research has introduced Hard2Verify, a rigorous new benchmark designed to test the ability of Large Language Models (LLMs) to verify complex mathematical proofs step-by-step. The initial results paint a stark picture: while frontier LLMs are capable of generating advanced proofs, current verifiers—including sophisticated open-source models—are fundamentally insufficient to ensure the reliability of these proofs, often failing by being overly generous.

Hard2Verify was meticulously constructed over 500 hours of human annotation by PhD-level math experts. Unlike prior benchmarks that often rely on easier problems or synthetically generated errors, Hard2Verify focuses exclusively on extremely difficult, recent, and open-ended questions sourced from international competitions like the IMO (International Mathematical Olympiad) and Putnam exams. Critically, 78.5% of the problems are open-ended, meaning verifiers cannot rely on a pre-existing final answer, forcing them to substantively assess the logical integrity of every step.

The benchmark imposes a strict grading philosophy: if a step contains a mistake, or if it logically depends on an *already incorrect* previous step, it is marked wrong. This "no error carried forward" rule mirrors the standards used in competitive mathematics.

### Performance Cliff for Step-Level Verifiers

When tested on Hard2Verify, nearly all models showed a massive drop in performance compared to existing benchmarks. For instance, the specialized open-source model Qwen2.5-Math-PRM-72B, which achieved state-of-the-art accuracy of 78.3% on ProcessBench (a related benchmark), saw its performance plummet to just 37.3% on Hard2Verify’s error identification task.

Among the 29 proprietary and open-source verifiers tested, only closed-source models like GPT-5 and Gemini 2.5 Pro demonstrated reliable performance in step-level verification, significantly outpacing their open-weight counterparts.

Analysis of the failure modes reveals that weak verifiers often collapse into a state of excessive leniency. Instead of identifying subtle errors (a capability measured by the True Negative Rate, or TNR), they overwhelmingly label steps as correct. As model strength decreases, the TNR rapidly approaches zero, indicating that verifiers are unable to distinguish incorrect reasoning from correct claims.

### Intuition: Accepting Under-Justified Claims

The researchers found that a recurring theme in verifier mistakes is the incorrect acceptance of "partial or under-justified claims."

For example, when evaluating a proof derived by the ByteDance Seed-OSS-36B model for an IMO Shortlist problem, the verifier accepted a step involving Weyl’s equidistribution theorem despite the human annotators marking it as incomplete. The LLM failed to specify the necessary conditions and bounds required for the theorem’s proper application, a subtle but fatal logical gap that the automated verifier missed.

Despite these challenges, the study offers a glimmer of optimism: models consistently showed that *verifying* a solution is easier than *generating* an error-free solution from scratch. This suggests that future LLM-based verifiers may not need to be as powerful as the models generating the proofs to be effective, pointing toward promising directions for research aimed at building reliable, trustworthy AI reasoning systems.