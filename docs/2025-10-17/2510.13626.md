---
layout: paper
pdf_url: https://arxiv.org/pdf/2510.13626
permalink: 2025-10-17/2510.13626/
title: Robotics Report Card&#58; Top AI Manipulation Models Found Brittle, Often Ignore
  Language Commands
---



Despite achieving near-perfect scores on conventional benchmarks, state-of-the-art Vision-Language-Action (VLA) models—the AI systems that power robotic manipulation—are fundamentally fragile and exhibit critical flaws in generalization, according to a comprehensive new analysis.

Researchers introduced the **LIBERO-Plus** benchmark, subjecting leading VLA models (including OpenVLA and RIPT-VLA) to seven systematic real-world perturbations spanning visual input, physical state, and linguistic commands. The findings expose a consistent "brittleness" hidden beneath apparent competency, challenging the assumption that high success rates equate to true robotic intelligence.

### Extreme Sensitivity to Physical Shifts

The analysis revealed that contemporary VLA models are overwhelmingly vulnerable to changes in the visual and kinematic environment, exhibiting drastic performance drops even under modest adjustments.

Models demonstrated extreme sensitivity to **Camera Viewpoints** and **Robot Initial States**. For instance, performance success rates that were near 95% on unperturbed tasks plummeted to below 30% when the camera angle or the robot arm’s starting pose was slightly shifted. This suggests that the models rely on fixed, narrow visual features and lack a true high-level understanding of spatial geometry necessary for adaptable movement.

In contrast, models that incorporated a close-range **wrist camera** proved significantly more robust to variations in lighting and background textures. The close-up view provides stable, illumination-invariant geometric cues, whereas third-person views are easily confused by shadows or changes in ambient light color.

### The Language Deception: Positional Bias Over Semantics

Perhaps the most surprising finding concerned the role of language. Initial tests showed that models were relatively unaffected by complex linguistic rewrites of instructions. However, a deeper investigation revealed this was not due to sophisticated linguistic generalization, but rather a tendency to **ignore the language input completely**.

In a crucial experiment, researchers tested cross-object instruction-following. If the original task was “pick up the alphabet soup,” and the instruction was explicitly changed to “pick up the tomato sauce,” the model often ignored the new command and still attempted to pick up the alphabet soup. This behavioral pattern indicates the VLA models frequently degenerate into a Vision-Action framework, relying on memorized visual-action sequences rather than dynamically integrating language signals for task decision-making.

Furthermore, models exhibited a strong **positional bias**. While they successfully ignored distracting (confounding) objects added to the scene, they failed dramatically when the target object itself was slightly displaced. This suggests the systems learn to associate an action with a specific object *at a specific location*, failing to learn invariant object semantics.

The LIBERO-Plus benchmark, built on 10,030 tasks categorized into five difficulty levels across seven perturbation dimensions, urges the robotics community to shift evaluation focus from peak success rates to genuinely robust and reliable performance under variable conditions.