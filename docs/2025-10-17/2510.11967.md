---
layout: paper
pdf_url: https://arxiv.org/pdf/2510.11967
permalink: 2025-10-17/2510.11967/
title: LLM Agents Learn to Manage Their Own Memory, Scaling Complex Tasks by 10x
---



A team of researchers from ByteDance Seed and academic institutions has unveiled a novel framework that teaches large language model (LLM) agents how to actively manage their own memory, effectively eliminating the critical constraint posed by finite context windows in long-horizon tasks.

Dubbed **Context-Folding**, the mechanism empowers agents to procedurally condense verbose interaction histories into concise, high-level summaries. This innovation addresses the long-standing problem that, in complex tasks like automated software engineering or deep web research, standard agents (like ReAct) linearly accumulate every step—reasoning, tool calls, and observations—quickly maxing out their token limits and suffering performance degradation.

Context-Folding introduces two specialized actions: `branch` and `return`. When an agent encounters a localized subtask (such as verifying a specific fact or diagnosing a code bug), it uses `branch` to open a temporary sub-trajectory with its own working context. Once the subtask is complete, the agent uses `return`, which triggers the "folding" process. All the intermediate tokens generated during the subtask—which could involve dozens of search queries or bash executions—are collapsed and removed from the main context thread, leaving only a succinct summary of the outcome.

To build intuition, consider a deep research task: "Identify the artist who released an album in November, followed by a second album three years later." A traditional agent would store every failed search, successful lookup, and verification step. The Folding Agent, however, offloads token-intensive work. It might `branch` to a subtask titled "Search for November album." After a long search process involving browsing multiple sites, it `returns` to the main thread with a message like: "Found candidate Gyedu-Blay Ambolley; first album confirmed." The 100,000+ tokens generated in the sub-trajectory are replaced by just a few thousand, preserving the agent's high-level focus.

Making this sophisticated behavior learnable required a new reinforcement learning (RL) framework called **FoldGRPO**. This system goes beyond standard success/failure rewards by implementing dense, token-level process rewards that directly shape the folding behavior.

Crucially, FoldGRPO introduces the "Unfolded Token Penalty" to penalize the agent for performing extensive, token-heavy operations (like prolonged web searches) in the main reasoning thread, thereby forcing them into localized branches. It also employs an "Out-of-Scope Penalty" to ensure branches stay focused on their delegated subtasks.

When evaluated on challenging benchmarks—BrowseComp-Plus (deep research) and SWE-Bench Verified (agentic coding)—the Folding Agent delivered exceptional results. Using a compact 32K active token budget managed across a maximum of 10 branches, the agent achieved a Pass@1 score of 62.0% on BrowseComp-Plus and 58.0% on SWE-Bench.

These scores matched or surpassed baselines that required massive 327K context windows, demonstrating remarkable efficiency. Ablation studies confirmed that FoldGRPO training was essential, enabling the agent to achieve over **90% context compression** in the main thread during complex problem-solving while processing significantly more information overall. The results suggest that active context management, rather than merely extending context limits, is a principled and highly scalable path toward building stronger, more autonomous LLM agents.