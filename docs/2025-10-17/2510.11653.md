---
layout: paper
pdf_url: https://arxiv.org/pdf/2510.11653
permalink: 2025-10-17/2510.11653/
title: New Benchmark Reveals AI Math Models Struggle to Go Beyond Memorization
---



A new study by researchers has introduced MATH-Beyond (MATH-B), a demanding new benchmark for mathematical reasoning that empirically confirms a critical limitation in current open-source Large Language Models (LLMs) fine-tuned with Reinforcement Learning (RL).

The findings suggest that state-of-the-art RL methods used to improve mathematical LLMs are primarily "sharpening" existing solution pathways rather than fostering the genuine exploration required to discover entirely new reasoning capabilities.

For years, the promise of reinforcement learning in AI, exemplified by systems like AlphaGo mastering complex games through self-play and exploration, has fueled research into LLMs. However, the authors argue that benchmarks like MATH and AIME are now largely saturated: small open-source models (under 8 billion parameters) can already solve nearly all problems if given a large sampling budget (pass@1024). Any subsequent improvement measured by RL fine-tuning merely confirms a faster path to an already known solution.

To address this evaluation blind spot, the researchers constructed MATH-B as a "zero-baseline" test. Sourced from high-difficulty datasets (DAPO-Math-17K and DeepScaleR) and rigorously filtered for quality and novelty, MATH-B contains high-school-level competition math problems specifically chosen because the current suite of base models fail them even with 1024 attempts.

The benchmark consists of a Union Set of 181 problems that defeated at least one tested base model, and a core Intersection Set of 41 problems that proved unsolvable for *all* evaluated base models. Because the base models’ success rate on MATH-B is zero, any successful solve by a post-trained model unambiguously represents a genuine expansion of the model’s reasoning boundary, measurable by the metric "Expansion Rate."

When tested on this new standard, leading community RL-finetuned models exhibited only marginal success. For example, some RL models, such as Nemotron-Research-Reasoning-Qwen-1.5B, achieved an Expansion Rate of less than 10% on their respective difficult subsets. This suggests that their RL training failed to help them navigate the complex exploratory landscape needed to find novel solution strategies.

In sharp contrast, the researchers noted a striking success when testing models fine-tuned via Supervised Fine-Tuning (SFT) using "long Chain-of-Thought (CoT) distillation." Models like Qwen3-8B, which were trained by distilling reasoning steps from a more capable teacher model, achieved Expansion Rates as high as 66.38%.

This juxtaposition is the study's central insight: the difficulty is not that the base models are inherently incapable of the necessary reasoning steps, but that current RL exploration methods are insufficient to discover those effective pathways on their own.

The authors conclude that MATH-Beyond offers a clear, challenging testbed intended to catalyze the development of new RL approaches that can achieve genuine exploration, moving the field away from simply polishing pre-existing capabilities toward true skill acquisition.