---
layout: paper
pdf_url: https://arxiv.org/pdf/2510.11062
permalink: 2025-10-17/2510.11062/
title: AI Teams Achieve Near-Perfect Planning Accuracy with New Multi-Agent Reinforcement
  Learning Method
---



A novel approach to training collaborative Large Language Model (LLM) teams, dubbed AT-GRPO (Agent- and Turn-wise Grouped Reinforcement Learning Optimization), has delivered dramatic performance improvements across complex tasks, including boosting long-horizon planning accuracy from a mere 14% to nearly 100%.

Developed by researchers from the University of California, San Diego, and Intel Corporation, the Stronger-MAS framework tackles a core challenge in advancing LLM agents: effectively combining the structured collaboration of Multi-Agent Systems (MAS) with the powerful policy optimization of Reinforcement Learning (RL).

RL, which trains models using environment rewards, is crucial for strong LLM decision-making. However, standard RL optimization techniques, like Group Relative Policy Optimization (GRPO), assume agents are acting in similar contexts—a premise that breaks down in sophisticated MAS. In a true multi-agent setting, agents have specialized roles (e.g., a Coder and a Tester), different prompts, and highly divergent interaction histories turn by turn.

### The Algorithm and System

The research introduces AT-GRPO, an algorithm specifically designed to adapt RL optimization to these heterogeneous environments. AT-GRPO ensures fair credit assignment by defining a unique "group key" for experiences based on the specific *agent* and the current *turn* within a multi-turn dialogue.

To support this, the team also engineered a novel MAS Training System capable of concurrent on-policy training for multiple, distinct LLM policies. This system utilizes "tree-structured sampling," where agents explore multiple candidate actions at each turn, and the system greedily selects the best action to continue the environment rollout. This balance of exploration and exploitation stabilizes learning, particularly for coordinated decisions.

### Intuition: From Failure to Mastery

The most striking results appeared in long-horizon planning tasks, such as the grid-based Plan-Path environment and Sokoban. Single-agent RL models struggled, achieving accuracies in the 14–47% range. When trained using AT-GRPO, the multi-agent system reached accuracies of 96.0% to 99.5%.

This immense leap comes from fostering specialized collaboration. For example, in a game scenario, the multi-agent team might consist of a Tool Agent (which executes pathfinding algorithms like BFS) and a Plan Agent (which provides high-level oversight and action verification). Before RL training, agents often misunderstood their roles, leading to immediate failure—like the Plan Agent trying to execute the box’s path itself and running into a wall.

With AT-GRPO training, the Plan Agent learns to correctly interpret the Tool Agent’s output (the box’s path) and translate it into a sequence of coordinated low-level moves (first, navigate *behind* the box, then *push* it along the planned path). The resulting policy is a cohesive, specialized team that overcomes complex bottlenecks single agents cannot.

The method also delivered consistent reasoning gains, with average accuracy increases of 3.87–7.62% on coding benchmarks and 9.0–17.93% on mathematical reasoning tasks.

### The Trade-off of Specialization

The researchers also found that the choice between a shared or specialized policy depends heavily on the task.

In domains where roles are highly distinct, like the Coder and Unit-Tester roles in code generation, specialized policies for each agent performed best, fostering deep expertise for separate functions. However, in math reasoning, where the Reasoner and Tool Agent roles have significant functional overlap (both often perform reasoning), a shared policy sometimes proved superior, allowing both agents to pool data and collectively refine their skills.

Overall, the AT-GRPO framework offers a robust, scalable blueprint for training sophisticated LLM ensembles, providing a critical path toward more reliable and powerful autonomous agents.