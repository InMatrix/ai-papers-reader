---
layout: paper
pdf_url: https://arxiv.org/pdf/2510.13786
permalink: 2025-10-17/2510.13786/
title: Researchers Define Predictable Scaling Laws for Reinforcement Learning in LLMs
---



In a foundational step toward bringing scientific rigor to large language model (LLM) training, a group of researchers has established the first predictive scaling framework for reinforcement learning (RL) compute.

While scaling laws have long governed the pre-training phase of LLMs, the subsequent RL fine-tuning—critical for advanced reasoning and instruction following—has remained largely an unpredictable "art." The new work, based on an exhaustive empirical study spanning over 400,000 GPU-hours, introduces a principled methodology and a best-practice recipe, dubbed **ScaleRL**, that allows engineers to accurately forecast RL performance gains at extreme compute scales.

The core innovation is modeling RL performance using a *sigmoidal* scaling curve, moving away from the unbounded power laws used in pre-training. This curve captures the inevitable saturation of performance when metrics like pass rate (accuracy) are bounded.

This framework defines two key metrics for any RL algorithm: the **Asymptotic Reward ($A$)**, which is the maximum achievable performance ceiling, and the **Scaling Exponent ($B$)**, which dictates the compute efficiency—how quickly that ceiling is reached.

The study revealed a crucial distinction: most minor algorithmic modifications (like loss aggregation or normalization) only modulate the efficiency ($B$) without significantly altering the final performance ceiling ($A$). However, a few critical choices fundamentally determine the ceiling itself.

For instance, the researchers found that utilizing **FP32 precision** in the LLM's final layer drastically improved the asymptotic reward ($A$), raising the performance ceiling from $0.52$ to $0.61$. Similarly, adopting the **CISPO loss function** (a robust variant of importance sampling) resulted in a much higher potential asymptote compared to common alternatives like DAPO or GRPO, highlighting these decisions as paramount for high-performance systems.

By combining the optimal choices across all design axes, the team engineered ScaleRL. When benchmarked against prevalent public RL recipes (like MiniMax and Magistral), ScaleRL not only showed superior compute efficiency but also reached a higher asymptotic performance ceiling.

The researchers validated ScaleRL's predictability in a massive 100,000 GPU-hour run. Critically, performance predicted by extrapolating the sigmoidal curve from the initial stages of training perfectly aligned with the final observed performance, demonstrating stability and reliability at scale.

Beyond algorithmic tweaks, the framework was used to analyze compute distribution. When evaluating different scaling axes—model size, generation length, and batch size—the study found that scaling model size provides the largest returns. For example, a larger **17B Mixture-of-Experts (MoE) model** trained with ScaleRL significantly outperformed the performance of a dense 8B model, achieving better results while consuming just one-sixth of the total RL training compute.

This work establishes a robust scientific basis for RL development, providing practitioners with a predictive tool to cost-effectively evaluate new algorithms and allocate compute resources, guiding the field toward reliably scalable LLM fine-tuning.