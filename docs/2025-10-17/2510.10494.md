---
layout: paper
pdf_url: https://arxiv.org/pdf/2510.10494
permalink: 2025-10-17/2510.10494/
title: AI Breakthrough&#58; Scientists Find Way to "Trace the Traces" of LLM Reasoning,
  Cutting Compute Costs by Up to 70%
---



Large Language Models (LLMs) solve complex problems by scaling up inference, generating long, elaborate "chains of thought." While effective, this process is notoriously inefficient—models often waste vast amounts of computation following unproductive reasoning paths.

New research from a collaboration including Microsoft Research and Goethe University Frankfurt introduces a novel solution: **Latent-Trajectory (LT) signals**. These signals analyze the temporal evolution of an LLM's internal "hidden states" as it generates reasoning tokens, providing a reliable, training-free method to predict whether a thought process will lead to a correct answer.

### Understanding the Internal Journey

The core idea relies on characterizing how a model’s internal representation shifts in its vast latent space during problem-solving. Think of the LLM’s reasoning process as a journey in a complex landscape. The researchers introduced three key metrics to quantify this trajectory:

1.  **Net Change:** Measures the total displacement of the model's internal state from start to finish. A larger net change suggests a deeper shift in understanding.
2.  **Cumulative Change:** Quantifies the total distance traveled, capturing how much the model’s "thought process" wandered or overthought the problem.
3.  **Aligned Change:** Measures how consistently the model's intermediate steps move toward the final representational state—essentially checking if the journey is direct and focused.

The researchers found that successful reasoning trajectories consistently exhibit a pattern: **high Net Change** and **high Aligned Change** (a focused, directed journey) but **low Cumulative Change** (minimal wandering). Conversely, incorrect answers often involved extensive wandering and shorter overall representational shifts.

### Efficiency and Accuracy Boost

Traditionally, ensuring reliable LLM reasoning requires generating multiple reasoning paths and using computationally expensive methods like majority voting (MV) to select the best answer.

The LT signals proved significantly more reliable at predicting solution accuracy (high ROC-AUC) than conventional measures, such as output-based confidence scores (like logit margin or entropy) or superficial cross-layer metrics.

When integrated into inference-time scaling policies, LT signals delivered massive efficiency gains. By using LT signals to guide answer selection across multiple sampled generations, the models (tested across Deepseek-R1-Distill-Qwen-14B, Phi-4-Reasoning-Plus, and Qwen3-14B) preserved, and often improved, accuracy while drastically reducing compute.

Across diverse benchmarks—including graduate-level science (GPQA), mathematics (AIME), and algorithmic tasks (TSP)—LT-guided selection reduced average token usage by 48%, with some models achieving reductions of up to 70%, while simultaneously boosting average accuracy by 2.6%.

Furthermore, the predictive power of these signals often emerged very early in the reasoning trace (within the first 2,000 to 4,000 tokens). This early detection allows for the immediate termination of unproductive reasoning paths, allocating computational effort only to the most promising candidates.

This approach offers a practical, model-agnostic strategy for dramatically increasing the efficiency of complex reasoning and provides critical insight into the underlying mechanisms that differentiate productive from wasteful LLM thought processes.