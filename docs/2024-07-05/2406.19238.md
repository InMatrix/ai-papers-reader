---
layout: paper
pdf_url: https://arxiv.org/pdf/2406.19238
permalink: 2024-07-05/2406.19238/
title: LLM “Tropes” Reveal Hidden Consensus and Instability in AI Political Bias
---



A new study by researchers at the University of Copenhagen has exposed the deep, often contradictory, values embedded within leading Large Language Models (LLMs). By analyzing over 156,000 model responses to political and moral questions, the team found that LLMs' expressed biases are highly unstable and easily "steered" by prompt design.

Crucially, the study introduces a novel concept: "LLM Tropes." These are recurrent, semantically similar phrases that LLMs repeatedly use to justify their political stances, providing a fine-grained look at the underlying reasoning mechanisms that categorical surveys often miss.

### The Instability of Persona

To assess how deeply political and social values are embedded, the researchers tested six major models (including Llama 2, Llama 3, Mistral, and Mixtral) against the 62 propositions of the widely used Political Compass Test (PCT). They introduced 420 variations in prompting, instructing the models to adopt different demographic "personas"—spanning age, gender, economic class, and political orientation.

The results confirm that LLM biases are highly inconsistent. When prompted to adopt a "far right" or "far left" political orientation, models like Mixtral demonstrated dramatic shifts, moving their alignment across the PCT map purely based on the assigned persona. Demographic features like gender and economic class also resulted in significant systemic changes in political stance.

"The responses of the models can change substantially when prompted with different personas," the authors note, highlighting the potential for these latent values to be exploited or to vary wildly depending on how a user interacts with the AI.

### The Open-Ended Paradox

A central finding concerned the difference between "closed-form" answers (forcing a choice like "Strongly Agree" or "Disagree") and open-ended justifications.

Llama 3, for instance, showed a high tendency to *agree* with propositions in the closed-form setting. However, when asked to provide an open-ended justification (the base case with no demographic persona), it often refused to answer or defaulted to a neutral stance. This suggests that models, when forced into a category, reflect a perceived bias, but when left free to express themselves, they prioritize neutrality or evasion. This disparity became even more pronounced when political demographics were introduced.

### Unpacking LLM Reasoning

The paper’s most innovative contribution is the analysis of "tropes": the common, consistent linguistic patterns models employ when generating justifications. By clustering the sentences in the 70,000 open-ended responses, the researchers distilled 584 distinct tropes.

This revealed a surprising commonality beneath the surface instability of the categorical stances. Many justifications were shared across models and even across prompts with disparate political leanings.

For example, the trope *"Marijuana is less harmful than other legal substances and has medical benefits"* appeared regardless of the political orientation assigned to the LLM persona, suggesting a consistent, underlying line of reasoning hardcoded into the models. Similarly, the belief that *"A just society ensures equal opportunities for all"* was generated across multiple politically opposed prompt variations by Llama 2 and Llama 3, hinting at core principles that persist even when the models are "steered."

The findings argue that evaluating LLM values requires moving beyond simple categorical stances to robust, fine-grained analysis of the text itself, revealing how models justify their decisions in natural language.