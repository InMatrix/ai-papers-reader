---
layout: paper
pdf_url: https://arxiv.org/pdf/2407.01470
permalink: 2024-07-05/2407.01470/
title: Model Merging Breakthrough Instills Expert Domain Knowledge into AI Reward
  Systems
---



A team of researchers has introduced a novel technique called DogeRM (Domain knowledge merged Reward Model) that drastically reduces the resource costs associated with aligning large language models (LLMs). By strategically merging models, DogeRM equips general-purpose Reward Models (RMs) with high-level domain expertise—such as advanced math or coding skills—without requiring the prohibitively expensive collection of expert-annotated preference data.

Reward Modeling is a critical step in Reinforcement Learning from Human Feedback (RLHF), the popular strategy used to ensure LLMs adhere to human instructions and values. Traditionally, RMs are trained on massive datasets where human annotators judge which of two model responses is preferable. However, gathering this paired preference data becomes slow and costly when dealing with specialized fields like medicine or advanced mathematics, necessitating expert review.

DogeRM circumvents this bottleneck by leveraging readily available Supervised Fine-Tuning (SFT) data, which is much cheaper to acquire. The method takes a general RM (like a LLaMA-2 RM trained on broad preference data) and merges its underlying parameters with those of an SFT-trained domain-specific LLM (like MetaMath-7B, a specialist math model).

This merging is accomplished through weighted averaging of parameters, effectively transferring the specialist model's *knowledge* into the RM's *preference function*.

The results across specialized benchmarks, including RewardBench and Auto-J Eval, were striking. When tested on reasoning tasks, DogeRM significantly outperformed the baseline RM. For instance, merging a LLaMA-2 RM with the MAmmoTH-7B math model yielded a performance enhancement of 17% in the math reasoning category on RewardBench. Similarly, incorporating a fine-tuned Code Model boosted coding performance by 5.4% and 6% on the respective benchmarks.

The true intuition behind DogeRM's success lies in its ability to correct fundamental reasoning errors. In a case study involving a complex math problem—calculating the minimum number of times buckets must be lowered to empty wells using the ceiling function—the baseline RM incorrectly preferred a response using simple integer division (floor function). After merging with the domain-specific model, DogeRM corrected its prediction, assigning a higher reward to the response that correctly applied the mathematical concept of `math.ceil`. This shows the merged RM gained a more accurate understanding of *why* one answer is superior.

Beyond accuracy in RM evaluation, DogeRM also demonstrated utility in reranking generated responses (Best-of-N sampling), improving the likelihood of selecting the best answer from a pool of candidates. For the difficult GSM8K grade school math benchmark, merging models resulted in accuracy improvements of up to 5%.

The researchers emphasize that the framework is architecture-agnostic, having successfully generalized the approach to Mistral-based models, and confirming that the integration of multiple related domain models (e.g., combining math and code knowledge) can further enhance performance in targeted areas. DogeRM presents a strong case for using model merging as a scalable and efficient path toward domain-specific LLM alignment.