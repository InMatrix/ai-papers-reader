---
layout: paper
pdf_url: https://arxiv.org/pdf/2407.00114
permalink: 2024-07-05/2407.00114/
title: OmniJARVIS&#58; Unified Tokenization Unlocks Superior Reasoning and Action
  in Open-World AI
---



A team of researchers has introduced OmniJARVIS, a novel Vision-Language-Action (VLA) model designed to overcome a critical bottleneck in building autonomous agents: integrating complex reasoning with precise, efficient control in challenging open-world environments like Minecraft.

Unlike previous VLA models that either rely on separate natural language goals passed to a controller or attempt to generate raw, low-level control commands directly, OmniJARVIS achieves strong performance by adopting a unified tokenization scheme that treats vision, language, and physical actions as a single, contiguous data stream for an autoregressive transformer.

The core innovation is the introduction of **Behavior Tokenization**. Low-level action trajectories—such as pressing keys and moving the mouse—are notoriously difficult for large models to process over long sequences. OmniJARVIS addresses this by using a self-supervised behavior encoder, based on a Finite Scalar Quantizer (FSQ), to convert long streams of raw actions into discrete, semantically meaningful “behavior tokens.”

This process is analogous to translating a thousand individual movements into a single, high-level instruction. For example, instead of modeling a sequence of a hundred micro-commands like "move forward," "look down," and "left-click," the system generates a compact token, such as `TOKEN[BEHAVIOR-1375]`, which the policy decoder recognizes as the complete subroutine for "crafting an iron pickaxe."

These learned behavior tokens are then augmented into the vocabulary of a pre-trained Multimodal Language Model (like LLaVA). This allows the entire decision-making process—including task instructions, memories, visual observations, internal chain-of-thoughts, and the resulting behavior tokens—to be modeled end-to-end as a unified, autoregressive sequence.

The ability to integrate actions directly into the language vocabulary significantly enhances the agent's planning capabilities. By predicting a short sequence of high-level behavior tokens, OmniJARVIS can maintain long-horizon goals without drowning in low-level environmental noise.

Evaluations conducted in the open-world Minecraft environment demonstrated OmniJARVIS’s superior ability to follow complex, long-horizon programmatic instructions. In tests involving tasks that require chaining multiple skills, such as obtaining a diamond pickaxe (which necessitates a long sequence of gathering, crafting, and mining steps), OmniJARVIS achieved an average success rate of 59%, substantially surpassing strong baselines like DEPS (43%) and GROOT (2%).

Furthermore, the unified structure proved crucial for reasoning. The model demonstrated robust performance on open-ended embodied question-answering tasks by using synthesized "thought tokens" alongside memory and observation to inform its subsequent actions, validating the researchers’ hypothesis that successful open-world decision-making requires modeling the full human-like loop of mental, verbal, and physical interactions. The team also confirmed that the model exhibits promising scaling potential, with validation loss decreasing consistently as model and data scales increase.