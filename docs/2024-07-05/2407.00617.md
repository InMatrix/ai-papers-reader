---
layout: paper
pdf_url: https://arxiv.org/pdf/2407.00617
permalink: 2024-07-05/2407.00617/
title: Iterative Nash Policy Optimization Elevates LLM Alignment Beyond Simple Preferences
---



A new machine learning technique called Iterative Nash Policy Optimization (INPO) promises to bring large language model (LLM) alignment closer to the complexities of human judgment, achieving substantial performance gains over existing methods.

Developed by researchers at the University of Illinois Urbana-Champaign and Tencent AI Lab, INPO reformulates the standard Reinforcement Learning with Human Feedback (RLHF) problem. Instead of relying on the restrictive Bradley-Terry (BT) model, INPO utilizes a general preference framework solved through a self-play, game-theoretic approach.

### The Limits of Simple Preference

The success of LLMs like ChatGPT and Claude hinges on RLHF, which trains models to align with human values. However, most popular RLHF methods, including Proximal Policy Optimization (PPO) and Direct Preference Optimization (DPO), are built upon the BT model.

The BT model assumes that preferences are always transitive: if a user prefers response A over B, and B over C, they must prefer A over C. As the authors note, human decision-making is often far more complex, especially in subjective or nuanced tasks, leading to non-transitive preferences. Basing alignment on the BT model can cap the LLM's potential accuracy, especially since BT-based reward models often only achieve about 70% accuracy on preference benchmarks.

### A Game of Self-Improvement

INPO tackles this by modeling LLM alignment as a two-player symmetric game. The goal is to find a "Nash policy"—a stable equilibrium where the LLM cannot improve its performance by unilaterally changing its strategy.

The core intuition is drawn from self-play in competitive games like Chess or Go: the current policy (the LLM) is pitted against itself in an iterative learning process known as no-regret learning. In each iteration, the model learns the optimal policy against its previous version, ensuring constant self-improvement while minimizing regret.

Crucially, this approach introduces a novel loss objective that allows the algorithm to learn directly from human preference datasets (which response, $y_w$, was preferred over another, $y_l$). This design bypasses a significant computational hurdle faced by prior online RLHF methods: the need to estimate the *expected win rate* for every potential response. For a vast response space, calculating these win rates is prohibitively expensive and prone to estimation errors. By using simple binary preference signals, INPO gains both theoretical rigor and practical efficiency.

### Superior Performance on Tough Benchmarks

The INPO algorithm demonstrated its effectiveness across several competitive LLM evaluation benchmarks using a LLaMA-3-8B base model.

On the rigorous AlpacaEval 2.0 benchmark—which measures instruction-following capability—INPO (using a preference model) achieved a 42.6% length-controlled win rate. This represents a substantial relative improvement of at least 27.7% compared to other state-of-the-art online RLHF algorithms, confirming the advantage of considering general preferences. It also secured a 37.8% win rate on Arena-Hard, a benchmark focused on complex technical problem-solving.

Furthermore, INPO was found to preserve academic capabilities, achieving the highest average score (60.0%) across six academic benchmarks (including MMLU and TruthfulQA) when compared to other 8B-parameter alignment methods. The results indicate that this robust, game-theoretic approach is not only more effective at capturing sophisticated human preferences but also more stable and efficient for aligning LLMs in practice.