---
layout: paper
pdf_url: https://arxiv.org/pdf/2407.00468
permalink: 2024-07-05/2407.00468/
title: New Benchmark Reveals GPT-4o, Top Multimodal AIs Lag Human Performance by Over
  30%
---



A new study published by researchers from Peking University, the Chinese Academy of Medical Sciences, and Alibaba Group has exposed a "trustworthiness crisis" in how Large Multimodal Models (LMMs) like GPT-4o are currently evaluated, leading to inflated performance scores.

The paper, titled "MMEVALPRO: Calibrating Multimodal Benchmarks Towards Trustworthy and Efficient Evaluation," introduces a rigorous new benchmark designed to prevent models from "hacking" multiple-choice questions (MCQs) without genuine visual understanding or reasoning.

### The Illusion of Intelligence

Existing benchmarks, including popular datasets like MMMU, ScienceQA, and MathVista, often utilize MCQs combining an image and a text prompt. The researchers found that these tests suffer from a Type-I error: models often provide the correct answer through shortcuts, not true comprehension.

The primary evidence came from a "Seeing-or-Not Comparison." When researchers withheld the image input from purely text-based Large Language Models (LLMs)—models that lack any visual capabilities—these LLMs still achieved surprisingly high scores. In some cases, the best LMM’s performance was only 1.1 times better than the best LLM’s performance on the original MMMU dataset.

This narrow gap showed that LLMs were succeeding through “data leakage” (memorizing answers from training data) or “educated guessing,” meaning the visual information in the benchmark was often not necessary.

### Introducing the Triplet Evaluation

To fix this flaw, the MMEVALPRO benchmark re-engineers 2,138 original MCQs into 6,414 unique "question triplets." Each triplet consists of:

1.  **The Original Question:** The complex multimodal problem.
2.  **A Perception Question:** A prerequisite question testing visual understanding of the image.
3.  **A Knowledge Question:** A prerequisite question testing the underlying knowledge required for the solution.

The primary metric, **Genuine Accuracy (GA)**, only grants a point if the model correctly answers all three questions in the triplet simultaneously.

For example, a complex geometry problem might ask for the measure of an angle ($\angle BOC$) within a triangle figure. An advanced LMM like GPT-4o might quickly recall the correct geometric formula ($\angle BOC = 90^\circ + 1/2 \angle BAC$) and arrive at the correct numerical answer for the Original Question. However, MMEVALPRO tests whether the model also correctly answers the Perception Question ("How many triangles are there in the figure?") and the Knowledge Question ("What is the relation between $\angle BAC$ and $\angle BOC$?") based on the image context.

The study found that even when the LMM answered the original question correctly, it often failed the two anchor questions, demonstrating an "answer consistency" failure and low Genuine Accuracy.

### True Performance Gap Revealed

When tested on MMEVALPRO, the performance of all models dropped drastically, and the functional difference between vision-enabled LMMs and non-vision LLMs became clearer. For instance, the best LLM's performance (non-vision GPT-4o) trailed the best LMM's performance (vision-enabled GPT-4o) by 23.09% in Genuine Accuracy—a gap much larger and more intuitive than observed on previous tests.

Crucially, the evaluation revealed that even the most advanced LMMs, including GPT-4o and Qwen-VL-Max, exhibit a substantial gap of over 30% in Genuine Accuracy compared to human graduate students. This contrasts sharply with the average 8.03% gap seen on prior benchmarks.

The large drop highlights that MMEVALPRO is a significantly more challenging and trustworthy evaluation, providing a sober assessment of where current AI models truly stand in the pursuit of AGI.