---
layout: paper
pdf_url: https://arxiv.org/pdf/2407.01370
permalink: 2024-07-05/2407.01370/
title: New "Haystack" Benchmark Reveals Current LLMs and RAG Systems Fail Complex
  Long-Context Summarization
---



Despite recent breakthroughs allowing Large Language Models (LLMs) to process millions of input tokens, a new benchmark reveals that even the most advanced systems are severely challenged by complex long-context tasks, consistently performing worse than human experts.

Researchers at Salesforce AI Research introduced the "Summary of a Haystack" (SummHay), a synthetic task designed to replace simpler evaluations like the widely cited "Needle-in-a-Haystack." While traditional needle tests only require a model to recall a single, isolated fact, SummHay forces models and Retrieval Augmented Generation (RAG) systems to synthesize, reason, and attribute information scattered across a vast corpus.

A typical Haystack consists of 100 documents, totaling roughly 100,000 tokens of text, across domains like news or conversation transcripts. The documents are intentionally synthesized to contain multiple repeating "insights." The system is then given a query (e.g., "Summarize top insights about student stress management") and must produce a bulleted summary.

Crucially, the task is measured by two key metrics: **Coverage** (ensuring all expected insights are included) and **Citation** (precisely linking each bullet point to *all* supporting source documents).

### Performance Gap Is Significant

The evaluation of 10 state-of-the-art LLMs and 50 RAG systems confirmed that SummHay is an open challenge. Human annotators, working in a simplified setting, achieved a Joint Score (combining Coverage and Citation quality) of 56.1%. In comparison, the top-performing commercial models struggled significantly.

The best realistic performance—achieved by a RAG system combining the advanced Cohere Rerank3 retriever with the Gemini 1.5 Pro summarizer—reached only 36.0% Joint Score. Even when granted an "Oracle" signal (perfect knowledge of relevant documents), the top model (Claude 3 Opus) only achieved 44.6%, underscoring a persistent gap of over 10 points between current AI capabilities and human accuracy.

For instance, if a query asks about student study techniques, one insight might be that "A student suggests taking a 5-minute break after every 25 minutes of studying, and mentions the Pomodoro technique as helpful." A high score requires the summary to both include this idea (high Coverage) and cite every document in the Haystack where this fact appeared (e.g., [8, 32, 79, 83]).

### The RAG vs. Full-Context Trade-Off

The results reveal a clear trade-off between RAG systems and LLMs relying on full context. Long-context LLMs like GPT-4o and Claude 3 Opus, when given the entire 100,000-token Haystack at once, achieved strong *Coverage* (including most insights) but delivered very poor *Citation* quality, leading to Joint Scores below 20%.

RAG pipelines, which pre-filter the 100k-token Haystack down to a smaller, more relevant 15k-token chunk, dramatically improved Citation scores. Researchers attribute this to the RAG process effectively removing irrelevant noise, allowing the model to focus its attribution efforts. However, this often came at the cost of lower Coverage, as the retriever sometimes missed relevant documents.

Furthermore, testing exposed the "lost in the middle" phenomenon: when documents were randomly ordered, models like GPT-4o and Claude 3 Opus consistently performed better when relevant documents were placed at the very beginning or end of the context window, showing a sensitivity bias that degrades reliability in real-world applications where document order is arbitrary.

SummHay provides a robust evaluation framework for systems where reliable reasoning and verifiable citations are essential, particularly in enterprise applications.