---
layout: paper
pdf_url: https://arxiv.org/pdf/2511.11793
permalink: 2025-11-21/2511.11793/
title: MiroThinker Introduces 'Interactive Scaling' to Push Open-Source AI Agents
  Past Commercial Rivals
---



A new open-source large language model (LLM) agent, MiroThinker v1.0, is redefining the path toward autonomous research AI, proposing that the depth of an agent’s interaction with its environment is as crucial as its size and context window.

Developed by the MiroMind Team, MiroThinker aims to bridge the performance gap between open-source models and proprietary systems like ChatGPT Agent and Claude Research. While previous AI development focused primarily on scaling up model parameters (size) or token limits (context length), MiroThinker introduces "interactive scaling"—systematically training the model to handle deeper and more frequent iterative reasoning and tool use.

The key to this scaling is allowing the agent to engage in extensive trial-and-error, correcting mistakes and refining its strategy using real-time feedback. Unlike traditional LLMs that often struggle with long, complex reasoning chains and isolated tool use, MiroThinker leverages reinforcement learning (RL) to internalize the value of sustained interaction.

To facilitate this deep engagement, the 72B-parameter MiroThinker model is equipped with a 256K context window and is capable of performing up to **600 tool calls per task**. This represents a massive leap from older open-source agents, which were often constrained to fewer than 100 interactions.

### Performance Rivals Proprietary Giants

This deep interactive training translates directly into superior performance across complex, real-world research benchmarks. MiroThinker v1.0 consistently achieves state-of-the-art results among open-source counterparts and actively approaches the level of proprietary models often compared to the hypothetical "GPT-5-high" standard.

For example, on the challenging GAIA-Text-Only benchmark, which tests general AI assistant capabilities, the MiroThinker 72B model achieved an accuracy of 81.9%, surpassing its strongest open-source rival, MiniMax-M2, by 6.2 percentage points. Furthermore, on the rigorous Humanity's Last Exam (HLE) benchmark, the agent scored 37.7%, demonstrating stronger reasoning capacity than the proprietary GPT-5-high model in the evaluation.

To give a concrete intuition for interactive scaling: when tasked with a complex problem—such as synthesizing evidence from three competing scientific papers, cross-referencing public figures mentioned in each, and running a simple Python script to analyze a dataset—a conventionally trained agent might attempt one or two searches before concluding. MiroThinker, guided by RL, learns to execute dozens or even hundreds of cycles of **Thought, Action, and Observation** (the ReAct paradigm), allowing it to iteratively search for information, execute code in a sandbox, intelligently scrape web results for specific details, and consolidate evidence.

The deep interactive trajectories enabled by RL are shown to directly correlate with accuracy gains, establishing interaction depth as a third critical dimension for building next-generation AI research agents.

The MiroMind Team has released MiroThinker v1.0 in 8B, 30B, and 72B variants, making this interaction-scaled performance accessible to the wider open-source community for further research and development.