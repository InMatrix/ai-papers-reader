---
layout: paper
pdf_url: https://arxiv.org/pdf/2511.14659
permalink: 2025-11-21/2511.14659/
title: Next-Generation Robotics Model NORA-1.5 Achieves Unprecedented Reliability
  Using AI-Generated Rewards
---



In a significant advance for embodied AI, researchers have unveiled NORA-1.5, a new Vision-Language-Action (VLA) model that dramatically improves robotic reliability and generalization. Built upon the powerful NORA backbone, NORA-1.5 integrates a novel post-training paradigm that leverages lightweight, synthetic rewards derived from a world model, sidestepping the typically slow and expensive process of training robots using real-world data rollouts.

Vision-Language-Action models are crucial for enabling robots to follow natural language commands (e.g., "Pick up the soda bottle and move it near the orange"). However, existing VLAs often struggle with robustness, especially when generalizing to new environments, unseen objects, or different robot bodies—a critical barrier to real-world deployment.

### Combining Speed with Wisdom

The core architectural enhancement of NORA-1.5 is the integration of a flow-matching-based action expert with the pre-trained autoregressive VLA model. This hybrid design allows the model to generate smooth, fast actions while maintaining the long-horizon planning capabilities derived from its robust VLA backbone.

However, the real breakthrough lies in the post-training process, which uses Direct Preference Optimization (DPO). Instead of collecting millions of new, costly, human-labeled demonstrations or time-intensive real-robot rollouts for reinforcement learning, NORA-1.5 generates its own high-quality training signals.

The researchers achieved this by creating a synthetic reward mechanism with two complementary parts:

1.  **World Model (WM) Guidance:** An action-conditioned world model (a variant of V-JEPA2) simulates the consequences of a robot’s generated actions, estimating if they successfully achieve the specified goal or immediate subgoal.
2.  **Action-Based Reward (GTA):** A distance-based heuristic that anchors the reward signal by measuring how far the generated action deviates from known successful trajectories in the training data.

The model uses these signals to construct preference pairs (e.g., "Action A is better than Action B because A moves closer to the subgoal"), which DPO then uses to refine the policy.

### Robustness in the Real World

This reward-driven refinement significantly boosted NORA-1.5’s performance across diverse simulation benchmarks (SimplerEnv and LIBERO) and in real-world tests using the Galaxea A1 robotic arm.

One of the most striking results was the increase in robustness, particularly in tasks involving distractors or unseen objects. In real-world tests where the robot had to manipulate objects like "Put grape in plate" with an eggplant acting as a distractor, the DPO-trained NORA-1.5 significantly reduced accidental grasps of the wrong item.

The improved efficiency is also intuitive: for a task requiring the Galaxea A1 arm to grasp a target, the non-DPO model executed a “zig-zag” trajectory requiring about 40 action chunks (strokes). After DPO post-training, the robot’s movement became "smoother and more consistent," completing the task in only 8 strokes.

By proving that simple, lightweight world-model-derived rewards combined with DPO can yield significant and reliable performance gains, NORA-1.5 establishes a scalable and data-efficient pathway for the future of generalist robot policy refinement.