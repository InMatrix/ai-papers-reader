---
layout: paper
pdf_url: https://arxiv.org/pdf/2511.13026
permalink: 2025-11-21/2511.13026/
title: New AI Framework "REVISOR" Gives Video Models Multimodal Introspection for
  Long-Form Content
---



In a significant advance for artificial intelligence, researchers have unveiled **REVISOR** (REflective VIsual Segment Oriented Reasoning), a novel framework that equips Multimodal Large Language Models (MLLMs) with the ability to perform true multimodal self-reflection, dramatically boosting their accuracy in understanding long-form videos.

While traditional MLLMs have excelled at visual tasks using self-reflection—revising their reasoning based on a purely text-based review—this approach fails when confronted with lengthy, dynamic video inputs.

"Long-form video understanding involves richer and more dynamic visual inputs," the authors note. "Purely text-based reflection is insufficient to correct reasoning errors without explicitly reconsidering visual information."

### The Dilemma of Text-Only Reflection

Current reflection mechanisms function like proofreading a document: they check the logical steps of the internal text-based reasoning trace. This works for static images, but for a two-hour film, re-evaluating the text trace often won't resolve visual ambiguities.

For instance, if a question asks who a character catches mid-air during a chaotic fight sequence (as seen in the paper's example), the initial quick textual inference might incorrectly guess "The Flash." A standard text-based reflection, based only on the sparse transcript or low-resolution visual tokens, confirms the error.

REVISOR solves this by integrating a "Visual Toolbox." It operates in two stages: First, the MLLM performs an initial inference and simultaneously proposes the specific time segment (e.g., "220-260 seconds") that requires closer examination. The Visual Toolbox then densely resamples high-resolution frames from only that critical segment, providing the model with fresh, fine-grained visual evidence. In the second stage, the model performs true **multimodal reflective reasoning**, integrating its initial textual logic with the newly retrieved visual cues to correct the initial error, identifying the correct character (e.g., "Batman") based on the detailed armored silhouette visible in the zoomed frames.

### Learning to Focus: The DADR Mechanism

A key challenge in training this system is ensuring the model doesn't just guess the right answer, but learns to accurately *locate* the most essential video evidence.

To address this, the team introduced the **Dual Attribution Decoupled Reward (DADR)** mechanism, integrated into the reinforcement learning (RL) training process. DADR splits the reward into two parts:

1.  **Final Answer Verification Reward:** Rewards the model for getting the ultimate answer correct.
2.  **Causal Segment Sufficiency Reward ($R_{causal}$):** Rewards the model only if the correct answer can be derived exclusively from the *specific, short video segment* it chose to review.

This innovation prevents the model from relying on irrelevant or overly long segments, effectively teaching it introspective precision. By enforcing causal alignment between the model's reasoning and the selected evidence, DADR ensures the MLLM learns to pinpoint the minimal, most informative visual cues necessary to resolve the query.

REVISOR achieved substantial performance gains across four widely used benchmarks, including a 2.8% improvement on the long-form subset of VideoMME and a 2.5% boost on MLVU, showcasing its robust capability to handle videos stretching up to 120 minutes. The framework represents a major step toward building MLLMs capable of deep, reliable comprehension of dynamic, extended video content.