---
layout: paper
pdf_url: https://arxiv.org/pdf/2511.14460
permalink: 2025-11-21/2511.14460/
title: Agent-R1 Framework Trains Powerful LLMs to Become Autonomous Agents Using End-to-End
  Reinforcement Learning
---



A team of researchers has introduced Agent-R1, a novel training framework designed to transform Large Language Models (LLMs) from sophisticated text generators into truly autonomous, interactive agents capable of complex, multi-step problem-solving. By systematically extending traditional Reinforcement Learning (RL) techniques, Agent-R1 tackles the instability and complexity inherent in training LLMs to use external tools and adapt to dynamic environments.

While LLMs have excelled at static tasks like code generation, applying them as agents—systems that must make sequential decisions, interact with tools (like search engines), and retain memory across multiple turns—presents unique challenges. Standard RL approaches fail because they treat the process as a single, static sequence, ignoring the unpredictable, multi-turn nature of real-world interaction.

Agent-R1 resolves this by redefining the core components of the Markov Decision Process (MDP) to suit agentic behavior.

### The Detective’s Case File: Redefining State and Action

In traditional LLM training, the *state space* only includes the current prompt and generated text. Agent-R1 expands this significantly to maintain a comprehensive *history* of all multi-turn interactions, including every tool invocation and subsequent environmental feedback.

To illustrate, a conventional LLM solving a problem is like a person writing a single draft. An Agent-R1 LLM is like a detective keeping a meticulous case file, where the state includes the initial instructions, all previous actions taken (e.g., "called the database API"), and all resulting evidence (e.g., "database returned result X").

The framework achieves its flexibility through a modular architecture centered on two core modules: `Tool` and `ToolEnv`. The `Tool` is an executor—it performs atomic actions, such as executing a piece of code or calling an external search API. The crucial orchestrator is the `ToolEnv`, which acts as the environment’s interpreter. After a `Tool` returns raw data, the `ToolEnv` determines how that data affects the agent's perceived state, calculates immediate rewards, and prepares the next state for the agent.

### Precision Training with Intermediate Rewards

A key innovation is the refined reward system, which moves beyond sparse feedback. Instead of waiting until the entire multi-step task is complete to assign a single *final outcome reward* (e.g., $+10$ for a correct answer), Agent-R1 incorporates *intermediate process rewards*.

For example, when an agent successfully formulates and executes a tool call to retrieve information, the `ToolEnv` immediately assigns a positive process reward. This dense feedback is critical for training, guiding the agent through complex reasoning pathways.

Furthermore, Agent-R1 introduces an "Action Mask" and "Advantage Mask" to ensure precise credit assignment. In multi-turn dialogue, the LLM’s input sequence contains the initial prompt, the agent's generated text (the action), and the external environment's response (the feedback). The Action Mask ensures that the model’s learning updates are applied only to the tokens representing the agent’s actual decisions, preventing extraneous environmental text or initial prompts from skewing the policy optimization.

### Validation in Multi-Hop Reasoning

The team validated Agent-R1 on Multi-hop Question Answering (QA) benchmarks, a demanding task requiring sequential information retrieval and cross-document logical chaining. The results show that all Agent-R1 trained models—using algorithms like PPO and GRPO—substantially outperformed baseline methods, achieving performance improvements of 2.5 times or more over methods like Naive RAG (Retrieval-Augmented Generation).

These experimental gains validate Agent-R1’s ability to train LLMs proficiently in tool use and complex multi-turn decision-making, setting a strong foundation for building scalable and unified RL training platforms for future intelligent agents.