---
layout: paper
pdf_url: https://arxiv.org/pdf/2511.14159
permalink: 2025-11-21/2511.14159/
title: New Benchmark Exposes Vision-Language Models’ Fundamental Weakness to Misleading
  Visual Cues
---



A new comprehensive benchmark, MVI-Bench (Misleading Visual Input Benchmark), reveals that state-of-the-art Large Vision-Language Models (LVLMs)—including proprietary giants like GPT-5 and Gemini—are highly vulnerable to natural, yet subtly misleading, visual inputs. These models, crucial for real-world applications ranging from autonomous systems to image analysis, struggle with ambiguities that humans can often resolve through context.

The paper, "MVI-Bench: A Comprehensive Benchmark for Evaluating Robustness to Misleading Visual Inputs in LVLMs," addresses a major gap in AI evaluation, moving beyond simple text-based deception or adversarial attacks. Researchers found that when exposed to images containing natural visual confusion, LVLM performance degrades substantially, with some popular open-source models seeing accuracy drop by nearly 50%.

MVI-Bench is structured as a paired Visual Question Answering (VQA) benchmark, where each instance consists of a "normal" image and a corresponding "misleading" image. These pairs are nearly semantically identical, differing only by the subtle visual cue designed to confuse the model, allowing for a controlled analysis of robustness.

### Six Categories of Visual Confusion

The benchmark organizes misleading cues into a taxonomy covering three hierarchical levels and six categories, reflecting common human perceptual errors:

1.  **Visual Concept Misleading:** Confusing objects that look similar but are semantically different. For instance, models are asked to identify an object that looks like *French fries* in the normal image but is replaced by a visually similar *umbrella* in the misleading counterpart.
2.  **Visual Attribute Misleading:** Confusion over fine-grained properties like material or texture. An example involves models confusing items based on whether a *spoon* is made of conductive metal or non-conductive plastic.
3.  **Visual Relationship Misleading (Spatial Reasoning):** This proved the hardest area for models. It includes **Mirror Reflection**, where models confuse reflections for actual, real-world objects (e.g., counting two chairs instead of one chair and its reflection), and **Occlusion Confusion**, where objects are partially hidden.

### Perception, Not Just Reasoning, is the Bottleneck

To quantify the degradation, the researchers introduced **MVI-Sensitivity**, a metric measuring the relative performance drop from normal to misleading inputs. The experiments on 18 state-of-the-art LVLMs yielded several crucial insights.

First, LVLMs demonstrate a pervasive lack of robustness, with even high-end proprietary models showing overall MVI-Sensitivity above 20%. GPT-5-Chat, for example, saw its accuracy plummet from 90% on normal images to 61% when exposed to visual illusions.

Second, the study found that **visual perception**—the ability of the model’s vision encoder to accurately encode details—is the primary bottleneck, outweighing the benefit of enhanced reasoning. When a strong auxiliary model was used to provide detailed text captions (acting as "enhanced eyes"), the performance on misleading images improved significantly, sometimes surpassing models three times the size.

Finally, the analysis uncovered instances of **spurious correlation**, where models answer misleading questions correctly by relying on irrelevant artifacts instead of true visual evidence. In one example, a model incorrectly counted a visible *receipt* as an additional *book* to arrive at the right answer for the misleading image, confirming that the model wasn't relying on true object recognition but on a false shortcut cue.

The findings suggest that simply scaling up the language model component will not solve these fragility issues. Developing more reliable and robust LVLMs will require targeted efforts to improve fine-grained visual perception and enforce causally aligned visual reasoning.