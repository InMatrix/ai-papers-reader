---
layout: paper
pdf_url: https://arxiv.org/pdf/2511.13524
permalink: 2025-11-21/2511.13524/
title: Socially Grounded AI&#58; New Simulator Teaches Robots to Ask for Directions
---



A team of researchers has unveiled FreeAskWorld, a sophisticated, closed-loop simulation framework designed to push embodied artificial intelligence (AI) beyond simple instruction following and into complex, human-centric social interaction. This new platform, coupled with a massive synthetic dataset, addresses a crucial bottleneck in Vision-and-Language Navigation (VLN): the ability of AI agents to engage in dynamic, multi-turn dialogues to solve navigation problems.

Traditional VLN tasks, such as those relying on the Room-to-Room benchmark, confine agents to following static, pre-provided instructions (e.g., "Walk straight, turn left at the couch, and stop by the window"). FreeAskWorld, however, introduces the **Direction Inquiry Task**, which requires agents to actively seek external help and adapt their plans in real-time.

### Interaction as Information

The framework leverages Large Language Models (LLMs) to model high-level human behavior, intention, and semantic instruction generation.

To visualize the shift, consider an AI agent trying to find a specific store in a crowded, reconstructed urban environment. In the old paradigm, if the agent got lost halfway, the episode failed. In FreeAskWorld, the agent can initiate an inquiry, approaching a human avatar—modeled with a unique personality, schedule, and navigation style—and ask: “Excuse me, where is the nearest Women's and Children's Clothing Store?”

The simulated human, powered by an LLM knowledge base, responds with context-aware, human-like guidance: *"Of course, I can guide you! You're starting from 'The Best Pork Stir-Fry.' First, head diagonally left toward that area..."*

If the agent fails to reach the destination after following the initial guidance, it automatically initiates a new inquiry, showcasing self-assessment and real-time adaptation. This closed-loop interaction underscores the paper’s key finding: interaction itself serves as an additional information modality, enabling agents to acquire knowledge that static perception alone cannot provide.

### Simulating a Dynamic Digital Twin

Realism is key to validating these social skills. FreeAskWorld achieves this through dynamic systems for traffic, weather (including rain and fog), and complex human avatars. The human avatars are designed using a "People Simulation" module that integrates high-level scheduling (e.g., Lucy needs to leave home by 8:30 a.m. to get to the hospital), animation control, and socially compliant low-level navigation using a Social Force Model. This ensures agents avoid collisions with pedestrians and vehicles naturally.

The accompanying FreeAskWorld Dataset is one of the largest and most diverse of its kind, featuring 63,429 annotated sample frames, over 17 hours of interaction data, and diverse task types across reconstructed indoor and outdoor scenes. The rich data includes panoramic RGB images, dialogue transcripts, and 2D occupancy heatmaps for robust training.

Experiments show that existing VLN models, when fine-tuned on the FreeAskWorld data, demonstrate enhanced semantic understanding and interaction competence. Crucially, human baselines in the closed-loop setting who were allowed to ask questions achieved a success rate (82.6%) more than double that of models unable to perform inquiries, validating the necessity of social interaction for robust navigation in complex, dynamic environments.