---
layout: paper
pdf_url: https://arxiv.org/pdf/2511.11831
permalink: 2025-11-21/2511.11831/
title: Leading AI Models Are ‘Globally Blind,’ Failing Basic Visual Perception Test
---



A new diagnostic benchmark has revealed a critical failure in the most powerful Large Vision-Language Models (LVLMs), including OpenAI’s GPT-4o and Google’s Gemini-2.5 Pro. Despite their celebrated abilities in complex visual reasoning, these state-of-the-art models are fundamentally incapable of perceiving the overall global structure of an image, often performing no better than random chance on the simplest tests.

The researchers from Tsinghua University, who developed the new benchmark called **TopoPerception**, argue that conventional evaluation methods contain "local shortcuts." These shortcuts allow models to succeed by identifying small, local features (like a specific texture or an isolated object) without grasping the image's complete configuration. For instance, an LVLM might correctly identify a soccer ball by recognizing its characteristic stitching pattern, even if it completely fails to understand the overall connectivity or spatial relationships within the scene.

To eliminate these confounding factors, TopoPerception isolates global perception using topological properties—attributes of an image, such as connectivity and the number of closed loops (or "holes"), that are independent of local color, texture, or semantics.

The benchmark employs a scalable dataset of synthetic, maze-like images. The models are presented with an image and a fixed multiple-choice question asking them to identify the image’s global topological class.

For example, a model might be shown a simple grid pattern and asked: "Which of the following best describes the topological structure of the white regions?" The options could include: "A single closed loop," "Two separate closed loops," or "Two closed loops, with one completely enclosed inside the other." To answer correctly, the model must scan the entire visual field, count the "holes," and determine how they relate spatially.

The results are stark. Across the lowest difficulty level (Level 0), which features images comparable in resolution to the MNIST dataset, the evaluated models demonstrated severe deficiencies. Most models hovered around 20% accuracy—a figure statistically indistinguishable from random guessing (which would be 33.3% if they focused only on the three correct topological categories). This suggests current LVLMs suffer from a substantial loss of global visual features during their initial visual processing pipeline.

Even more troubling is a counter-intuitive trend observed across model families: models with stronger, larger reasoning capabilities exhibited lower accuracy. For example, in the Gemini family, the smaller Gemini-2.5-flash outperformed the more powerful Gemini-2.5-pro. The researchers hypothesize that when global visual input is fragile, models with advanced reasoning abilities may default to language-based reasoning, inadvertently overriding or distorting the weak visual signals they receive.

These findings suggest that merely scaling up model size or reasoning capabilities is insufficient, and may even be detrimental, to robust visual perception. The study highlights a critical architectural bottleneck: the process by which high-dimensional visual data is compressed into discrete tokens for the language model sacrifices crucial global structural information.

TopoPerception offers a new diagnostic tool for researchers, suggesting that future LVLMs may require entirely new architectures—such as more expressive visual encoders or mechanisms that allow the model to re-examine the image at every step of its linguistic reasoning—to truly grasp the world they observe.