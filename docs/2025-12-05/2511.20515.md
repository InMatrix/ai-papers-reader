---
layout: paper
pdf_url: https://arxiv.org/pdf/2511.20515
permalink: 2025-12-05/2511.20515/
title: ''
---


# New Benchmark Reveals Major Blind Spots in State-of-the-Art AI Vision Models

A new comprehensive benchmark, AlignBench, has exposed critical vulnerabilities in modern Vision-Language Models (VLMs), revealing that even top-tier systems struggle to detect subtle, fine-grained factual errors—or "hallucinations"—in detailed image descriptions.

Developed by researchers from OMRON SINIC X Corporation and The University of Osaka, AlignBench addresses the limitations of older evaluation methods that relied on simple captions or rule-based modifications. Instead, AlignBench uses detailed, multi-sentence captions generated by a diverse array of advanced generative VLMs and text-to-image models (including GPT-4o, Llama-4, and Stable Diffusion).

By evaluating models against these challenging, organically generated captions—comprising nearly 90,000 human-annotated sentences—the researchers found that VLMs acting as alignment "detectors" were often overconfident in incorrect statements, especially when the errors were nuanced.

The core challenge lies in detecting subtle visual inconsistencies that often plague advanced VLM outputs. For instance, a caption might accurately describe a man holding a surfboard in the ocean, but then hallucinate a non-existent detail, such as claiming the man is holding a **yellow** surfboard when it is actually blue. Or, an image of a rabbit might be correctly described, but the caption falsely claims the animal is "intently **looking at the carrot**," requiring fine-grained visual attention to detect the directional error.

Benchmarking a wide range of popular models yielded three critical insights into VLM behavior:

First, older, foundational image-text alignment models, particularly those based on the CLIP architecture (including specialized models like TripletCLIP), were found to be "nearly blind" to these fine-grained hallucinations, scoring only around 50% accuracy—the equivalent of random guessing.

Second, successful VLM detectors, while significantly better, exhibit problematic scoring biases. Models consistently show a strong **positional bias**, assigning higher confidence scores to sentences appearing early in a caption, irrespective of their factual correctness. This is likely because the first sentence often provides a general image overview, a style VLMs are heavily trained on.

Furthermore, VLMs demonstrate a clear **self-preference**, performing measurably worse when asked to detect errors in captions they generated themselves. For example, the Llama-4 model showed degraded detection performance when reviewing its own hallucinated captions compared to those generated by other models.

Finally, the study pinpointed specific types of hallucinations where VLMs universally fail. Errors related to **Attributes** (like color or texture) and **Direction** (such as which way an object is facing) remain the most frequent detection failures, highlighting that advanced models still struggle with basic compositional understanding and locating objects in space.

While proprietary models like GPT-5 showed the highest average performance, the leading open-source model, Llama-4, proved surprisingly robust, often matching or exceeding the performance of smaller proprietary systems.

The creation of AlignBench provides a far more rigorous metric for developers, ensuring that future VLMs can not only generate detailed descriptions but also rigorously verify their own factual grounding, moving the field closer to truly reliable multimodal AI.