---
layout: paper
pdf_url: https://arxiv.org/pdf/2408.02373
permalink: 2024-08-08/2408.02373/
title: AI Assistants Learn Privacy Boundaries Using Contextual Integrity
---



Advanced AI assistants, capable of autonomously executing complex tasks like booking appointments or filling out job applications, pose a fundamental challenge to user privacy. While Large Language Models (LLMs) gain immense utility from accessing sensitive user information (emails, financial data), they often lack the contextual awareness to prevent oversharing this data with third parties.

Researchers from Google DeepMind and Google Research have proposed a framework to address this, operationalizing the long-standing privacy concept of Contextual Integrity (CI) to steer AI behavior. CI defines privacy not as a strict separation of public and private data, but as the *appropriate flow* of information within a specific social context.

For instance, sharing your detailed medical history with a doctor is appropriate and expected within the *healthcare context*. However, sharing that exact same information when applying for a software engineering job is entirely inappropriate. The task and the recipient fundamentally change the privacy norm.

To enforce these context-dependent norms, the researchers designed and evaluated several assistant architectures centered on structured reasoning. The most successful approach utilizes a specialized CI-based reasoning supervisor. This supervisor doesn't just guess whether to share a piece of data; it first constructs a formal *Information Flow Card (IFC)*.

The IFC forces the LLM to identify all relevant features of the information flow: the data **Sender** (the user), the **Receiver** (the third party, e.g., a credit card company), the **Context** (e.g., a financial transaction), and the **Goal** (e.g., assessing creditworthiness). Only once this context is established can the assistant reason about whether sharing a specific piece of data (the **Information Type**) is compliant with societal norms.

The team benchmarked these assistants using a novel form-filling dataset composed of synthetic user personas, synthetic forms mimicking real-world scenarios (like medical check-ups or job applications), and crucial human annotations defining which fields were *necessary* versus merely *relevant* to complete the task.

In one example from the benchmark, an assistant is tasked with filling out a credit card application form that includes a field for "Preferred gender." The CI-based reasoning supervisor builds an IFC, concluding that the Information Type (gender) is not relevant to the Context (credit card application) or the Goal (assessing creditworthiness). The supervisor thus refuses to fill the field, preventing unnecessary privacy leakage.

Across multiple LLMs (including Gemini Ultra), the CI-based reasoning assistant architecture consistently demonstrated the best balance of performance. It achieved high utility (successfully completing tasks that require sharing necessary data) while significantly reducing privacy leakage (oversharing information that was not contextually required), outperforming simpler binary supervision or naive self-censoring methods.

The research marks a crucial step toward building robust, human-centered AI agents whose autonomous actions align with deeply ingrained societal privacy expectations, moving beyond absolute notions of privacy to create AI assistants that are truly contextually aware.