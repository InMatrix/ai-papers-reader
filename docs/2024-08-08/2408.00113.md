---
layout: paper
pdf_url: https://arxiv.org/pdf/2408.00113
permalink: 2024-08-08/2408.00113/
title: Chess and Othello Games Provide Ground Truth for AI Interpretability
---



Evaluating how internal features are represented inside large language models (LLMs) has long been a challenge for AI researchers, but a new study leverages the formalized structure of board games—Chess and Othello—to introduce objective, quantifiable metrics for measuring progress in AI interpretability.

Recent efforts to reverse-engineer neural networks rely on Sparse Autoencoders (SAEs), a method that disentangles complex internal computations into human-readable features. However, assessing the quality of these SAEs in general-purpose LLMs is nearly impossible because researchers lack a "gold-standard" dictionary of features the model should have learned.

Researchers addressed this gap by training LMs specifically on game transcripts of Chess and Othello. Unlike natural language, these domains contain a clear, formal set of "Board State Properties" (BSPs) that the model must internally track to predict the next legal move.

For instance, an interpretable feature in a Chess LM could be the binary property: "There is a knight on F3," or the strategic concept: "The bishop on F5 is pinned." Because these game features are objectively definable, they serve as the crucial ground truth needed to measure the effectiveness of SAEs objectively.

The team introduced two novel metrics to evaluate SAE quality based on these ground-truth BSPs. The first, **Coverage**, measures how many predefined interpretable features an SAE successfully captures. The study found specific SAE features acting as high-precision detectors for strategic concepts like a "rook threat" or an "en passant available" move.

The second metric, **Board Reconstruction**, assesses the extent to which the full state of the game board can be recovered simply by analyzing the active features identified by the SAE. This tests whether the SAE features provide an exhaustive, holistic representation of the model’s internal knowledge. While SAEs performed well on reconstruction, they did not perfectly match the accuracy of traditional linear probes, suggesting there is still information in the model’s representations that SAEs fail to capture fully.

To guide improvement in dictionary learning, the researchers also introduced a novel training technique called **p-annealing**. Standard SAEs use an L1 penalty to encourage sparsity, but this method can cause features to "shrink" or suppress their activations. $p$-annealing incrementally shifts the sparsity penalty from $L_1$ (convex and easy to optimize) toward $L_p$ (a non-convex, more direct approximation of true sparsity).

The results show that standard SAEs trained with $p$-annealing consistently outperform those trained with traditional $L_1$ penalties. Notably, $p$-annealing achieved comparable performance to Gated SAEs—a prior state-of-the-art architecture—but without the significant increase in computational expense (Gated SAEs require about 50% more compute per forward pass).

Crucially, the new objective metrics revealed improvements in SAEs with larger hidden dimensions that were entirely missed by traditional, unsupervised proxy metrics, validating the utility of formalized testing environments like the board game setting for quantitative progress in AI interpretability.