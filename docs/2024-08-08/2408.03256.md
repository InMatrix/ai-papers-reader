---
layout: paper
pdf_url: https://arxiv.org/pdf/2408.03256
permalink: 2024-08-08/2408.03256/
title: Open-Source AI Closes Text-to-SQL Gap with Novel Synthetic Data Strategy
---



A new specialized large language model (LLM) called SENSE has achieved state-of-the-art results in converting natural language questions into database queries (Text-to-SQL), effectively eliminating the performance disparity between open-source models and powerful, proprietary systems like GPT-4.

Developed by researchers from the Chinese Academy of Sciences and Alibaba Group, SENSE employs an innovative synthetic data approach that combines high-quality outputs from "strong" LLMs with deliberately flawed examples generated by "weak" LLMs. The resulting models, SENSE-7B and SENSE-13B, set new benchmarks on the industry-standard Spider and BIRD datasets, offering a cost-effective and open alternative for complex data interactions.

For years, the ability of LLMs to generate accurate SQL queries from human language has been hampered by data scarcity and the inability of open-source models to match the generalization capabilities of their closed-source rivals. Standard open-source models, like CodeLLaMA-13B-Instruct, lagged up to 30% behind GPT-4 on challenging benchmarks.

The SENSE strategy mitigates this by utilizing a two-pronged data synthesis process:

**1. Strong Data for Diversity and Depth**

To ensure the model can handle diverse, novel databases, SENSE first relies on powerful, closed-source models (such as GPT-4) to generate "strong data." This synthetic data is designed to enhance cross-domain generalization by focusing on complexity and diversity.

For instance, instead of simple `SELECT` statements, the strong data includes queries requiring multiple `JOIN` operations across several tables or utilizing advanced database features, all guided by prompts designed to control the difficulty level. This process rapidly expands the training domain beyond what expensive, manually annotated datasets typically provide.

**2. Weak Data for Learning from Errors**

The second, more nuanced phase involves teaching the LLM to identify and correct its own mistakes, a skill critical for real-world reliability. Researchers used smaller, less well-aligned open-source models (like DeepSeek-Coder) to intentionally generate "weak data"â€”queries that are syntactically plausible but logically incorrect.

These incorrect SQL samples are then fed to an SQL executor, which determines if the result matches the true database answer. If a weak model outputs a query that executes to an incorrect result, that mistake is labeled as a "negative sample." SENSE is then fine-tuned using Direct Preference Optimization (DPO), a form of preference learning, which encourages the model to choose the correct query over the incorrect one.

This is analogous to a human programmer learning that while `SELECT MAX(Population)` might return *a* population, if the question asks for countries with *smaller* populations than any country in Asia, the correct operator should be `MIN`. By learning from the executor's feedback on these weak, erroneous samples, SENSE refines its SQL syntax and logic, reducing hallucinations and alignment errors.

When evaluated, the 13-billion parameter version, SENSE-13B, surpassed the previous best methods on the Spider benchmark, demonstrating superior robustness and setting a new high-water mark on the challenging BIRD benchmark, which focuses on massive real-world databases and external knowledge reasoning.

The public release of SENSE data and models marks a significant step toward democratizing high-performance Text-to-SQL capabilities, demonstrating that specialized fine-tuning on synthetic data can enable open-source LLMs to match, and even exceed, the capabilities of proprietary technology.