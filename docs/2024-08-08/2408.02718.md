---
layout: paper
pdf_url: https://arxiv.org/pdf/2408.02718
permalink: 2024-08-08/2408.02718/
title: New Benchmark Reveals Major Weaknesses in Top AI Models, Even GPT-4o Stumbles
  on Multi-Image Reasoning
---



A new comprehensive evaluation suite for Large Vision-Language Models (LVLMs) has exposed significant limitations in the current generation of AI, particularly when required to synthesize and reason across multiple images simultaneously.

Researchers from OpenGVLab, Shanghai AI Laboratory, and collaborating universities introduced the Multimodal Multi-image Understanding (MMIU) benchmark, designed to evaluate how deeply LVLMs can process complex visual scenes captured from different perspectives or across time. The results indicate a substantial gap between proprietary models like OpenAI’s GPT-4o and the requirements of real-world multi-image cognition.

Despite its sophisticated multimodal abilities, GPT-4o achieved only 55.7% accuracy on MMIU, confirming the benchmark’s inherent difficulty and underscoring a key bottleneck in the path toward truly generalized visual AI.

### A Challenge Built on Cognition

Previous LVLM evaluations typically focused on single-image perception. MMIU, inspired by human memory mechanisms (semantic, temporal, and spatial), vastly expands this scope. The benchmark comprises 77,000 images, 52 diverse tasks, and 11,698 meticulously curated multiple-choice questions, organized into seven types of relationships, making it the largest multi-image evaluation suite to date.

The evaluation revealed that while models perform relatively well on semantic tasks—such as comparing the low-level visual quality (e.g., lighting or blurriness) between two images—they fail dramatically in tasks requiring spatial and temporal reasoning.

For instance, in **spatial tasks** like Jigsaw Puzzle Solving, models must correctly order shuffled patches of an image (e.g., identifying the sequence for top-left, top-right, bottom-left, bottom-right). Current LVLMs often confuse these 2D spatial relationships.

The failure is even more pronounced in **3D spatial tasks**, such as 3D Object Detection or Multi-View Reasoning, where the model must deduce an object’s true position or the relative camera pose from several different viewpoints of the same scene.

Similarly, models struggle acutely with **temporal ordering**. When presented with a shuffled sequence of images showing an action (like a tennis player serving), models like GPT-4o achieved accuracy as low as 28% in identifying the correct chronological sequence, indicating poor long-context memory and reasoning ability.

### Spatial Blind Spots and the Path Forward

The researchers’ analysis, broken down by task difficulty, suggests that multi-image comprehension lags far behind single-image understanding, which remains the foundational strength of top-performing LVLMs (like InternVL2, the strongest open-source model).

Tasks involving spatial understanding and sequential ordering are currently "out-of-domain" for most LVLMs. Simply pre-training models on vast amounts of interleaved image-text data is insufficient. The study concludes that achieving better performance requires dedicated multi-image supervised fine-tuning (SFT) and the incorporation of specific training techniques designed to enhance strong memory and spatial reasoning capabilities.

The MMIU benchmark is now open-sourced, providing the research community a critical tool to diagnose these weaknesses and push the frontier of LVLM development toward achieving sophisticated multi-image user interactions.