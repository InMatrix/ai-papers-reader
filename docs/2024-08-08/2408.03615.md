---
layout: paper
pdf_url: https://arxiv.org/pdf/2408.03615
permalink: 2024-08-08/2408.03615/
title: Optimus-1 AI Agent Excels at Complex Tasks Using Human-Inspired Hybrid Memory
---



A new general-purpose agent named Optimus-1 is demonstrating near human-level capability in completing extremely complex, long-horizon tasks in open-world environments, a longstanding challenge in artificial intelligence. Developed by researchers at the Harbin Institute of Technology and Peng Cheng Laboratory, the agent’s success stems from a novel architecture centered on a "Hybrid Multimodal Memory" system designed to mimic how humans utilize structured knowledge and accumulated experience.

Existing AI agents often struggle in open-world games like Minecraft because they lack robust, long-term memory to handle multi-step, multi-hour missions. Optimus-1 addresses this through two interconnected memory components.

First is the **Hierarchical Directed Knowledge Graph (HDKG)**, which acts as the agent’s semantic memory. The HDKG explicitly stores world knowledge and the logical relationships between objects—essential for effective planning. For instance, if the agent is tasked with crafting a stone sword, the HDKG immediately provides the knowledge map: a stone sword requires cobblestone and sticks, which in turn necessitates mining materials with wood-based tools first. This train-free knowledge allows the agent’s Knowledge-Guided Planner to generate a complete, efficient sequence of sub-goals in a single step, rather than iteratively figuring out each move.

The second component is the **Abstracted Multimodal Experience Pool (AMEP)**, serving as the agent’s episodic memory. AMEP dynamically summarizes and stores multimodal historical information—including visual frames, agent states, and task plans—from both successful and failed past attempts. This is crucial for adaptive behavior and reflection.

Optimus-1’s architecture includes an **Experience-Driven Reflector** that periodically checks the agent’s current situation against the AMEP. If the agent encounters a problem—such as falling into a cave or failing a crucial crafting step—the Reflector retrieves relevant past cases (including failures) to understand the predicament and initiate a "replan" command to the Planner.

This memory and reflection loop drastically improves survival and efficiency. For example, when faced with the task "Mine 1 diamond," Optimus-1 first generates a multi-step plan involving acquiring the necessary iron pickaxe (which requires stone tools, smelting, etc.). If the agent spawns in a dark, dangerous cave, the Reflector analyzes the current visual input, determines the situation is dangerous, and forces a dynamic adjustment: the first sub-goal changes to "Leave the cave" before proceeding with the original plan. In comparison, rival agents often waste critical time iteratively planning while a hazard, like a zombie, approaches.

Tested on a benchmark of 67 long-horizon Minecraft tasks, Optimus-1 showed dramatic performance gains. It achieved success rates near 100% on early-game tasks (like "Wood" group) and showed improvement of 29% and 53% on challenging groups like "Diamond" and "Redstone" over state-of-the-art competitors like Jarvis-1. Crucially, the Hybrid Multimodal Memory is "plug-and-play," enabling various Multimodal Large Language Models (MLLMs) to achieve 2 to 6 times performance improvement, demonstrating a major step toward building robust, generalized agents capable of self-evolution.