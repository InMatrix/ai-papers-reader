---
layout: paper
pdf_url: https://arxiv.org/pdf/2505.08638
permalink: 2025-05-16/2505.08638/
title: New Dataset Exposes Weaknesses in AI Agent Debugging
---



**AI agents, powered by Large Language Models (LLMs), are increasingly automating complex tasks, but ensuring their reliability remains a significant challenge. A new dataset called TRAIL (Trace Reasoning and Agentic Issue Localization) has been created to address this challenge, revealing that even the most advanced LLMs struggle with debugging agentic workflows.**

TRAIL, presented in a new research paper, focuses on evaluating the ability of LLMs to analyze and identify errors within the detailed "traces" of agentic systems. These traces, generated as agents interact with various tools and environments, provide a step-by-step record of their actions and decisions. Analyzing these traces is crucial for pinpointing the root causes of errors and improving agent performance.

Unlike traditional software debugging, debugging AI agents is complicated by the interplay of LLM reasoning, external tool outputs, and the non-deterministic nature of agent behavior. For example, an agent tasked with retrieving information about a specific scientific topic might fail due to:

*   **Poor Information Retrieval:** The agent formulates an ineffective search query.
*   **Tool Output Misinterpretation:** The agent misinterprets the information returned by the search engine.
*   **Decision Making Errors:** The agent selects the wrong tool for a given subtask.
*   **Hallucinations:** The agent generates factually incorrect information, even when presented with correct data.

The TRAIL dataset addresses these complexities by providing 148 human-annotated traces, carefully curated from real-world applications like software engineering and open-world information retrieval. These traces are annotated with a formal taxonomy of agentic errors, categorized into reasoning, planning, and execution failures.

Researchers tested state-of-the-art LLMs on TRAIL, including GEMINI-2.5-PRO, and found that their performance was surprisingly poor. The best performing model achieved a mere 11% accuracy in correctly identifying both the category and location of errors within the traces. This highlights a significant gap in the ability of current LLMs to effectively debug agentic workflows.

The paper also found that solving TRAIL requires LLMs to process lengthy input sequences, often exceeding their maximum context length. For instance, some traces required LLMs to process more than 7 million tokens, while still not managing to debug errors. 
Additionally, the study found that the effectiveness of reasoning effort also affects a model's performance.  Models performing better in this area benefited from both the presence and greater extent of reasoning chains.

TRAIL is publicly available to support and accelerate future research in scalable evaluation for agentic workflows. The hope is that this dataset will serve as a valuable benchmark for developing new techniques to improve the reliability and robustness of AI agents. By providing a standardized and comprehensive evaluation framework, TRAIL paves the way for more trustworthy and effective AI systems in the future.