---
layout: paper
pdf_url: https://arxiv.org/pdf/2509.19185
permalink: 2025-10-03/2509.19185/
title: AI Agent Testing&#58; A Pragmatic but Incomplete Adaptation
---



A recent study by researchers at Queen's University reveals that while developers of AI agent frameworks and applications are adapting traditional software testing practices, there are significant gaps and a surprising lack of focus on crucial components, particularly those related to how agents are prompted.

The study, published in "Empirical Software Engineering," analyzed 39 open-source AI agent frameworks and 439 agentic applications to understand current testing practices. The findings indicate that developers are largely reusing established testing patterns like assertion-based testing and parameterized testing, demonstrating a pragmatic approach to handling the inherent non-determinism of foundation models (FMs) that power these agents.

**Key Findings:**

*   **Reliance on Traditional Patterns:** The vast majority (80%) of testing patterns observed are inherited from classical software engineering or machine learning practices. This suggests that established techniques are robust and adaptable to the new domain. For instance, **parameterized testing**, which allows running the same test with multiple inputs, is used much more frequently (28.7% in frameworks, 26.1% in applications) than in traditional software (9%). This is crucial for testing agent systems that deal with dynamic data and probabilistic outputs.

*   **Emerging Patterns Lagging:** Novel, agent-specific testing patterns like **DeepEval** (for evaluating FM outputs) and **Hyperparameter Control** (for managing FM randomness) are adopted by a tiny fraction of practitioners (around 1%). This low adoption might be due to a lack of awareness, complexity, or insufficient integration with existing tools.

*   **Inverted Testing Effort:** In a significant departure from traditional machine learning testing, where model testing is paramount, developers are heavily focused on testing **deterministic infrastructure components**. Resource Artifacts (like tools and APIs) and Coordination Artifacts (workflows) together account for over 70% of testing effort.

*   **Critical Blind Spot: Prompts:** The most concerning finding is the severe under-testing of the **Trigger component**, which primarily refers to the prompts used to interact with FMs. This component appears in only about 1% of all test functions. This neglect poses a significant risk, as FMs are frequently updated, potentially leading to "prompt decay" and silent failures if prompts are not rigorously validated. For example, an agent designed to generate a story might start producing nonsensical narratives after an FM update, a problem that prompt regression testing could help detect.

*   **Divergent Philosophies:** While frameworks and applications test similar architectural components, their testing philosophies differ. Frameworks tend to focus on universal robustness with rigorous checks, while applications prioritize context-specific correctness with more adaptive and relaxed patterns.

**Implications and Recommendations:**

The study's authors recommend a multi-pronged approach to improve the reliability of AI agents:

*   **Framework Developers:** Should integrate advanced semantic verification capabilities, such as **DeepEval**, into their infrastructure to better handle the complex and sometimes unpredictable outputs of FMs. They should also establish clear "testing contracts" that define responsibilities between frameworks and applications.

*   **Application Developers:** Must implement systematic **prompt regression suites** to safeguard against FM updates and model evolution. They should also adopt **hyperparameter control** as a debugging technique to isolate FM-induced non-determinism.

*   **Researchers:** Need to investigate the barriers preventing the adoption of novel testing patterns and formalize a comprehensive testing methodology for agentic systems, accounting for their unique characteristics.

In essence, the research highlights that building robust AI agents requires not abandoning established software engineering principles, but rather augmenting them with specialized techniques and a more strategic allocation of testing effort, particularly towards the critical prompt interface.