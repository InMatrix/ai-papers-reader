---
layout: paper
pdf_url: https://arxiv.org/pdf/2512.21332
permalink: 2025-12-26/2512.21332/
title: Adaptive Cross-Attention System Propels New Code AI to Top of Retrieval Leaderboard
---



A collaboration between researchers at Ant Group and Shanghai Jiao Tong University has unveiled C2LLM (Contrastive Code Large Language Models), a new family of AI embedding models that have redefined the state-of-the-art in code retrieval by fundamentally rethinking how sequence information is summarized.

Code retrieval—the ability for AI agents or developer search engines to find the most relevant code snippet from massive databases based on a natural language query (e.g., "Write a Python function to parse JSONL")—is a bottleneck task crucial for modern software development and emerging autonomous code agents. The challenge lies in converting lengthy, complex code sequences into a single, concise vector (an embedding) that captures all its semantic and syntactic meaning.

The C2LLM models, built upon the powerful Qwen-2.5-Coder backbone, tackle the inherent limitations of standard LLM pooling methods using an innovative architecture called Pooling by Multihead Attention (PMA).

### Breaking the Information Bottleneck

Current state-of-the-art embedding models typically employ one of two strategies: *mean pooling* (averaging all token representations) or using the *End-of-Sequence (EOS) token* as the representation vector.

Researchers argue these methods are ill-suited for the code domain. Mean pooling often requires deviating from the efficient, causal architecture of modern LLMs, while the EOS token creates a severe information bottleneck. Imagine attempting to represent a 1,000-line function—complete with imports, comments, and complex logic—using the vector from only the very last token.

C2LLM’s PMA module solves this by inserting a lightweight cross-attention layer. This layer utilizes a single, "learnable query" vector to attend to the entire sequence of token embeddings produced by the LLM.

Conceptually, the PMA acts like a highly specialized reviewer. Instead of just summarizing all lines equally or collapsing all meaning into the final punctuation mark, the PMA uses its learnable query to ask the code sequence, "What are the most functionally critical elements here?" This allows the system to selectively aggregate salient information—such as core function signatures, key loop logic, or variable declarations—while filtering out boilerplate or less relevant tokens.

### State-of-the-Art Performance in Retrieval

This selective aggregation preserves the causal structure of the base LLM while simultaneously generating a superior, bottleneck-free sequence vector.

The results, measured on the demanding MTEB-Code benchmark, validate this approach across different scales. The flagship **C2LLM-7B** model achieved an average performance score of 80.75, ranking 1st overall among all tested open- and closed-source models. Its performance was particularly strong in complex reasoning tasks, suggesting its ability to effectively capture the high-level intent behind natural language queries directed at code.

Furthermore, the smaller, more efficient **C2LLM-0.5B** variant demonstrated remarkable efficiency, scoring 75.46 and claiming the top spot among models with fewer than one billion parameters, surpassing significantly larger competitors.

The adoption of PMA also offers practical benefits for deployment, allowing the final embedding vector dimension to be decoupled from the LLM’s internal hidden dimension. This flexibility makes C2LLM capable of producing compact embeddings suitable for large-scale, real-world vector databases without requiring costly auxiliary training steps.

Trained on three million publicly available code and natural language pairs, the C2LLM family sets a new standard for efficient and accurate code representation, pushing the frontier for AI tools in software engineering.