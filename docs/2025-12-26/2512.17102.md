---
layout: paper
pdf_url: https://arxiv.org/pdf/2512.17102
permalink: 2025-12-26/2512.17102/
title: AI Agents Learn to Evolve&#58; New RL Framework SAGE Boosts Performance and
  Efficiency with Reusable Skill Libraries
---



In a significant advance for autonomous AI systems, researchers have developed a novel reinforcement learning (RL) framework that enables Large Language Model (LLM) agents to systematically generate, save, and reuse complex operational skills.

Dubbed Skill Augmented GRPO for self-Evolution (SAGE), the framework tackles a core challenge facing LLM-based agents: their difficulty in continuously self-improving and adapting when deployed in dynamic, real-world environments. While current agents excel at complex reasoning, they often rely on repetitive, long action sequences, making them inefficient and inconsistent.

The team, including researchers from the University of Wisconsin-Madison and AWS Agentic AI, demonstrated that SAGE achieves state-of-the-art results on the challenging AppWorld benchmark—a simulated environment where agents solve everyday digital tasks through code and API calls. SAGE improved Scenario Goal Completion (SGC) by 8.9% compared to traditional RL baselines, while simultaneously achieving massive efficiency gains, requiring 26% fewer interaction steps and generating 59% fewer tokens.

### The Power of Skill Chains and Integrated Rewards

SAGE’s effectiveness stems from two primary mechanisms designed to integrate skill creation into the learning process.

First is **Sequential Rollout**, which trains agents not on isolated tasks, but on a continuous chain of similar tasks within a single scenario.

For instance, consider a scenario requiring an agent to manage a digital music library across three related steps. The agent performing the first task—say, “Log into Spotify and save the login logic”—generates a reusable function, such as `def spotify_login(email: str)`. This function is immediately added to the agent’s skill library.

When the agent moves to the second task, “Find the number of songs in all saved playlists,” it can simply call the newly acquired `spotify_login` function, rather than performing a sequence of primitive actions (like looking up documentation, extracting a password, and calling the API) all over again.

This skill reuse is formalized by the second mechanism: **Skill-integrated Reward**. Beyond the standard binary reward for task success, SAGE introduces an extra bonus reward that specifically reinforces two critical behaviors: the successful generation of a high-quality skill in an early task, and the successful utilization of that skill in a subsequent task within the chain. This incentive structure ensures that the agent learns to create modular, reliable code that transfers effectively.

### From Primitive Steps to Complex Operations

The results highlight that treating LLM agents as self-improving entities capable of abstracting complex action sequences into single, reusable skills leads to much more robust performance.

In environments like AppWorld, where agents navigate applications like Spotify and Venmo by writing code and executing API calls, SAGE transforms long, iterative processes into compact, efficient operations. Instead of prompting the LLM for step-by-step reasoning and multiple API interactions for common sub-tasks, the agent leverages the skill library to condense these actions, drastically reducing context length and computational cost.

By applying RL to enhance both skill generation quality and accurate skill usage, SAGE provides a pathway for open-source LLMs—when pre-trained using expert experiences—to surpass the performance of even powerful prompting-based agents, ushering in a new era for genuinely autonomous AI assistants.