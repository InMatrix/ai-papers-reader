---
layout: paper
pdf_url: https://arxiv.org/pdf/2512.19673
permalink: 2025-12-26/2512.19673/
title: Breakthrough RL Method Optimizes LLMs by Tuning Their "Internal Policies"
---



Large Language Models (LLMs) are typically treated as black boxes when optimized using Reinforcement Learning (RL). However, new research reveals that these models secretly house distinct "Internal Policies" within their layer structure, and directly optimizing these policies can dramatically enhance complex reasoning capabilities.

The paper, titled "Bottom-up Policy Optimization: Your Language Model Policy Secretly Contains Internal Policies," introduces a novel RL paradigm called BuPO (Bottom-up Policy Optimization) that exploits the internal logic of the Transformer architecture.

Researchers decomposed the monolithic LLM output policy into **Internal Layer Policies** (representing the cumulative reasoning up to a specific layer) and **Internal Modular Policies** (attributing contribution to the self-attention and Feed-Forward Network, or FFN, components). By analyzing the policy entropy—a measure of uncertainty or exploration—across these internal stages, the team mapped the LLM’s reasoning process.

### The Stages of LLM Reasoning

The analysis confirmed a universal trend: early layers maintain high entropy, focused on broad exploration of the answer space, while top layers converge to near-zero entropy, signaling final prediction refinement.

Crucially, the study uncovered significant architectural differences in *how* models handle this convergence. For example, Llama-series models exhibit an abrupt convergence, where the prediction space collapses suddenly only in the final few layers.

In contrast, Qwen-series models, particularly Qwen3, display a progressive, “human-like” cognitive process dubbed **Exploration-Integration-Convergence (EIC)**, primarily driven by their FFNs:

1.  **Exploration (Lower Layers):** Entropy increases (expansion of possibilities).
2.  **Integration (Middle Layers):** Entropy stabilizes near zero change, indicating the model is effectively integrating or retrieving parametric knowledge.
3.  **Convergence (Upper Layers):** Entropy decreases sharply as the model locks onto the final answer.

This finding suggests that Qwen models structure their reasoning in predictable, hierarchical stages, allocating distinct functions to different layer depths.

### The BuPO Optimization Strategy

Inspired by the progressive emergence of reasoning from bottom to top, the BuPO paradigm breaks from traditional RL, which optimizes the final output policy holistically.

Instead, BuPO sequentially optimizes a specific lower-layer Internal Layer Policy during the early training phase. This "bottom-up" alignment forces the foundational layers to preemptively encode high-level reasoning signals, establishing a robust cognitive structure before the overall policy (the final output) is fine-tuned.

The effect of this targeted, early training is significant. Experiments on complex reasoning benchmarks—including MATH500, AMC23, AIME24, and AIME25—showed that BuPO consistently yields superior average performance compared to conventional methods like GRPO and PPO. For instance, the Qwen3-4B model achieved a gain of 4.69 points on AIME24 over the GRPO baseline.

By moving beyond the black-box view and applying targeted supervision to a model’s internal decision-making process, BuPO offers a promising new direction for enhancing the reliability and reasoning depth of next-generation LLMs.