---
layout: paper
pdf_url: https://arxiv.org/pdf/2512.16144
permalink: 2025-12-26/2512.16144/
title: INTELLECT-3&#58; New 106B Open Model Outperforms Larger Competitors Using Industrial-Scale
  Reinforcement Learning Infrastructure
---



A new large language model, INTELLECT-3, has debuted with state-of-the-art performance in its weight class across critical reasoning and coding benchmarks, fueled by a powerful, newly open-sourced reinforcement learning (RL) infrastructure stack.

Developed by the Prime Intellect team, INTELLECT-3 is a 106-billion parameter Mixture-of-Experts (MoE) model (with 12 billion active parameters). Crucially, the model's success stems from a comprehensive post-training regimen using RL with verifiable rewards (RLVR), and the team has released the full end-to-end stack—including the model, training framework, and environments—to the public.

INTELLECT-3’s evaluation scores are impressive, often matching or surpassing much larger models. On the highly challenging AIME 2024 high-school mathematics competition, INTELLECT-3 achieved 90.8%, significantly outperforming the post-trained GLM-4.5-Air (84.6%) and the three-times larger GLM-4.5 model (85.8%).

The gains were particularly striking in domains requiring tool use and agentic behavior, such as code generation. On the LiveCodeBench v6, a single-turn coding evaluation, INTELLECT-3 scored 69.3%, an 8% gain over its GLM-4.5-Air base model counterpart, demonstrating enhanced long-context reasoning capabilities.

### The RL Engine Under the Hood

The high performance is attributed to the custom-built infrastructure designed to handle the massive scale and unique demands of agentic RL training.

At the core is **prime-rl**, an asynchronous RL framework engineered for maximum efficiency. Traditional synchronous RL systems often struggle when generating the long rollouts typical of complex reasoning tasks, leading to bottlenecks as the trainer waits for the inference server. `prime-rl` solves this by running the trainer and inference service on separate, disaggregated GPU sets, enabling continuous batching and "in-flight weight updates." This means as soon as a new, improved policy becomes available, the inference servers instantly begin using it, sustaining peak throughput for trajectories spanning up to 65,536 tokens.

To standardize the RL training process, the team introduced **Verifiers** and the Environments Hub. These libraries define reusable, versioned RL tasks—ranging from math and science problems to "Deep Research" (using web search tools) and "Software Engineering" (fixing issues in a repository).

Handling these agentic environments requires secure, high-speed code execution. For this, Prime Intellect developed **Prime Sandboxes**, a container orchestration system that allows the untrusted code generated by the model (like Python solutions or Bash commands) to be executed securely with millisecond-level latency. This throughput is vital for running thousands of concurrent rollouts needed for efficient training on coding tasks.

The training involved a massive compute deployment, utilizing 512 NVIDIA H200 GPUs across 64 interconnected nodes over two months. The team notes that, even at the end of the run, the benchmark scores were still climbing without signs of plateauing, suggesting significant potential for future iterations.

By open-sourcing INTELLECT-3 and its entire training stack, the Prime Intellect team aims to provide a reliable, industrial-grade foundation for the next wave of open research in reasoning and agentic language models.