---
layout: paper
pdf_url: https://arxiv.org/pdf/2512.18470
permalink: 2025-12-26/2512.18470/
title: New Benchmark Reveals Major Flaw in AI Coders&#58; GPT-5 Resolves Only 21%
  of Real-World Software Evolution Tasks
---



AI coding agents, heralded for their ability to fix bugs and implement minor features, are struggling significantly when faced with the full complexity of real-world software engineering, according to a new analysis published today.

Researchers have introduced a rigorous new benchmark, **SWE-EVO**, designed to test *long-horizon software evolution*—the sustained, multi-step process of updating and maintaining large codebases across multiple versions. Initial evaluations show a striking capability gap: the best performing agents, running on proprietary large language models like GPT-5, achieved a meager 21% success rate on SWE-EVO, a drastic drop from the 65% they achieve on simpler, single-issue resolution benchmarks like SWE-Bench Verified.

The consensus among existing coding benchmarks has been to focus on isolated tasks, such as resolving a single, well-defined bug from a GitHub issue. However, real software development, responsible for up to 80% of all engineering effort, involves interpreting high-level requirements, coordinating massive changes across numerous interdependent files, and maintaining existing functionality—a process often spanning multiple developer iterations.

SWE-EVO addresses this gap by creating 48 complex tasks sourced from the actual release notes and commit histories of seven mature open-source Python projects, including popular repositories like `dask` and `scikit-learn`.

### From Single Fixes to System Overhaul

The complexity jump is dramatic. A typical SWE-EVO task requires multi-step modifications spanning an average of 21 files and must pass a large test suite averaging 874 tests to ensure no regressions are introduced.

To understand the difference, consider a simple task on an old benchmark: fixing a typo in a function name. In contrast, an SWE-EVO task might be derived from a release note demanding the system evolve an existing email-based account registration feature to integrate Google or GitHub authentication.

This requires the agent to demonstrate comprehensive program understanding: mapping out new authentication APIs, updating user models and session management, refactoring relevant components across multiple files, ensuring security compliance, and verifying changes through iterative testing. It’s a systemic overhaul, not a simple patch.

The analysis revealed that agent failures on SWE-EVO are not typically due to low-level issues like syntax errors or broken tool usage. Instead, stronger models like GPT-5 primarily fail due to "Instruction Following"—misinterpreting or incompletely addressing the long, nuanced release notes that define the high-level evolution requirements.

### Measuring Partial Progress

Given the sheer scale of the required changes, the researchers also introduced a refined metric called **Fix Rate**. Unlike the strict binary pass/fail *Resolved Rate*, Fix Rate measures partial progress by calculating the percentage of newly fixed tests, provided the patch doesn't break any existing tests (a crucial regression check). This soft score offers finer granularity, showing that while an agent may not fully resolve a task, it might successfully address a significant portion of the changes.

In one example, two different models showed the same Resolved Rate of 2.08% (only solving one out of 48 instances), yet their Fix Rates were 4.65% and 2.08% respectively. This subtle difference confirms that one model consistently achieved more partial fixes per instance than the other, information lost by the binary score.

The results strongly suggest that the path toward truly autonomous software engineering agents requires significant advancements in planning, long-context reasoning, and multi-file orchestration, moving far beyond capabilities assessed by current, simpler benchmarks. SWE-EVO is positioned to become the new standard for evaluating agents on these critical evolution-style challenges.