---
layout: paper
pdf_url: https://arxiv.org/pdf/2512.20092
permalink: 2025-12-26/2512.20092/
title: AI Finally Masters Conversational Time&#58; New Framework MEMORY-T1 Excels
  at Temporal Reasoning in Long-Term Chat
---



A new reinforcement learning (RL) framework, **MEMORY-T1**, is dramatically improving how large language models (LLMs) reason about time across extensive, multi-session dialogues. Developed by researchers from the Chinese University of Hong Kong, Huawei Technologies, and other institutions, the system tackles the critical problem of temporal inconsistency—where conversational AI agents misinterpret event timelines, leading to factually inaccurate or nonsensical responses in long chats.

Current LLMs often struggle to ground vague temporal expressions like "last night" or "the week before that" when the dialogue history is vast and noisy. For example, to correctly answer, "What time did Emi mention that some 'Suits' characters were together at the Golden Globes?" the agent must not only find the relevant utterance but must correctly map the relative time expression ("last night") to the actual session date (say, 10.01.2024) to infer the absolute date (January 9, 2024).

MEMORY-T1 addresses this via a novel RL approach centered on learning a precise, time-aware memory selection policy.

### Coarse-to-Fine Filtering and Dense Rewards

The framework operates in a coarse-to-fine cascade to efficiently filter millions of tokens in a dialogue history down to the relevant snippets.

First, the **Candidate Generation** phase quickly prunes the search space. An initial Temporal Filtering step predicts the query’s target time range (e.g., “this month”) and discards any dialogue sessions that fall completely outside that window. This is followed by Relevance Filtering, which uses lexical matching to create a small, high-recall candidate pool.

Second, the **Fine-grained Selection** phase uses an RL agent to choose the precise evidence sessions from this pool. This training is guided by a dense, multi-level reward function, which is the core innovation enabling robust temporal reasoning.

Beyond standard Answer Accuracy and Evidence Grounding (ensuring the model cites the correct sessions), MEMORY-T1 introduces the **Temporal Consistency Reward ($R_t$)**. This reward explicitly enforces chronological alignment at two levels:

1.  **Chronological Proximity:** This measures whether the timestamp of a selected dialogue session is chronologically near the target time scope of the query. Since conversational timing is often fuzzy, the reward uses a "soft" logistic function, rewarding sessions that are close but applying a gentle, differentiating penalty to distant sessions.
2.  **Chronological Fidelity:** This is a fine-grained check ensuring that the specific events *within* the selected utterances are temporally aligned with the query. This prevents a common failure mode where a model selects the correct dialogue session but pulls a line of text that discusses an entirely different event from the wrong time.

### Outperforming Larger Models and Long-Context Robustness

The system was validated on the Time-Dialog benchmark, establishing a new state-of-the-art performance of 67.0% overall.

Crucially, MEMORY-T1’s focus on structured retrieval and temporal consistency proved highly efficient: the model, built on an open-source 7B parameter foundation, significantly outperformed a much larger 14B baseline model by over 10.2 percentage points.

Furthermore, MEMORY-T1 demonstrated superior resilience in long-context scenarios. While traditional long-context LLMs suffer performance collapse due to "attentional dilution" as context lengths grow beyond 64k tokens, MEMORY-T1 maintained high and stable accuracy up to 128k tokens. This robust performance confirms that by learning a policy to select temporally precise memories, conversational agents can overcome the challenges posed by vast, noisy dialogue histories, paving the way for more reliable and factually consistent long-term interactions.