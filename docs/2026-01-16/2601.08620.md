---
layout: paper
pdf_url: https://arxiv.org/pdf/2601.08620
permalink: 2026-01-16/2601.08620/
title: New Benchmark ViDoRe V3 Exposes Critical Weaknesses in AI Document Understanding
---



Artificial intelligence systems designed to answer user questions based on corporate documents—a technology known as Retrieval-Augmented Generation (RAG)—are struggling to handle the complexities of real-world data, according to the creators of a new, comprehensive benchmark.

Researchers from Illuin Technology and NVIDIA introduced ViDoRe V3, a multilingual, multi-modal RAG benchmark designed to move beyond simple text-based, single-document lookups. The benchmark spans 10 diverse professional domains, from Finance and Pharmaceuticals to Industrial Maintenance, comprising 26,000 document pages and over 3,000 human-verified queries, available in six European languages. Crucially, the dataset heavily features visually rich content—such as tables, charts, and infographics—and requires complex reasoning, including multi-hop synthesis across several pages.

A core feature of ViDoRe V3 is the requirement for "visual grounding": systems must not only generate an accurate answer but also provide specific source locations by outputting bounding boxes around the supporting evidence on the document image.

To illustrate the challenge, consider a query like, “Where is the S-3's single point receptacle located?” An effective RAG system must retrieve both textual instruction manuals *and* an accompanying infographic image, synthesize this cross-modal information, and then pinpoint the exact location on the image of the aircraft drawing.

Evaluating state-of-the-art RAG pipelines on this rigorous dataset yielded several key insights and revealed significant limitations in current AI.

In the retrieval phase, visual retrievers that process page images (like ColEmbed-3B-v2) generally outperformed text-only methods. However, the most striking gain came from combining standard textual retrieval with a sophisticated reranker (zerank-2), which boosted performance by an average of 13.2 Normalized Discounted Cumulative Gain at 10 (NDCG@10) points, elevating the textual pipeline to the highest overall retrieval score.

For generating final answers, a hybrid retrieval approach proved essential for difficult questions. Pipelines combining both visual (image) and textual context achieved 54.7% accuracy on complex queries requiring multi-step reasoning, surpassing both the strongest textual (52.1%) and visual (54.5%) baselines. The researchers noted that preserving the visual information of document pages provided better grounding for complex answer generation compared to text-only contexts.

Despite these advances, the study found that the performance of Visual Language Models (VLMs) in visual grounding lags significantly behind human capability. While human annotators achieved an F1 score of 0.602 in bounding box agreement, the best-performing VLM only reached a score of 0.089. This massive performance gap highlights that current models can generally answer a question but cannot reliably identify *where* in the document the evidence is located, undermining trust and verifiability.

ViDoRe V3 is now integrated into the Massive Text Embedding Benchmark (MTEB) leaderboard, setting a new, higher bar for robust document understanding in commercial RAG applications.