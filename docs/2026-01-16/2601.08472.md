---
layout: paper
pdf_url: https://arxiv.org/pdf/2601.08472
permalink: 2026-01-16/2601.08472/
title: New AI Model `sui-1` Delivers Verifiable, Long-Form Summaries to Combat LLM
  Hallucinations
---



A team of researchers has introduced sui-1, a 24-billion-parameter large language model (LLM) specifically engineered to solve one of the greatest barriers to LLM adoption in sensitive fields: factual hallucination. The model generates abstractive summaries for documents spanning up to 2 million tokens, embedding verifiable inline citations that allow users to trace every generated claim directly back to its source text.

In compliance-sensitive domains such as government, law, and corporate due diligence, LLMs are frequently unusable because their summaries, while fluent, cannot be instantly trusted or verified. The sui-1 model addresses this critical limitation by training the model not just to summarize, but to rigorously anchor every statement within the source document.

The results demonstrate that this focused, task-specific training regimen dramatically outperforms models relying on sheer scale alone. In evaluation using an LLM-as-a-judge framework, sui-1 achieved 84.2% overall accuracyâ€”substantially higher than open-weight baselines, including models with three times the parameters, such as the 70-billion-parameter Llama-3.3-70B (42.7%).

### Inline Citations Guarantee Reliability

The core innovation is the use of unique, language-agnostic XML tags as inline citations. Before summarization, the source document is pre-processed, and every sentence is assigned an 8-character hexadecimal identifier based on an MD5 hash (e.g., `<a3f5e823>`).

When sui-1 generates a summary, it includes this tag immediately after the supported claim, effectively creating a machine-verifiable footnote. For instance, instead of generating an unverified statement, the model might output: "The Federal Ministry of Finance announced a 12% increase in infrastructure spending [<b7d2c941>] to be included in the new budget." The tag instantly confirms that the claim originates directly from the sentence associated with `<b7d2c941>` in the source text.

This mechanism ensures what the researchers call "internal grounding," enabling verification without complex external retrieval systems.

### Synthetic Data and Compliance Prowess

To achieve this level of accuracy and adherence, the team developed a novel synthetic data pipeline. A highly capable teacher LLM, guided by extensive chain-of-thought prompting and 16 to 18 specific citation placement rules, generated over 22,000 high-quality training examples across five languages (primarily German, English, French, Italian, and Spanish). Crucially, a multi-stage automated verification process filtered this data to guarantee the accuracy and distribution of citations before the 24B sui-1 model was fine-tuned.

The training proved especially effective in handling complex user requests. The most striking improvement was in format compliance, where sui-1 scored 0.895 compared to baseline scores ranging from 0.137 to 0.411. This capability is vital for real-world deployment, allowing the model to simultaneously adhere to rigid structural rules (like mandatory citation placement) and flexible user instructions (such as "Summarize in exactly 5 bullet points").

The model is also designed to handle massive inputs, processing documents up to 100,000 tokens (over 100 pages) in a single pass, and supporting texts up to 2 million tokens through an iterative chunking and merging process that carefully preserves all citations.

The trained model weights and the underlying dataset are publicly available, demonstrating that targeted training can create specialized LLMs that offer trustworthiness and functional reliability far beyond generalist models.