---
layout: paper
pdf_url: https://arxiv.org/pdf/2601.06786
permalink: 2026-01-16/2601.06786/
title: New Training Method EPICAR Creates LLMs That "Know What They Don't Know"
---



A new training framework called Epistemically-Calibrated Reasoning (EPICAR) has successfully addressed the critical issue of overconfidence in Large Language Models (LLMs), leading to models that are both more accurate and significantly more trustworthy.

While conventional self-training methods like STaR (Self-Taught Reasoner) effectively boost a model's raw reasoning accuracy, they suffer from a major "calibration cost." By exclusively reinforcing successful reasoning steps, these methods cause models to become pathologically overconfident, even when generating logically flawed answers—a phenomenon described as "Model Collapse." For high-stakes applications like medical diagnosis or complex code generation, knowing when an LLM is unsure is as vital as producing a correct answer.

The researchers, from Seoul National University, address this by reframing reasoning training as an *epistemic learning* problem: models must learn not only *how* to reason, but also *when* their reasoning should be trusted.

### The Dual-Objective Solution

EPICAR introduces a dual-objective training loop that incorporates explicit self-evaluation signals. Unlike prior methods that discard incorrect attempts, EPICAR uses them as negative signals. For any generated reasoning path:
1. If the path leads to the correct answer, it reinforces the reasoning and trains the self-evaluation objective with a "yes" label.
2. If the path is incorrect, the reasoning is discarded, but the failure is explicitly used to train the self-evaluation objective with a "no" label.

This exposure to both correct and incorrect logical structures forces the LLM to internalize the features of logical failure, thereby stabilizing its predictive uncertainty.

Experiments on the Llama-3 and Qwen-3 families demonstrated that EPICAR consistently achieved **Pareto-superiority**—simultaneous gains in both accuracy and reliability—across benchmarks like MATH and the out-of-distribution (OOD) reasoning task GSM8K.

For instance, the Llama-3-3B model trained with EPICAR saw its accuracy increase to 8.58% (up from 7.56% in the baseline) while drastically reducing its overconfidence. Its Expected Calibration Error (ECE)—a measure of weighted accuracy-confidence discrepancy—plummeted from 0.376 to 0.108.

### Inference Efficiency Breakthrough

Beyond reliability, EPICAR provides a critical foundation for efficient scaling at inference time. By internalizing high-fidelity confidence signals, the model can leverage Confidence-Informed Self-Consistency (CISC), a weighted ensemble approach that uses the model's self-assessed confidence to prioritize reliable paths.

This strategy led to a substantial reduction in computational overhead. On the MATH-500 benchmark, EPICAR-trained models achieved the accuracy of baselines that used 30 reasoning paths ($K=30$) with only 10 paths ($K=10$). This translates to an effective **3x reduction in inference compute** for comparable performance.

The findings suggest a paradigm shift is necessary in LLM alignment, arguing that uncertainty calibration should be viewed not as an optional safety constraint but as an integral objective for creating robust, compute-optimal reasoning agents.