---
layout: paper
pdf_url: https://arxiv.org/pdf/2601.06789
permalink: 2026-01-16/2601.06789/
title: AI Coding Agents Break Free From 'Closed World' by Learning from 135,000 Human
  Debugging Records
---



A major limitation plaguing autonomous software engineering (SWE) agents—their inability to effectively leverage the immense trove of historical human debugging knowledge—has been addressed by a new framework called MemGovern. Developed by researchers from institutions including UCAS, NUS, and QuantaAlpha, MemGovern transforms raw, messy GitHub repair records into standardized, actionable memories, enabling AI agents to fix real-world bugs with significantly higher accuracy.

While large language models (LLMs) have driven remarkable progress in code generation, current SWE agents often operate in a "closed-world," attempting to fix bugs from scratch using only local context. They struggle to parse the unstructured, noisy, and fragmented data found in GitHub issues and pull requests, which contain crucial human reasoning and repair patterns.

MemGovern solves this by introducing **Experience Governance**, a systematic pipeline designed to clean, standardize, and organize cross-repository human experience into 135,000 "experience cards."

Crucially, each card is structured into two distinct parts:

1.  **The Index Layer:** Contains high-signal retrieval cues like a normalized *Problem Summary* and *Diagnostic Signals* (e.g., exception types or error signatures). This allows the agent to quickly search for similar past problems.
2.  **The Resolution Layer:** Encapsulates the transferable debugging logic, including the *Root Cause* analysis, an abstract *Fix Strategy*, and a concise *Patch Digest*. This decouples the repair logic from concrete, repository-specific code artifacts, making the solution reusable.

To utilize this new memory, MemGovern introduces **Agentic Experience Search**, which mirrors how human engineers browse documentation. Instead of a single, context-overloading retrieval (Standard RAG), agents use a dynamic, two-step "Search-then-Browse" mechanism. The agent first *Searches* the Index Layer for broad relevance, then *Browses* the Resolution Layer of selected candidates for deep, actionable insights.

The practical impact of this structured memory is substantial. On the SWE-bench Verified benchmark, MemGovern improved the bug resolution rate by 4.65% on average across various LLM backbones. For instance, the improvement was particularly notable for models with lower baseline performance, boosting GPT-4o's resolution rate from 23.2% to 32.6% (a 9.4% increase).

The governance framework prevents the kind of superficial fixes often seen in existing agents. Consider a bug in the Django framework's `order_by()` function. A baseline agent might attempt a "Defensive Bypass"—a quick fix that stops the immediate crash but violates the function’s internal contract, leading to silent failures downstream.

With MemGovern, the agent retrieves an experience card that highlights the underlying *Root Cause* (lack of explicit validation) and the correct *Fix Strategy* (implementing explicit type checking). Guided by this governed human logic, the agent implements a robust, semantically correct repair that maintains the API contract, demonstrating the framework's ability to drive deeper reasoning over blind code mutation.

By providing a high-fidelity, structured memory, MemGovern offers a path for autonomous agents to move beyond simple pattern matching and integrate collective human expertise into the future of software development.