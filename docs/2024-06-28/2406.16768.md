---
layout: paper
pdf_url: https://arxiv.org/pdf/2406.16768
permalink: 2024-06-28/2406.16768/
title: New WARP Strategy Uses ‘Weight Averaging’ to Create Smarter, More Robust LLMs
---



Reinforcement Learning from Human Feedback (RLHF) has become the dominant strategy for aligning large language models (LLMs) with human values. However, this powerful alignment tool faces a critical challenge: the "alignment tax." When fine-tuning a model to maximize a reward signal, it often suffers from catastrophic forgetting, losing the broad knowledge acquired during pre-training.

To prevent this decay, RLHF typically includes a KL (Kullback-Leibler) regularization term, forcing the policy to stay close to its initial state. This creates a fundamental trade-off: high rewards mean high KL divergence (forgetting), and low KL means low reward.

A new paper from Google DeepMind introduces Weight Averaged Rewarded Policies (WARP), an iterative alignment strategy designed to overcome this trade-off by systematically improving the KL-reward Pareto front—achieving higher rewards for the same level of knowledge retention.

WARP achieves this through a three-stage process of "model merging" policies in the weight space, utilizing different forms of weight averaging:

1.  **Exponential Moving Average (EMA) Anchor:** Unlike standard RLHF, which uses a static anchor (the initial fine-tuned model) for KL regularization, WARP employs an EMA of the policy itself. This dynamic anchor serves as a "mean teacher," allowing the policy to progressively relax the constraint and explore more aggressively later in training, leading to faster and more stable reward optimization.

2.  **Spherical Linear intERPolation (SLERP) of Policies:** The second stage involves independently fine-tuning multiple policies (each using its own EMA anchor). WARP then merges these policies using SLERP applied to their "task vectors"—the geometric difference between the fine-tuned model and its initialization. This step is analogous to creating a "model soup," smoothly blending the acquired strengths of several individual models to yield a robust, higher-performing merged policy. The use of spherical interpolation ensures that the combined policy preserves the performance norms better than standard linear interpolation.

3.  **Linear Interpolation Towards Initialization (LITI):** In the final stage, the merged policy is linearly interpolated back toward the original supervised fine-tuned (SFT) initialization. This step creates an improved Pareto front of solutions. By adjusting the interpolation coefficient (η), developers can precisely balance the high rewards gained from the merged model against the general, foundational knowledge retained from the SFT base.

Crucially, WARP is applied iteratively: the optimally balanced model obtained via LITI serves as the enhanced initialization point for the next round of alignment. This procedure progressively refines the policy's capabilities.

Experiments conducted on the Gemma 7B model validate WARP’s efficiency. The iterative process consistently pushes the KL-reward Pareto front into superior regions. In comparative tests against powerful open-source models, WARP-aligned Gemma policies were preferred over variants of Mistral 7B and even Mixtral 8x7B.

Furthermore, WARP showed strong performance gains on reasoning benchmarks such as GSM8K and MATH, indicating that the merging process not only aligns the model but also enhances its analytical capabilities while protecting it from catastrophic forgetting. By leveraging model merging, WARP provides a highly scalable and flexible approach to post-training alignment, turning the compute cost into a direct enhancement of LLM performance and safety.