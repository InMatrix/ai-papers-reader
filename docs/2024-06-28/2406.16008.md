---
layout: paper
pdf_url: https://arxiv.org/pdf/2406.16008
permalink: 2024-06-28/2406.16008/
title: Researchers Solve LLM’s ‘Lost in the Middle’ Problem by Calibrating Attention
  Bias
---



A team of researchers has uncovered the intrinsic cause of a major performance roadblock in large language models (LLMs)—the "lost-in-the-middle" problem—and proposed a calibration mechanism that dramatically improves how these models handle long, complex inputs.

The study, titled “Found in the Middle: Calibrating Positional Attention Bias Improves Long Context Utilization,” establishes that when LLMs are tasked with processing thousands of tokens of information, they exhibit a strong, intrinsic **U-shaped attention bias**, favoring content placed at the very beginning and very end of the prompt, regardless of its actual relevance.

This phenomenon is particularly damaging in Retrieval-Augmented Generation (RAG) systems, where an LLM is given numerous retrieved documents to answer a query. If the crucial "gold document" containing the answer is buried deep within the input, the model often overlooks it entirely.

### The Intrinsic Bias

To understand why this happens, the researchers analyzed the self-attention weights of models like Vicuna and Tulu. They confirmed that the attention mechanism itself—the component responsible for deciding which parts of the input are important—is positionally biased, manifesting as the U-shape.

Imagine a user provides 20 retrieved documents to an LLM to answer a technical question. The actual answer might be in Document 10. The researchers demonstrated that the uncalibrated model consistently assigns higher attention weights to Documents 1 and 20, treating the material in the middle as less important by default. Critically, this bias persisted even when the documents were randomly shuffled, proving the preference was based on position, not content.

For instance, if a query asks about a specific Marvin Gaye album, and the correct information is Document 10 in a 20-document prompt, the model is likely to rely on irrelevant information from Document 1 simply because it appears first.

### Found-in-the-Middle Calibration

The breakthrough solution, dubbed **found-in-the-middle**, works by modeling and removing this positional attention bias.

The researchers hypothesized that an LLM’s attention to a document is driven by two factors: its inherent relevance and its position (the U-shaped bias). By introducing a calibration mechanism, they calculated a baseline positional bias using a "dummy document" (a placeholder with constant relevance). Subtracting this estimated positional bias from the total observed attention scores allowed them to isolate the document’s *true relevance* score.

This calibrated attention score is then used to intervene directly on the model’s attention mechanism during generation. Instead of the LLM deciding importance based on proximity to the start/end, it is forced to focus on contexts based solely on their calculated relevance.

The results were transformative. In challenging open-domain question answering tasks (NaturalQuestion and SynthWiki), the attention calibration mechanism consistently outperformed standard generation methods, achieving up to 15 percentage points improvement when the relevant document was placed mid-sequence.

Furthermore, applying found-in-the-middle on top of existing solutions, such as prompt reordering (where models attempt to guess relevance and move documents to the front), provided a complementary performance boost. This suggests that the calibration fundamentally enhances the model’s capacity for long-context utilization, rather than simply working around a known flaw.

The findings validate that modern LLMs, even those trained for long contexts, are fundamentally capable of locating relevant information hidden in the middle, but they are often overwhelmed and distracted by their own positional bias. The proposed calibration mechanism offers an efficient, inference-time solution to unlock this untapped long-context potential.