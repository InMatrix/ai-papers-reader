---
layout: paper
pdf_url: https://arxiv.org/pdf/2406.15877
permalink: 2024-06-28/2406.15877/
title: BigCodeBench Reveals Large Language Models Are Failing at Real-World Code Generation
---



A new, rigorous benchmark dubbed BigCodeBench has exposed significant weaknesses in the ability of Large Language Models (LLMs) to perform complex, real-world programming tasks, revealing a massive performance gap between the best models and human developers.

While previous coding benchmarks, such as HumanEval, focused primarily on short, self-contained algorithmic puzzles, BigCodeBench evaluates two critical aspects of practical software engineering: the use of diverse function calls as tools, and the ability to follow complex, compositional instructions. The study, which tested 60 state-of-the-art LLMs, found that the highest-performing model, GPT-4o, succeeded on only 60% of structured tasks, significantly lagging behind the 97% human success rate.

Real-world coding demands complex integration. For instance, creating a network utility to fetch data from an HTTPS server doesn't involve one simple command; it requires chaining together calls from multiple specialized librariesâ€”like managing SSL contexts with `ssl`, handling connections with `socket`, and defining client behavior with `http.client`.

BigCodeBench challenges models with 1,140 tasks requiring multiple function calls drawn from 139 popular libraries across seven domains, including computation, visualization, and networking. These tasks demand deep compositional reasoning, where the model must synthesize a correct sequence of operations based on intricate requirements.

The benchmark was split into two variants: BigCodeBench-Complete, which provides structured, verbose instructions via Python docstrings, and BigCodeBench-Instruct, which uses more natural, less detailed language prompts, mimicking casual user requests.

The results clearly showed that LLMs struggle significantly when instructions are less structured, with average performance dropping even further on the `Instruct` variant, sometimes falling below 50%. This failure highlights a lack of robust instruction-following capabilities, where models fail to properly understand condensed human requirements.

Worryingly, researchers also observed instances of "model laziness" in instruction-tuned LLMs, where the models omitted essential setup details, such as necessary import statements, causing the code to fail execution despite appearing logically sound.

Furthermore, LLMs frequently misuse required APIs, demonstrating a superficial understanding of function semantics. For example, in a task designed to unzip a list of mixed-type tuples, an LLM might attempt to use `itertools.zip_longest` incorrectly, feeding it single values instead of unpacking the iterable arguments, leading to incorrect calculations and test failures.

The authors emphasize that LLMs particularly struggle in domains involving network and system operations. These findings underscore that while AI has made strides in generating simple code, the fundamental ability to use diverse tools precisely and follow complex, multi-step directions remains a major hurdle for deployment in advanced software development roles.