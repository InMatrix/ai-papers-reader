---
layout: paper
pdf_url: https://arxiv.org/pdf/2406.18082
permalink: 2024-06-28/2406.18082/
title: Octo-planner Breakthrough Brings Sophisticated AI Planning to Smartphones
---



A team of researchers from Nexa AI, Stanford, MIT, and IBM has unveiled "Octo-planner," an innovative framework designed to deploy high-performance AI agents directly onto resource-constrained edge devices like smartphones and wearables. The system bypasses the high costs and latency typically associated with large language models (LLMs) by separating the complex task of planning from action execution, achieving a 97% success rate in in-domain planning tests.

The rise of AI agents—systems capable of autonomous decision-making—has been hampered by the computational demands of the underlying LLMs, which often require expensive cloud infrastructure. Octo-planner solves this constraint by employing a specialized Planner-Action framework built around efficiency.

The architecture splits the agent into two distinct components: the **Octo-planner**, a fine-tuned, small LLM optimized for edge deployment, and the **Octopus action agent**, which handles function execution. When a user issues a complex command, the Octo-planner's sole job is to quickly break it down into a coherent sequence of sub-steps. These steps are then executed sequentially by the Octopus action agent, which is already optimized for on-device function calling.

For example, if a user queries, "Check news and find videos about Jensen Huang from Nvidia, then email both to jimmy@gmail.com," the Octo-planner decomposes this into a three-step plan:
1. Search for news on Jensen Huang from Nvidia.
2. Find videos about Jensen Huang from Nvidia on YouTube.
3. Email the collected news and videos to the recipient.

To ensure the planner is fast and efficient, the team prioritized fine-tuning the base model—specifically the lightweight Microsoft Phi-3 Mini (a 3.8 billion parameter model)—on a curated dataset of planning tasks. This instructional fine-tuning allows the model to "internalize" descriptions of available functions, eliminating the need for lengthy function descriptions in the prompt (known as in-context learning). This minimizes the required Key-Value (KV) cache, significantly reducing latency and energy consumption on edge devices.

In their experiments, the Phi-3 Mini, when fully fine-tuned on 1,000 data points, achieved a planning accuracy of 98.1%, comparable to much larger proprietary models like Google’s Gemma 7b (99.7%).

Crucially, the researchers also developed a Multi-LoRA training method to address the challenge of scalability. LoRA (Low-Rank Adaptation) is a parameter-efficient technique for updating models, but traditional methods struggle when new functions or domains are introduced. The Multi-LoRA approach merges the weights of multiple LoRAs, each trained on a distinct set of functions (e.g., Android system controls vs. E-commerce tools).

This merging allows a single base model to flexibly handle complex, multi-domain queries without resource-intensive full retraining. While merging LoRAs across two domains (Android and E-commerce) resulted in a slight accuracy drop to 82.2%, the method provides a scalable path for expanding agent capabilities across diverse applications, from smart homes to robotics.

By making sophisticated AI planning efficient and locally executable, the Octo-planner paves the way for advanced AI assistants that prioritize data privacy, low latency, and offline functionality, making intelligent assistance truly practical for everyday use.