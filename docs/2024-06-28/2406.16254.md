---
layout: paper
pdf_url: https://arxiv.org/pdf/2406.16254
permalink: 2024-06-28/2406.16254/
title: LLMs Possess Internal ‘Calibration Circuits’ to Regulate Confidence
---



A team of researchers from ETH Zürich, MIT, and the University of Sheffield has published findings identifying and detailing specialized internal components within large language models (LLMs) dedicated solely to regulating prediction confidence and uncertainty.

The study, which analyzes models ranging from GPT-2 to LLaMA2 7B, shows that LLMs use two distinct types of "confidence regulation neurons" to calibrate their next-token predictions, offering unprecedented insight into how these systems manage risk associated with overconfident errors.

For decades, the mechanisms by which deep neural networks quantify their own uncertainty have remained largely opaque. This new work highlights that LLMs have evolved specific, general-purpose circuitry for calibration, suggesting a deliberate hedge against failure.

### The Hedgers: Entropy Neurons and the Null Space Trick

The first class of components investigated are "entropy neurons," characterized by high weight but surprisingly low direct impact on the model’s final prediction (the logits). Researchers found that these neurons operate indirectly, functioning as an internal "hedging mechanism."

Instead of altering which token the model predicts (e.g., changing "apple" to "banana"), entropy neurons primarily modulate the overall scale of the model’s prediction distribution, thereby increasing or decreasing its certainty.

This is achieved through an ingenious architectural trick. The entropy neurons write their signal onto a low-impact "unembedding null space" in the final layer. This signal bypasses the direct prediction path but leverages the LayerNormalization (LayerNorm) component—the model’s internal volume dial—to effectively scale down the logits.

**A concrete example:** In tasks involving "induction" (repeating a previously seen sequence, like "The quick brown fox... The quick brown f..."), the model becomes highly confident about predicting the next token ("ox"). If the model is wrong, this overconfidence leads to a massive loss spike. The study demonstrates that entropy neurons actively fire during these confident moments, increasing the output entropy to "hedge the bet," marginally increasing loss on correct predictions while severely mitigating catastrophic loss spikes on wrong, high-confidence predictions.

### The Baseline Setters: Token Frequency Neurons

The paper also introduces a newly discovered class of components: "token frequency neurons." These neurons manage uncertainty by comparing the model's predicted distribution against the default statistical baseline—the unigram distribution (how often tokens appear in the general training corpus).

When the LLM is highly uncertain about what comes next, it tends to default to predicting common words. Token frequency neurons act to either boost or suppress the logits of tokens proportional to their overall frequency.

The research suggests these neurons fine-tune the model's fallback behavior. For instance, if the LLM is stumped, a Token Frequency Neuron might activate to push the prediction closer to common words (e.g., boosting the likelihood of "the" or "a"), or, conversely, suppress common words to promote rarer tokens if context suggests a high-uncertainty but low-frequency output.

By mapping these dedicated circuits, the study provides a foundational understanding of internal confidence regulation, a critical step toward ensuring that LLMs used in high-stakes applications are not just powerful, but reliably calibrated in their decision-making.