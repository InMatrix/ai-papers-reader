---
layout: paper
pdf_url: https://arxiv.org/pdf/2406.16714
permalink: 2024-06-28/2406.16714/
title: AUTODETECT&#58; New AI Framework Acts as an Automated ‘Examiner’ to Expose
  Hidden Weaknesses in Large Language Models
---



Despite their impressive capabilities, Large Language Models (LLMs) like GPT-3.5 and Llama continue to harbor significant, yet often subtle, weaknesses that traditional benchmarks fail to capture. Researchers from Tsinghua University and Zhipu AI have introduced **AUTODETECT**, a novel, unified framework designed to systematically and automatically expose these vulnerabilities across critical tasks, including instruction-following, mathematical reasoning, and coding.

Traditional evaluation systems are typically static and model-agnostic, ranking models without pinpointing the specific defects of an individual LLM. AUTODETECT adopts a dynamic methodology inspired by educational assessment, utilizing a collaborative system of three LLM-powered agents: the Examiner, the Questioner, and the Assessor.

The **Examiner** initiates the process by developing a detailed, adaptable taxonomy of potential skills (e.g., separating 'coding' into subcategories like 'logic and control flow' and 'data container operations'). The **Questioner** then generates a series of challenging test cases for specific knowledge points. Finally, the **Assessor** analyzes the target LLM’s low-scoring responses (bad cases) to identify the root cause of the failure, feeding new information back to the Examiner to refine the testing taxonomy iteratively.

This cyclical, targeted approach has proven highly effective. The framework achieved an impressive identification success rate (ISR) exceeding 30% in robust closed-source models like GPT-3.5-turbo and Claude-3-sonnet.

AUTODETECT’s findings reveal critical limitations. For instance, while LLMs often excel at complex algorithmic problems, they frequently fail at simpler, foundational tasks. Researchers cited instances where a model, when instructed to import *all* functions from Python's 'os' module, incorrectly imported only the module itself. Another unexpected failure occurred when testing simple data container constraints: an LLM, asked to write a function operating on a set, mistakenly added duplicate elements during the set’s initialization—a fundamental error for a structure designed to hold unique values.

Furthermore, LLMs struggle acutely with multi-step reasoning coupled with complex constraints. The framework successfully generated test cases that required the model to follow multiple restrictive instructions simultaneously, such as explaining a technical concept while explicitly prohibiting the use of common query words like "what," "why," or "how."

Crucially, the identification of these flaws is not merely an assessment but a roadmap for improvement. By using the approximately 1,000 high-quality, targeted samples derived from the AUTODETECT process to fine-tune open-source models (including the Llama and Mistral series), the researchers demonstrated substantial performance gains. The fine-tuned models achieved over 10% average improvement across multiple industry benchmarks, validating that targeted weakness detection is significantly more effective than general, untargeted data augmentation.

The research highlights AUTODETECT's potential to guide future LLM development, paving the way for more robust and trustworthy AI systems built upon automatically identified and addressed vulnerabilities.