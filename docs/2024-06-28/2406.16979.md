---
layout: paper
pdf_url: https://arxiv.org/pdf/2406.16979
permalink: 2024-06-28/2406.16979/
title: Diagnostic Tool Uncovers Persistent Vulnerabilities in Deep AI Policies
---



A new analysis method is shedding light on the critical but poorly understood instability of deep reinforcement learning (DRL) policies, revealing that even state-of-the-art “robust” training techniques fail to eliminate dangerous vulnerabilities. Instead, these methods often simply relocate or intensify instability, presenting a significant challenge for deploying safe artificial intelligence systems.

Journalist Ezgi Korkmaz, in a paper published on arXiv, introduces a novel diagnostic technique called Robustness Analysis via Non-Lipschitz Directions (RA-NLD). This method provides a systematic way to map and measure the high-sensitivity areas—the "non-robust features"—in the decision-making landscape of DRL agents.

DRL policies, used in systems from automated finance to autonomous vehicles, are known to be vulnerable to adversarial attacks—imperceptible changes to their input that cause catastrophic failures. The complexity of these deep neural networks makes it difficult to diagnose *why* they fail.

RA-NLD addresses this by identifying correlated "unstable directions" across the policy's operational states. If the policy is viewed as a complex terrain, RA-NLD pinpoints the sharpest, most volatile slopes where a tiny shift in input (state observation) leads to a major change in the agent's action (decision).

### The Paradox of Robust Training

Using RA-NLD to analyze neural policies trained on the Arcade Learning Environment (ALE)—including classic games like *Pong* and *BankHeist*—the research delivered a counterintuitive finding regarding certified adversarial training.

Standard policies exhibit vulnerabilities that are relatively smooth and correlated across game states. However, the study found that policies trained using certified robust methods, designed to defend against attacks, did not become universally stable. Instead, they learned vulnerable representations that were "disjoint and spikier."

To build intuition, consider an AI agent playing *Pong*. A standard agent might have vulnerabilities that are moderately high across most states. The certified robust agent, however, may have eliminated those general vulnerabilities, but replaced them with sharp, concentrated cliffs of extreme instability in isolated states. The analysis showed that these robustly trained policies exhibited “dramatically larger oscillations over time” in sensitivity, resulting in high vulnerability variance and unpredictable decision-making.

Furthermore, RA-NLD demonstrated that different types of attacks—such as Carlini&Wagner versus Nesterov Momentum perturbations—target and surface distinct sets of non-robust features.

### Shifting Vulnerabilities

Beyond adversarial attacks, the research also quantified how natural, imperceptible changes in the environment affect stability. The team tested common "distributional shifts" like slight blurring, rotation, or adding compression artifacts to the game visuals.

For games like *RoadRunner* and *Freeway*, these minor environmental shifts caused the vulnerable features to remold and relocate drastically, often causing correlation scores to drop as significantly as targeted adversarial attacks. This highlights that policies are not just vulnerable to malicious attackers but also to slight variations in real-world sensor input or image processing, further underscoring the necessity of diagnostic tools like RA-NLD before AI policies are deployed in high-stakes environments.

The results provide crucial diagnostic insights for developers, allowing them to visualize and debug policy instabilities and potentially design training protocols that enforce genuine robustness rather than simply shifting volatility to a new, less predictable location in the decision manifold.