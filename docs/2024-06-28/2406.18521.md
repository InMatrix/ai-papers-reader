---
layout: paper
pdf_url: https://arxiv.org/pdf/2406.18521
permalink: 2024-06-28/2406.18521/
title: New AI Benchmark, CharXiv, Exposes Massive Gaps in Multimodal LLM Chart Understanding
---



A new, rigorous evaluation suite dubbed CharXiv has revealed that Multimodal Large Language Models (MLLMs)—both proprietary and open-source—are drastically overestimating their own capabilities when it comes to visual chart understanding and reasoning.

Developed by researchers at Princeton University and collaborators, CharXiv uses a diverse collection of complex, real-world charts, exposing deficiencies that existing, simpler benchmarks have failed to capture. The results demonstrate a profound skill gap between state-of-the-art MLLMs and human performance, especially in complex reasoning tasks.

### Stress Test Reveals Overestimation

The motivation behind CharXiv stems from the finding that models trained on existing, homogeneous datasets (like FigureQA or ChartQA) generalize poorly when exposed to minor variations.

To illustrate this weakness, researchers conducted a stress test, showing that simply modifying a question template applied to the same chart could cause leading open-source models like SPHINX V2 to suffer a performance drop of up to 34.5%. This indicates that MLLMs are learning superficial patterns on existing benchmarks rather than true chart interpretation skills.

“Existing datasets often focus on oversimplified and homogeneous charts with template-based questions, leading to an over-optimistic measure of progress,” the authors wrote.

### CharXiv: A Real-World Challenge

To provide a more faithful measure, CharXiv consists of 2,323 natural, challenging, and diverse charts handpicked from recent scientific preprints published on arXiv between 2020 and 2023. These charts feature complex compositions, overlapping elements, and heterogeneous styles typical of real-world scientific literature.

The evaluation suite uses two distinct types of human-curated questions:
1. **Descriptive questions** focus on extracting basic elements (e.g., "What is the label of the x-axis?").
2. **Reasoning questions** require synthesizing information, making comparisons, or performing fine-grained numerical analysis (e.g., determining which country showed a "significant bounce" in related Google searches shortly after a specific date, based on the shape of a line plot).

The performance gap on CharXiv is stark. Across reasoning questions, the strongest proprietary model tested, **GPT-4o, achieved only 47.1% accuracy**. This lags far behind human performance, which sits at 80.5%. The top open-source contender, **InternVL Chat V1.5, scored just 29.2% in reasoning**, highlighting a 17.9% gap even against the best proprietary model.

### Failure in Basic Compositional Skills

Even seemingly simple tasks reveal fundamental weaknesses in MLLMs’ basic visual understanding. Descriptive questions require models to locate and extract basic chart elements like labels and ticks. On average, GPT-4o achieved 84.5% accuracy on these, while InternVL Chat V1.5 managed 58.5%.

A detailed analysis showed models struggling particularly with "compositional tasks" that are trivial for humans. For instance, in one task requiring the model to count the total number of explicitly labeled ticks across all axes of a complex chart, humans achieved 92.86% accuracy. Yet, the strongest open-source model achieved a meager 5.80%, performing near the random baseline, indicating a core failure in parsing and aggregating visual components. Furthermore, the study introduced intentionally unanswerable questions (25% of descriptive tasks), finding that weak models often failed to identify when information was missing or inapplicable to the target subplot.

The authors hope that the introduction of CharXiv will facilitate more robust research, guiding developers toward MLLMs capable of genuine chart reasoning, a critical skill for tasks ranging from financial analysis to summarizing scientific breakthroughs.