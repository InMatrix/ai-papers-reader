---
layout: paper
pdf_url: https://arxiv.org/pdf/2406.15927
permalink: 2024-06-28/2406.15927/
title: New Semantic Entropy Probes Offer a Robust and Cheap Way to Detect LLM Hallucinations
---



A team of researchers from the University of Oxford’s OATML lab has introduced a novel method for detecting hallucinations in Large Language Models (LLMs) that dramatically reduces computational cost without sacrificing reliability. Dubbed Semantic Entropy Probes (SEPs), the system bypasses the slow, multi-generation sampling traditionally required for uncertainty estimation, establishing a new state-of-the-art for cost-efficient hallucination detection.

LLM hallucinations—plausible-sounding yet factually incorrect outputs—remain the greatest barrier to safe deployment in high-stakes fields like medicine and law. The gold standard for detecting these arbitrary responses has been Semantic Entropy (SE), which measures an LLM’s uncertainty by comparing the semantic meaning of multiple generated answers.

For example, if an LLM is asked, "What is the capital of France?", and consistently replies "Paris," "It's Paris," or "The capital of France is Paris," its semantic entropy is low, indicating high confidence. Conversely, if it returns highly varied answers like "Naples," "Rome," and "Berlin," its semantic entropy is high, signaling a potential hallucination.

The major drawback of Semantic Entropy is cost. Computing SE requires generating 5 to 10 distinct responses for every query, inflating processing costs by up to 10 times.

The new Semantic Entropy Probes address this bottleneck by training simple linear models on the LLM’s internal “hidden states.” Instead of waiting for multiple generated tokens, SEPs learn to predict the expensive Semantic Entropy score after just a single, greedy forward pass. This single-pass prediction reduces the computational overhead of semantic uncertainty quantification to near zero at test time.

Crucially, SEPs are trained to predict semantic *uncertainty* (SE), an intrinsic property of the model, rather than external *accuracy* labels, which rely on potentially noisy ground truth data. This design choice proved critical for robustness.

In experiments across models like Llama-2 and Mistral-7B and datasets like TriviaQA and SQuAD, SEPs demonstrated superior performance compared to traditional accuracy probes—especially when tested on unseen, out-of-distribution tasks.

“Semantic uncertainty is a better probing target than truthfulness, because semantic uncertainty is a more model-internal characteristic that can be better predicted from model hidden states,” the authors state.

The research also offers intriguing insights into where uncertainty resides within LLMs. SEPs were effective even when trained on the hidden state of the input query *before* the model generated any novel tokens (known as the token-before-generation, or TBG position). This suggests that LLMs implicitly capture their semantic uncertainty the moment a question is posed, making it possible to quantify risk instantly before committing to a costly, lengthy response.

While the new probes do not fully match the accuracy of 10x costlier sampling methods, they provide a reliable, highly cost-effective alternative. For organizations deploying LLMs in real-world environments where the type of user queries cannot be perfectly predicted, SEPs offer a practical mechanism to ensure greater factual alignment and trustworthiness.